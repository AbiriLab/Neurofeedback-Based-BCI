{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import unwrap, diff, abs, angle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.utils import shuffle\n",
    "import scipy\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import spectrogram\n",
    "from mne.viz import plot_topomap\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import optuna\n",
    "from sklearn.datasets import make_classification\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from joblib import dump\n",
    "from scipy.signal import butter, filtfilt, lfilter, lfilter_zi\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "selected_columns = ['Fz', 'C3', 'Cz', 'C4', 'Pz', 'Po7', 'Oz', 'Po8']\n",
    "fs=250\n",
    "\n",
    "####################################################################################\n",
    "#pre/processing functions\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def detrend(df, col_names):\n",
    "    df_detrended = df.copy()\n",
    "    for col in col_names:\n",
    "        y = df_detrended[col]\n",
    "        x = np.arange(len(y))\n",
    "        p = np.polyfit(x, y, 1)\n",
    "        trend = np.polyval(p, x)\n",
    "        detrended = y - trend\n",
    "        df_detrended[col] = detrended\n",
    "    return df_detrended\n",
    "\n",
    "def denoise_data(df, col_names, n_clusters):\n",
    "    df_denoised = df.copy()\n",
    "    for col_name, k in zip(col_names, n_clusters):\n",
    "        df_denoised[col_name] = pd.to_numeric(df_denoised[col_name], errors='coerce') # Convert column to numeric format\n",
    "        X = df_denoised.select_dtypes(include=['float64', 'int64']) # Select only numeric columns\n",
    "        clf = KNeighborsRegressor(n_neighbors=k, weights='uniform') # Fit KNeighborsRegressor\n",
    "        clf.fit(X.index.values[:, np.newaxis], X[col_name])\n",
    "        y_pred = clf.predict(X.index.values[:, np.newaxis]) # Predict values \n",
    "        df_denoised[col_name] = y_pred\n",
    "    return df_denoised\n",
    "\n",
    "def z_score(df, col_names):\n",
    "    df_standard = df.copy()\n",
    "    for col in col_names:\n",
    "        df_standard[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    return df_standard\n",
    "\n",
    "def custom_detrend(df, col_names):\n",
    "    df_detrended = df.copy()\n",
    "    for col in col_names:\n",
    "        y = df_detrended[col]\n",
    "        x = np.arange(len(y))\n",
    "        p = np.polyfit(x, y, 1)\n",
    "        trend = np.polyval(p, x)\n",
    "        detrended = y - trend\n",
    "        df_detrended[col] = detrended\n",
    "    return df_detrended\n",
    "\n",
    "def preprocess(df, col_names, n_clusters):\n",
    "    df_new = df.copy()\n",
    "    df_new = denoise_data(df, col_names, n_clusters)\n",
    "    # df_new = z_score(df_new, col_names)\n",
    "    df_new = detrend(df_new, col_names)\n",
    "    return df_new\n",
    "\n",
    "def reject_artifacts(df, channel):\n",
    "    threshold_factor = 3\n",
    "    median = df[channel].median()\n",
    "    mad = np.median(np.abs(df[channel] - median))\n",
    "    spikes = np.abs(df[channel] - median) > threshold_factor * mad\n",
    "    x = np.arange(len(df[channel]))\n",
    "    cs = CubicSpline(x[~spikes], df[channel][~spikes]) # Interpolate using Cubic Spline\n",
    "    interpolated_values = cs(x)\n",
    "    interpolated_values[spikes] *= 0.01  # Make interpolated values 0.01 times smaller\n",
    "    # Again Check each interpolated value's difference from median and compare to the threshold\n",
    "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
    "    interpolated_values[spike_values] *= 0.01 \n",
    "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
    "    interpolated_values[spike_values] *= 0.01 \n",
    "    df[channel] = interpolated_values\n",
    "    return df\n",
    "\n",
    "#########################################################################################\n",
    "current_directory = os.getcwd()\n",
    "patient_data_folder = os.path.join(current_directory, \"1-Data_for_Model_Test\")\n",
    "\n",
    "folder_name = input(\"Please enter the subject name: \")\n",
    "Report_Number = input(\"Please enter the reprt number: \")\n",
    "full_folder_path = os.path.join(patient_data_folder, folder_name)\n",
    "\n",
    "# root_folder = \"2-Patient Data\"\n",
    "sub_folders = [\"Pre Evaluation\", \"Neurofeedback\", \"Post Evaluation\"]\n",
    "phase = int(input(\"Enter the phase (0, 1, 2): \"))  # Or however you get the phase value\n",
    "# Determine which sub-folders to use based on the phase\n",
    "folders_to_use = []\n",
    "if phase == 0:\n",
    "    folders_to_use = [sub_folders[0]]  # Just \"Pre Evaluation\"\n",
    "elif phase == 1:\n",
    "    folders_to_use = sub_folders[:2]  # \"Pre Evaluation\" and \"Neurofeedback\"\n",
    "elif phase == 2:\n",
    "    folders_to_use = [sub_folders[2]]  # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "#Notice that: base:2, face:0, scene:1\n",
    "duration = 40 \n",
    "raw=[]\n",
    "event=[]\n",
    "PP=[]\n",
    "Human_Behavior=[]\n",
    "eeg_processed=[]\n",
    "for folder in folders_to_use:\n",
    "    full_folder_path_ = os.path.join(full_folder_path, folder)\n",
    "    if os.path.exists(full_folder_path_) and os.path.isdir(full_folder_path_):\n",
    "        for file_name in os.listdir(full_folder_path_):\n",
    "            if file_name.endswith('.csv') and (file_name.startswith('raw_eeg_block') or file_name.startswith('fl_') \n",
    "                                               or file_name.startswith('R') or file_name.startswith('B')):\n",
    "                file_path = os.path.join(full_folder_path_, file_name)\n",
    "                s_temp = pd.read_csv(file_path, header=None)\n",
    "                \n",
    "                inst_symbol = s_temp.iloc[:, 17]\n",
    "                \n",
    "                #Converting the labels to numbers\n",
    "                inst=[]\n",
    "                for i in range(len(inst_symbol)):\n",
    "                    if 'g' in inst_symbol[i]  or 'b' in inst_symbol[i] :\n",
    "                        inst.append(2)\n",
    "                    if 'M' in inst_symbol[i]  or 'F' in inst_symbol[i] :\n",
    "                        inst.append(0)\n",
    "                    if 'O' in inst_symbol[i]  or 'I' in inst_symbol[i] :\n",
    "                        inst.append(1) \n",
    "                \n",
    "                HB=s_temp.iloc[1750:, 17:21]\n",
    "                df_temp_eeg = s_temp.iloc[:, :8]\n",
    "                Human_Behavior.append(HB)\n",
    "\n",
    "                # 1. Band Pass\n",
    "                raw_bp = np.copy(df_temp_eeg)\n",
    "                for column in range(8):\n",
    "                    raw_bp[:, column] = butter_bandpass_filter(raw_bp[:, column], lowcut=.4, highcut=40, fs=250) \n",
    "                \n",
    "                # 2. Artifact rejection\n",
    "                BP_artifact_RJ = np.copy(raw_bp)\n",
    "                for channel in range (8):\n",
    "                    BP_artifact_RJ= reject_artifacts(pd.DataFrame(BP_artifact_RJ), channel)\n",
    "                \n",
    "                # 3. Preprocessing: Denoise, Z-Score, Dtrend\n",
    "                BP_artifact_RJ.columns = selected_columns\n",
    "                eeg_df_denoised = preprocess(pd.DataFrame(BP_artifact_RJ), col_names=selected_columns, n_clusters=[10]*len(selected_columns))\n",
    "                \n",
    "                # I preprocessed the whole data in each block, then, split it to the base and the activity signal\n",
    "                eeg_activity=eeg_df_denoised.iloc[1750:,]                \n",
    "                \n",
    "                #concate the labels to the processed data\n",
    "                eeg_df_denoised_inst=pd.concat([eeg_df_denoised,pd.DataFrame(inst)], axis=1) \n",
    "                \n",
    "                eeg_processed.append(eeg_df_denoised_inst) #all blocks processes eeg with instruction containing both base and activity signal\n",
    "                #len(eeg_processed)= the number of blocks\n",
    "                PP.append(eeg_activity)\n",
    "    else:\n",
    "        print(f\"{full_folder_path_} does not exist\")\n",
    "        \n",
    "block_number=len(eeg_processed)\n",
    "print(block_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total_base_rejected_with_inst_np.shape: (8, 11750, 9) face_np.shape: (4, 11750, 9)\n"
     ]
    }
   ],
   "source": [
    "#baseline rejection\n",
    "activity_baserejected_with_inst=[]\n",
    "Total_base_rejected_with_inst=[]\n",
    "face=[] #label=0\n",
    "scene=[]#label=1, base label=2\n",
    "instruction_of_activity=[]\n",
    "event_=[]\n",
    "for i in range (len(eeg_processed)):\n",
    "    pp_data=eeg_processed[i]\n",
    "    instruction = pp_data.iloc[:, 8]\n",
    "\n",
    "    mask = pp_data.iloc[:, -1] == 2\n",
    "    baseline = pp_data[mask].iloc[:, :-1]\n",
    "    baseline_avg = baseline.mean()\n",
    "    activity_data = pp_data[~mask]\n",
    "\n",
    "    inst_activity=activity_data.iloc[:, 8]\n",
    "    instruction_of_activity.append(inst_activity)\n",
    "    \n",
    "    adjusted_activity = activity_data.iloc[:, :-1] - baseline_avg\n",
    "    adjusted_baseline = baseline - baseline_avg\n",
    "    \n",
    "    concatenated_base_activity = pd.concat([adjusted_baseline, adjusted_activity], ignore_index=True)\n",
    "    pp_data_baseline_rejected= pd.concat([concatenated_base_activity, instruction], axis=1) #both activity ans base with labels\n",
    "    \n",
    "    activity_baseline_rejected_with_inst= pd.concat([adjusted_activity, inst_activity], axis=1)\n",
    "    activity_baserejected_with_inst.append(activity_baseline_rejected_with_inst) #All the activity data and their labels\n",
    "    \n",
    "    Total_base_rejected_with_inst.append(pp_data_baseline_rejected) #All the data both activity ans base with labels\n",
    "    \n",
    "    # splitting the data to face and scene category\n",
    "    if [1] in instruction.values :\n",
    "        scene.append(pp_data_baseline_rejected) # has also the base            \n",
    "    if [0] in instruction.values :\n",
    "        face.append(pp_data_baseline_rejected)  # has also the base                       \n",
    "    event_.append(instruction) # has also the base   \n",
    "     \n",
    "face_np=np.array(face) # has also the base with labels\n",
    "scene_np=np.array(scene) # has also the base with labels\n",
    "event_np=np.array(event_)\n",
    "\n",
    "Total_base_rejected_with_inst_np=np.array(Total_base_rejected_with_inst) #All the data both activity ans base with labels\n",
    "print(' Total_base_rejected_with_inst_np.shape:', Total_base_rejected_with_inst_np.shape, 'face_np.shape:', face_np.shape)\n",
    "\n",
    "labels_np=np.array(instruction_of_activity)\n",
    "\n",
    "#epoching the labels\n",
    "label_final=labels_np.reshape(int(labels_np.shape[0]*labels_np.shape[1]/fs), fs)\n",
    "label_final_np=np.array(label_final)\n",
    "label_final_np_squeeze=np.squeeze(label_final_np[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_z_score(df, col_names, mean_list, std_list):\n",
    "    df_standard = df.copy()\n",
    "    for index, col in enumerate(col_names):\n",
    "        df_standard[col] = (df[col] - mean_list[index]) / std_list[index]\n",
    "    return df_standard\n",
    "\n",
    "input_z_score= Total_base_rejected_with_inst_np.reshape( Total_base_rejected_with_inst_np.shape[0]* Total_base_rejected_with_inst_np.shape[1],  Total_base_rejected_with_inst_np.shape[2])\n",
    "inst_zscore=input_z_score[:,-1]\n",
    "\n",
    "input_z_score_df = pd.DataFrame(input_z_score[:,:-1], columns=selected_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std_df = pd.read_csv('mean_std_values.csv')\n",
    "\n",
    "mean_list = mean_std_df['Mean'].tolist()\n",
    "std_list = mean_std_df['Std'].tolist()\n",
    "\n",
    "z_scored_data = apply_z_score(input_z_score_df, selected_columns, mean_list, std_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def z_score(df, col_names):\n",
    "#     df_standard = df.copy()\n",
    "#     for col in col_names:\n",
    "#         df_standard[col] = (df[col] - df[col].mean())/df[col].std()\n",
    "#     return df_standard\n",
    "\n",
    "# input_z_score= Total_base_rejected_with_inst_np.reshape( Total_base_rejected_with_inst_np.shape[0]* Total_base_rejected_with_inst_np.shape[1],  Total_base_rejected_with_inst_np.shape[2])\n",
    "# inst_zscore=input_z_score[:,-1]\n",
    "\n",
    "# input_z_score_df = pd.DataFrame(input_z_score[:,:-1], columns=selected_columns)\n",
    "\n",
    "# input_z_score_df.shape\n",
    "\n",
    "# for channel in range(8):\n",
    "#     z_scored_data=z_score(input_z_score_df , col_names=selected_columns)\n",
    "\n",
    "inst_zscore_2d=inst_zscore.reshape(inst_zscore.shape[0], 1)\n",
    "\n",
    "z_scored_data_np=np.array(z_scored_data)\n",
    "z_scored_data_np_inst=np.concatenate([z_scored_data_np, inst_zscore_2d], axis=1)\n",
    "z_scored_data_reshape_inst=z_scored_data_np_inst.reshape(Total_base_rejected_with_inst_np.shape[0], Total_base_rejected_with_inst_np.shape[1], Total_base_rejected_with_inst_np.shape[2])\n",
    "data_T =z_scored_data_reshape_inst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wavelet\n",
    "#functions\n",
    "def morlet_wavelet(frequency, num_cycles, sampling_rate, duration=1):\n",
    "    t = np.linspace(-duration/2, duration/2, int(sampling_rate * duration), endpoint=False)\n",
    "    sine_wave = np.exp(2j * np.pi * frequency * t)\n",
    "    amplitude_envelope = np.exp(-t**2 * (np.pi * frequency / num_cycles)**2)\n",
    "    wavelet = sine_wave * amplitude_envelope\n",
    "    return wavelet\n",
    "\n",
    "def convolve_with_wavelet(data, wavelet):\n",
    "    # Ensure data is in 2D form\n",
    "    if data.ndim == 1:\n",
    "        data = data[np.newaxis, :]\n",
    "    n_signal = data.shape[1]\n",
    "    n_wavelet = len(wavelet)\n",
    "    n_convolution = n_signal + n_wavelet - 1\n",
    "\n",
    "    data_fft = np.fft.fft(data, n_convolution, axis=1)\n",
    "    wavelet_fft = np.fft.fft(wavelet, n_convolution)[np.newaxis, :]\n",
    "    convolution_result_fft = data_fft * wavelet_fft\n",
    "    convolution_result = np.fft.ifft(convolution_result_fft, axis=1)\n",
    "    # Cut the data\n",
    "    start = (n_wavelet - 1) // 2\n",
    "    end = start + n_signal\n",
    "    return convolution_result[:, start:end]\n",
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "frequencies = np.arange(.5, 40, 1)  # 0 to 40 Hz\n",
    "cycles = np.linspace(.1,10, len(frequencies))  # Variable cycles from 1 to 10\n",
    "sampling_rate = 250\n",
    "selected_columns = ['Fz', 'C3', 'Cz', 'C4', 'Pz', 'Po7', 'Oz', 'Po8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 8, 40, 250)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Extraction\n",
    "#Wavelet\n",
    "####################################################################################################################################\n",
    "# Constants\n",
    "frequencies = np.arange(.5, 40,1)  # 0 to 40 Hz\n",
    "cycles = np.linspace(1, 8, len(frequencies))  # Variable cycles from 1 to 10\n",
    "sampling_rate = 250\n",
    "selected_columns = ['Fz', 'C3', 'Cz', 'C4', 'Pz', 'Po7', 'Oz', 'Po8']\n",
    "data_T = Total_base_rejected_with_inst_np #shape: (32, 11750, 9)\n",
    "\n",
    "# Initialize a dictionary to store power matrices for each channel\n",
    "power_matrices_T = {channel: [] for channel in selected_columns}\n",
    "\n",
    "for channel_idx, channel_name in enumerate(selected_columns):\n",
    "    for freq, cycle in zip(frequencies, cycles):\n",
    "        power_blocks_base_corrected=[]\n",
    "        for i in range (len(data_T)): #8\n",
    "            data_T_i_np= data_T[i]\n",
    "            data_T_i_np_t=np.transpose(data_T_i_np)\n",
    "\n",
    "            # Extract one channel\n",
    "            data_T_i_t = data_T_i_np_t[channel_idx, :]  # Trials are on the second dimension\n",
    "            data_T_i_inst = data_T_i_np_t[8, :]\n",
    "            wavelet = morlet_wavelet(freq, cycle, sampling_rate)\n",
    "            convolution = convolve_with_wavelet(data_T_i_t , wavelet)    \n",
    "            power = np.abs(convolution)**2\n",
    "\n",
    "            data_T_i_inst_2d = data_T_i_inst[np.newaxis, :]\n",
    "            power_with_inst=np.concatenate([power,data_T_i_inst_2d], axis=0)\n",
    "            power_with_inst_t=np.transpose(power_with_inst)\n",
    "            mask = (power_with_inst_t[:, 1] == 2)\n",
    "            base_l= power_with_inst_t[mask]\n",
    "            base= base_l[:, :-1]\n",
    "            # print(base.shape)\n",
    "            mean_base= np.mean(base_l[:, :-1], axis=0)\n",
    "            # print('mean_base', mean_base)\n",
    "            \n",
    "            activity = power_with_inst_t[~mask]            \n",
    "            epoch_size = 250\n",
    "            epochs = []\n",
    "            for start in range(0, len(activity), epoch_size):\n",
    "                end = start + epoch_size\n",
    "                if end <= len(activity):\n",
    "                    epochs.append(activity[start:end, :-1])\n",
    "                else:\n",
    "                    epochs.append(activity[start:,:-1])\n",
    "                epochs_np=np.array(epochs)\n",
    "            # print('epochs_np', epochs_np.shape)\n",
    "            \n",
    "            activity_mean_epochs = np.mean(epochs_np, axis=0)\n",
    "            # print('activity_mean_epochs.shape:', activity_mean_epochs.shape) #(250, 1)\n",
    "            normalized=activity_mean_epochs/mean_base \n",
    "            \n",
    "            #baseline normalization\n",
    "            nb_e=[]\n",
    "            for i in range (epochs_np.shape[0]):\n",
    "                nb_b_e=[]\n",
    "                for j in range (epochs_np.shape[1]):\n",
    "                    vector1 = epochs_np[i, j,:]\n",
    "                    vector2 =activity_mean_epochs[j]\n",
    "                    result_vector = vector1\n",
    "                    # print(normalized[j], epochs_np[i, j,:]/mean_base)\n",
    "                    nbej=10*np.log10(float(result_vector/mean_base))\n",
    "                    # nbej=10*np.log10(float(1.2*normalized[j]+.5*epochs_np[i, j,:]/mean_base))\n",
    "                    nb_b_e.append(nbej)\n",
    "                nb_e.append(nb_b_e)\n",
    "            \n",
    "            nb_e_np=np.array(nb_e)\n",
    "\n",
    "            baseline_normalized=nb_e_np\n",
    "            # print(baseline_normalized.shape)\n",
    "            power_blocks_base_corrected.append(baseline_normalized)\n",
    "        power_matrices_T[channel_name].append(power_blocks_base_corrected)    \n",
    "        ###############################################################################################################################################################\n",
    "np.array(power_matrices_T['C3']).shape    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 8, 7)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "for channel_idx, channel_name in enumerate(selected_columns):\n",
    "    # print(channel_name)\n",
    "    wavelet_coefficient_np=np.array(power_matrices_T[channel_name])\n",
    "    # print(wavelet_coefficient_np.shape)\n",
    "    reshaped_np = wavelet_coefficient_np.transpose(1, 2, 3, 0).reshape(int(wavelet_coefficient_np.shape[1] * 40), 250, 40)\n",
    "    # Checking the new shape\n",
    "    # print(reshaped_np.shape) #(960, 250, 40)\n",
    "    \n",
    "    \n",
    "# Initialize the final numpy array to store the features for each input and each channel\n",
    "num_inputs = reshaped_np.shape[0]\n",
    "num_channels = 8  # As mentioned, assuming there are 8 channels\n",
    "num_features = 7  # Total number of features\n",
    "\n",
    "# The final shape will be (number of inputs, number of channels, number of features)\n",
    "wavelet_feature_array = np.zeros((num_inputs, num_channels, num_features))\n",
    "\n",
    "# Function to calculate entropy\n",
    "def calculate_entropy(data):\n",
    "    prob_dist = data / np.sum(data)\n",
    "    return -np.sum(prob_dist * np.log2(prob_dist + np.finfo(float).eps))\n",
    "\n",
    "# Loop over each input and each channel\n",
    "for i in range(num_inputs):\n",
    "    for j in range(num_channels):\n",
    "        # Extracting data for the current input and channel\n",
    "        input_data = reshaped_np[i, :, j]\n",
    "\n",
    "        # Calculating features\n",
    "        mean = np.mean(input_data)\n",
    "        variance = np.var(input_data)\n",
    "        peaks, _ = find_peaks(input_data)\n",
    "        peak_magnitudes = input_data[peaks] if peaks.size > 0 else np.array([0])\n",
    "        max_peak_idx = np.argmax(peak_magnitudes) if peaks.size > 0 else 0\n",
    "        # print(max_peak_idx)\n",
    "        peak_frequency = peaks[max_peak_idx] if peaks.size > 0 else 0\n",
    "        peak_magnitude = peak_magnitudes[max_peak_idx]\n",
    "        energy = np.sum(input_data ** 2)\n",
    "        # entropy = calculate_entropy(input_data)\n",
    "        skewness = skew(input_data)\n",
    "        kurtosis_value = kurtosis(input_data)\n",
    "\n",
    "        # Storing the features in the final array\n",
    "        wavelet_feature_array[i, j, :] = [mean, variance, peak_frequency, peak_magnitude,skewness,  energy, kurtosis_value]\n",
    "        # print(wavelet_feature_array[i, j, :])\n",
    "# Final feature array shape\n",
    "wavelet_feature_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 5, 47, 250)\n",
      "hilbert_feature_matrix (320, 8, 30)\n"
     ]
    }
   ],
   "source": [
    "#Envelop Hilbert feature\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.signal import hilbert, find_peaks, welch\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "fs=250\n",
    "frequency_bands = {\n",
    "    'delta': (1, 4), 'theta': (4, 8),'alpha': (8, 14), 'beta': (14, 30), 'gamma': (30, 40)}    #     'theta': (4, 8),'alpha': (8, 14), 'beta': (14, 30), 'gamma': (30, 40)\n",
    "\n",
    "num_items = len(frequency_bands)\n",
    "baseline_epochs=7\n",
    "epoch_length=250\n",
    "\n",
    "#Hilbert feature\n",
    "data_s= data_T #shape: (8, 11750, 9)\n",
    "hilbert_T=[]\n",
    "for channel_idx, channel_name in enumerate(selected_columns):\n",
    "    hil_block=[]\n",
    "    for block_index in range(len(data_T)):\n",
    "        f_band_env=[]\n",
    "        for band, (lowcut, highcut) in frequency_bands.items():\n",
    "            data = data_s[block_index, :, channel_idx]\n",
    "            data_band= butter_bandpass_filter(data, lowcut, highcut, fs, order=5)\n",
    "            envelope = np.abs(hilbert(data_band))\n",
    "            \n",
    "            # Split into epochs and calculate mean of activity epochs\n",
    "            epochs = np.split(envelope,envelope.shape[0] // epoch_length)\n",
    "            # mean_activity_epoch = np.mean(epochs[baseline_epochs:], axis=0)\n",
    "            \n",
    "            f_band_env.append(epochs)       \n",
    "        hil_block.append(f_band_env)   \n",
    "    hilbert_T.append(hil_block)\n",
    "hilbert_T_np=np.array(hilbert_T)\n",
    "print(hilbert_T_np.shape) #(8, 8, 5, 47, 250)\n",
    "\n",
    "# Initialize the list for the feature matrix\n",
    "hilbert_feature_matrices = []\n",
    "\n",
    "# Iterate over blocks and the last 40 epochs\n",
    "for block in range(block_number):\n",
    "    for epoch in range(7, 47):  # Last 40 epochs\n",
    "        # Initialize a 2D array for this input's features: 8 channels x (5 frequency bands * 6 features)\n",
    "        input_features = np.zeros((8, num_items * 6))\n",
    "\n",
    "        # Iterate over each channel and frequency band\n",
    "        for channel in range(8):\n",
    "            for freq_band in range(num_items):\n",
    "                # Extract the envelope for the current channel, block, frequency band, and epoch\n",
    "                envelope = hilbert_T_np[channel, block, freq_band, epoch, :]\n",
    "\n",
    "                # Compute the features\n",
    "                mean_env = np.mean(envelope)\n",
    "                median_env = np.median(envelope)\n",
    "                std_env = np.std(envelope)\n",
    "                skewness_env = skew(envelope)\n",
    "                kurtosis_env = kurtosis(envelope)\n",
    "                energy = np.sum(envelope**2)\n",
    "\n",
    "                # Combine the features\n",
    "                features = np.array([mean_env, median_env, std_env, skewness_env, kurtosis_env, energy])\n",
    "\n",
    "                # Place the features in the corresponding location in the input_features array\n",
    "                input_features[channel, freq_band * 6:(freq_band + 1) *6] = features\n",
    "\n",
    "        # Append this input's features to the feature matrix list\n",
    "        hilbert_feature_matrices.append(input_features)\n",
    "\n",
    "# Convert the list of feature matrices to a 3D NumPy array\n",
    "hilbert_feature_matrix = np.array(hilbert_feature_matrices) # shape (320, 8, 30)\n",
    "print('hilbert_feature_matrix',hilbert_feature_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 40, 50, 8)\n"
     ]
    }
   ],
   "source": [
    "#ERP feature\n",
    "# Band Pass\n",
    "data_bp = np.copy(data_T)\n",
    "for i in range (len(scene_np)):\n",
    "    for column in range(8):\n",
    "        data_bp[i,:, column] = butter_bandpass_filter(data_bp[i,:, column], lowcut=1, highcut=4, fs=250)\n",
    "data = data_bp \n",
    "data.shape\n",
    "\n",
    "epoch_size = 250\n",
    "epochs = []\n",
    "for i in range (len(data)):\n",
    "    block_epoch=[]\n",
    "    # block_label=data[i,:,-1]\n",
    "    for start in range(1750, (data.shape[1]), epoch_size):\n",
    "        end = start + epoch_size\n",
    "        if end <= (data.shape[1]):\n",
    "            block_epoch.append(data[i, start:end, :])\n",
    "        else:\n",
    "            block_epoch.append(data[i, start:,:])\n",
    "        block_epoch_np=np.array(block_epoch)\n",
    "    block_epoch_np_reshape=block_epoch_np.reshape(block_epoch_np.shape[0], 50,5, block_epoch_np.shape[2])\n",
    "    block_epoch_np_downsample=np.mean(block_epoch_np_reshape, axis=2)\n",
    "    # print(block_epoch_np_downsample.shape)\n",
    "    block_erp_mean=np.mean(block_epoch_np_downsample, axis=0)\n",
    "    # print(block_erp_mean)\n",
    "    epoch_erp=[]\n",
    "    for j in range (len(block_epoch_np_downsample)):\n",
    "        # erpj=(2*block_epoch_np_downsample[j,:,:]+block_erp_mean)/3\n",
    "        erpj=(block_epoch_np_downsample[j,:,:])\n",
    "        epoch_erp.append(erpj)\n",
    "        epoch_erp_np=np.array(epoch_erp)\n",
    "        # print(epoch_erp_np.shape, block_label.shape)\n",
    "    # block_ERP=np.contacenate()\n",
    "    epochs.append(epoch_erp)\n",
    "    ERP_np=np.array(epochs)\n",
    "    \n",
    "ERP_np_eeg=ERP_np[:,:,:,:-1]\n",
    "\n",
    "print(ERP_np_eeg.shape) #: (24, 40, 50, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 40, 50, 8)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERP_np_eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fft import fft\n",
    "c=(50/1000)\n",
    "W_1= (int(0*c),int(50*c))\n",
    "W_2= (int(80*c),int(210*c))\n",
    "W_3= (int(240*c),int(350*c)) \n",
    "W_4= (int(400*c),int(500*c)) \n",
    "W_5= (int(520*c),int(630*c)) \n",
    "W_6= (int(650*c),int(900*c))\n",
    "W_7= (int(0*c),int(1000*c)) \n",
    "\n",
    "def extract_ERP_features(epoch):\n",
    "    def extract_window_features(window):\n",
    "        amplitude_max = np.max(window)\n",
    "        amplitude_min = np.min(window)\n",
    "        mean_amplitude = np.mean(window)\n",
    "        variance = np.var(window)\n",
    "        std_deviation = np.std(window)\n",
    "        skewness = skew(window)\n",
    "        kurt = kurtosis(window)\n",
    "        peak_to_peak_amplitude = amplitude_max - amplitude_min\n",
    "        zero_crossings = np.where(np.diff(np.sign(window)))[0].size\n",
    "        \n",
    "        # Find peaks\n",
    "        peaks, _ = find_peaks(window)\n",
    "        # print(peaks)\n",
    "        number_of_peaks = len(peaks)\n",
    "        \n",
    "        # # Calculate peak latencies\n",
    "        # latency = (peak_index / sampling_rate) * 1000\n",
    "        peak_latencies = [peak / 50 * 1000 for peak in peaks]  # Replace 'sampling_rate' with your actual rate\n",
    "        \n",
    "        # Frequency domain features (e.g., dominant frequency)\n",
    "        freq_data = fft(window)\n",
    "        dominant_frequency = np.argmax(np.abs(freq_data))\n",
    "\n",
    "        return [mean_amplitude, variance, std_deviation, \n",
    "                peak_to_peak_amplitude, zero_crossings, number_of_peaks ]\n",
    "\n",
    "    W1_region = epoch[W_1[0]:W_1[1]]\n",
    "    W2_region = epoch[W_2[0]:W_2[1]]\n",
    "    W3_region = epoch[W_3[0]:W_3[1]]\n",
    "    W4_region = epoch[W_4[0]:W_4[1]]\n",
    "    W5_region = epoch[W_5[0]:W_5[1]]\n",
    "    W6_region = epoch[W_6[0]:W_6[1]]\n",
    "    W7_region = epoch[W_7[0]:W_7[1]]\n",
    "\n",
    "    W1_features = extract_window_features(W1_region)\n",
    "    W2_features = extract_window_features(W2_region)\n",
    "    W3_features = extract_window_features(W3_region)\n",
    "    W4_features = extract_window_features(W4_region)\n",
    "    W5_features = extract_window_features(W5_region)\n",
    "    W6_features = extract_window_features(W6_region)\n",
    "    W7_features = extract_window_features(W7_region)\n",
    "\n",
    "    return W1_features + W2_features + W3_features + W4_features + W5_features + W6_features  + W7_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs, num_trials_per_run, num_samples, num_channels =  ERP_np_eeg.shape\n",
    "ERP_epoch_EEG = ERP_np_eeg.reshape(num_runs * num_trials_per_run, num_samples, num_channels)\n",
    "\n",
    "ERP_TEPM_FEATURE=[]\n",
    "for i in range(len(ERP_epoch_EEG)):\n",
    "    ERP_W=[]\n",
    "    for j in range( num_channels):\n",
    "        ERP_W_CH=extract_ERP_features(ERP_epoch_EEG[i,:,j])\n",
    "        ERP_W.append(ERP_W_CH)\n",
    "    ERP_W_np=np.array(ERP_W)\n",
    "    ERP_TEPM_FEATURE.append(ERP_W_np)\n",
    "\n",
    "ERP_TEPM_FEATURE_np=np.array(ERP_TEPM_FEATURE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 8, 79)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features = np.concatenate([hilbert_feature_matrix,wavelet_feature_array, ERP_TEPM_FEATURE_np], axis=2)  #ERP_TEPM_FEATURE_np , hilbert_feature_matrix, ERP_matrices_np,wavelet_feature_matrix\n",
    "combined_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_data.shape (320, 632)\n",
      "(320, 632) (320,)\n",
      "(320, 632) (320,)\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#Input to the classifier\n",
    "mlp_data=combined_features.reshape(combined_features.shape[0], combined_features.shape[1]*combined_features.shape[2])\n",
    "print('mlp_data.shape', mlp_data.shape)\n",
    "\n",
    "af_mlp=mlp_data\n",
    "Y_mlp=label_final_np_squeeze\n",
    "\n",
    "print(af_mlp.shape, Y_mlp.shape)\n",
    "af_mlp, Y_mlp= shuffle(af_mlp, Y_mlp)\n",
    "print(af_mlp.shape, Y_mlp.shape)\n",
    "\n",
    "########################################################################################################################################\n",
    "X_train= af_mlp\n",
    "y_train=Y_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.509375\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load('final_svc_model.joblib')\n",
    "\n",
    "\n",
    "# Assuming X_val is your validation data\n",
    "y_pred = loaded_model.predict(X_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming y_val is your actual target values for the validation data\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.509375\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Load the model\n",
    "loaded_model_rf = load('random_forest_model.joblib')\n",
    "# Assuming X_val is your validation data\n",
    "y_pred_rf = loaded_model_rf.predict(X_train)\n",
    "\n",
    "# Assuming y_val is your actual target values for the validation data\n",
    "accuracy = accuracy_score(y_train, y_pred_rf)\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
