{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\11-software\\Python3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\tnlab\\OneDrive\\Documents\\GitHub\\AlphaFold\\Neurofeedback-Based-BCI\n",
      "Patient data folder: c:\\Users\\tnlab\\OneDrive\\Documents\\GitHub\\AlphaFold\\Neurofeedback-Based-BCI\\2-Patient Data\n",
      "folders_to_use: ['Pre Evaluation']\n",
      "full_folder_path_ c:\\Users\\tnlab\\OneDrive\\Documents\\GitHub\\AlphaFold\\Neurofeedback-Based-BCI\\2-Patient Data\\mz0\\Pre Evaluation\n",
      "Reading from: c:\\Users\\tnlab\\OneDrive\\Documents\\GitHub\\AlphaFold\\Neurofeedback-Based-BCI\\2-Patient Data\\mz0\\Pre Evaluation\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "eeg_df_denoised.shape (10000, 8)\n",
      "baseline_corrected_np.shape (8, 10000, 8)\n",
      "event_np.shape (8, 11750)\n",
      "label_np.shape (8, 10000)\n",
      "EVENTS [['F']\n",
      " ['F']\n",
      " ['F']\n",
      " ...\n",
      " ['I']\n",
      " ['I']\n",
      " ['I']]\n",
      "face.shape (40000, 9)\n",
      "scene.shape (40000, 9)\n",
      "label.shape (80000,) [0 0 0 ... 1 1 1]\n",
      "score length 80000\n",
      "S_np shape (320, 250, 1)\n",
      "Mean of result list: 0.903125\n",
      "n 90\n",
      "img_file_path c:\\Users\\tnlab\\OneDrive\\Documents\\GitHub\\AlphaFold\\Neurofeedback-Based-BCI\\2-Patient Data\\mz0\\Pre Evaluation\\Score.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from numpy import unwrap, diff, abs, angle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.utils import shuffle\n",
    "import scipy\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import CubicSpline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Dense,  BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "import pywt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import spectrogram\n",
    "from mne.viz import plot_topomap\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import optuna\n",
    "from sklearn.datasets import make_classification\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from joblib import dump\n",
    "from scipy.signal import butter, filtfilt, lfilter, lfilter_zi\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "selected_columns = ['Fz', 'C3', 'Cz', 'C4', 'Pz', 'Po7', 'Oz', 'Po8']\n",
    "fs=250\n",
    "\n",
    "####################################################################################\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def denoise_data(df, col_names, n_clusters):\n",
    "    df_denoised = df.copy()\n",
    "    for col_name, k in zip(col_names, n_clusters):\n",
    "        df_denoised[col_name] = pd.to_numeric(df_denoised[col_name], errors='coerce') # Convert column to numeric format\n",
    "        X = df_denoised.select_dtypes(include=['float64', 'int64']) # Select only numeric columns\n",
    "        clf = KNeighborsRegressor(n_neighbors=k, weights='uniform') # Fit KNeighborsRegressor\n",
    "        clf.fit(X.index.values[:, np.newaxis], X[col_name])\n",
    "        y_pred = clf.predict(X.index.values[:, np.newaxis]) # Predict values \n",
    "        df_denoised[col_name] = y_pred\n",
    "    return df_denoised\n",
    "\n",
    "def z_score(df, col_names):\n",
    "    df_standard = df.copy()\n",
    "    for col in col_names:\n",
    "        df_standard[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    return df_standard\n",
    "\n",
    "def custom_detrend(df, col_names):\n",
    "    df_detrended = df.copy()\n",
    "    for col in col_names:\n",
    "        y = df_detrended[col]\n",
    "        x = np.arange(len(y))\n",
    "        p = np.polyfit(x, y, 1)\n",
    "        trend = np.polyval(p, x)\n",
    "        detrended = y - trend\n",
    "        df_detrended[col] = detrended\n",
    "    return df_detrended\n",
    "\n",
    "def preprocess(df, col_names, n_clusters):\n",
    "    df_new = df.copy()\n",
    "    df_new = denoise_data(df, col_names, n_clusters)\n",
    "    return df_new\n",
    "\n",
    "def df_to_raw(df, sfreq=250):\n",
    "    info = mne.create_info(ch_names=list(df.columns), sfreq=sfreq, ch_types=['eeg'] * df.shape[1])\n",
    "    raw = mne.io.RawArray(df.T.values * 1e-6, info)  # Converting values to Volts from microvolts for MNE\n",
    "    return raw\n",
    "\n",
    "def reject_artifacts(df, channel):\n",
    "    threshold_factor = 3\n",
    "    median = df[channel].median()\n",
    "    mad = np.median(np.abs(df[channel] - median))\n",
    "    spikes = np.abs(df[channel] - median) > threshold_factor * mad\n",
    "    x = np.arange(len(df[channel]))\n",
    "    cs = CubicSpline(x[~spikes], df[channel][~spikes]) # Interpolate using Cubic Spline\n",
    "    interpolated_values = cs(x)\n",
    "    interpolated_values[spikes] *= 0.01  # Make interpolated values 0.01 times smaller\n",
    "    # Again Check each interpolated value's difference from median and compare to the threshold\n",
    "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
    "    interpolated_values[spike_values] *= 0.01 \n",
    "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
    "    interpolated_values[spike_values] *= 0.01 \n",
    "    df[channel] = interpolated_values\n",
    "    return df\n",
    "\n",
    "#########################################################################################\n",
    "current_directory = os.getcwd()\n",
    "patient_data_folder = os.path.join(current_directory, \"2-Patient Data\")\n",
    "\n",
    "print(f\"Current directory: {current_directory}\")\n",
    "print(f\"Patient data folder: {patient_data_folder}\")\n",
    "\n",
    "folder_name = input(\"Please enter the subject name: \")\n",
    "Report_Number = input(\"Please enter the reprt number: \")\n",
    "full_folder_path = os.path.join(patient_data_folder, folder_name)\n",
    "\n",
    "# root_folder = \"2-Patient Data\"\n",
    "sub_folders = [\"Pre Evaluation\", \"Neurofeedback\", \"Post Evaluation\"]\n",
    "phase = int(input(\"Enter the phase (0, 1, 2): \"))  # Or however you get the phase value\n",
    "# Determine which sub-folders to use based on the phase\n",
    "folders_to_use = []\n",
    "if phase == 0:\n",
    "    folders_to_use = [sub_folders[0]]  # Just \"Pre Evaluation\"\n",
    "elif phase == 1:\n",
    "    folders_to_use = sub_folders[:2]  # \"Pre Evaluation\" and \"Neurofeedback\"\n",
    "elif phase == 2:\n",
    "    folders_to_use = [sub_folders[2]]  # \n",
    "print('folders_to_use:', folders_to_use)\n",
    "# Iterate over each folder to read the csv files\n",
    "\n",
    "################################################################################################\n",
    "duration = 40 \n",
    "raw=[]\n",
    "event=[]\n",
    "BP=[]\n",
    "PP=[]\n",
    "B=[]\n",
    "Human_Behavior=[]\n",
    "for folder in folders_to_use:\n",
    "    full_folder_path_ = os.path.join(full_folder_path, folder)\n",
    "    print('full_folder_path_', full_folder_path_)\n",
    "    if os.path.exists(full_folder_path_) and os.path.isdir(full_folder_path_):\n",
    "        print(f\"Reading from: {full_folder_path_}\")\n",
    "        for file_name in os.listdir(full_folder_path_):\n",
    "            if file_name.endswith('.csv') and (file_name.startswith('raw_eeg_block') or file_name.startswith('fl_')):\n",
    "                file_path = os.path.join(full_folder_path_, file_name)\n",
    "                s_temp = pd.read_csv(file_path, header=None)\n",
    "                inst = s_temp.iloc[:, 17]\n",
    "                df_temp = s_temp.iloc[:, :8]\n",
    "                raw.append(df_temp)\n",
    "                event.append(inst)\n",
    "                HB=s_temp.iloc[1750:, 17:21]\n",
    "                inst = s_temp.iloc[:, 17]\n",
    "                Human_Behavior.append(HB)\n",
    "                \n",
    "                # 1. Band Pass\n",
    "                raw_bp = np.copy(df_temp)\n",
    "                for column in range(8):\n",
    "                    raw_bp[:, column] = butter_bandpass_filter(raw_bp[:, column], lowcut=.4, highcut=40, fs=250) \n",
    "                # print('raw_bp.shape', raw_bp.shape)\n",
    "                \n",
    "                # 2. Artifact rejection\n",
    "                BP_artifact_RJ = np.copy(raw_bp)\n",
    "                for channel in range (8):\n",
    "                    BP_artifact_RJ= reject_artifacts(pd.DataFrame(BP_artifact_RJ), channel)\n",
    "                \n",
    "                # 4. Denoising and other preprocessing\n",
    "                BP_artifact_RJ.columns = selected_columns\n",
    "                eeg_df_denoised = preprocess(pd.DataFrame(BP_artifact_RJ), col_names=selected_columns, n_clusters=[10]*len(selected_columns))\n",
    "                # I preprocessed the whole data in each block, then, split it to the base and the activity signal\n",
    "                baseline=eeg_df_denoised.iloc[:1750,]\n",
    "                dd=eeg_df_denoised.iloc[1750:,]\n",
    "                print('eeg_df_denoised.shape',dd.shape)\n",
    "                # eeg_df_denoised.plot(subplots=True, figsize=(15, 10), title='Denoised EEG Data')\n",
    "                # plt.show()\n",
    "                B.append(baseline)\n",
    "                PP.append(dd)\n",
    "    else:\n",
    "        print(f\"{full_folder_path_} does not exist\")\n",
    "\n",
    "#######################################################################################################################################################\n",
    "# Define the new list to store baseline corrected data\n",
    "baseline_corrected = []\n",
    "for baseline, dd in zip(B, PP):\n",
    "    baseline_avg = baseline.mean()\n",
    "    corrected = dd -baseline_avg\n",
    "    baseline_corrected.append(corrected)\n",
    "    \n",
    "baseline_corrected_np=np.array(baseline_corrected)\n",
    "print('baseline_corrected_np.shape',baseline_corrected_np.shape)\n",
    "\n",
    "event_np=np.array(event)\n",
    "print('event_np.shape',event_np.shape)\n",
    "label_np=event_np[:,1750:]\n",
    "print('label_np.shape',label_np.shape)\n",
    "\n",
    "B_N=int(len(baseline_corrected)) #Number of blocks\n",
    "PP_NP=baseline_corrected_np #shape: (B_N, 10000, 8=Channel Numbers)\n",
    "\n",
    "EVENTS=label_np.reshape(B_N*(baseline_corrected_np.shape[1]), 1)\n",
    "print('EVENTS', EVENTS)\n",
    "denoised=PP_NP.reshape(B_N*(baseline_corrected_np.shape[1]), 8) # seprate each blocks' signal \n",
    "pp_sig_event=np.concatenate((denoised,EVENTS), axis=1) \n",
    "\n",
    "event_column_index = pp_sig_event.shape[1] - 1\n",
    "\n",
    "# Create a boolean mask where the event is not 'n'\n",
    "mask = pp_sig_event[:, event_column_index] != 'N'\n",
    "\n",
    "# Apply the mask to filter out rows with event 'n', because our focus is on the correct attention\n",
    "pp_sig_event_filtered = pp_sig_event[mask]\n",
    "pp_sig_event_no_event_column = pp_sig_event_filtered[:, :-1]\n",
    "\n",
    "\n",
    "labels=[] \n",
    "face = [] #lable=0\n",
    "scene=[]#lable=1\n",
    "base=[] # label=2\n",
    "# Aassuming correctness for the human behavior\n",
    "for i in range(len(pp_sig_event_filtered)): #len(pp_sig_event) = the whole sample points, (df_temp.shape[0]*B_N)\n",
    "    if 'M' in pp_sig_event_filtered[i, 8] or 'F' in pp_sig_event_filtered[i, 8]:\n",
    "        face.append(pp_sig_event_filtered[i])\n",
    "        labels.append(0)\n",
    "    if 'I' in pp_sig_event_filtered[i, 8] or 'O' in pp_sig_event_filtered[i, 8] or 'S' in pp_sig_event_filtered[i, 8]:\n",
    "        scene.append(pp_sig_event_filtered[i]) \n",
    "        labels.append(1)        \n",
    "face = np.array(face)\n",
    "print('face.shape', face.shape)\n",
    "scene = np.array(scene)\n",
    "print('scene.shape', scene.shape)\n",
    "labels=np.array(labels) \n",
    "print('label.shape', labels.shape, labels)\n",
    "\n",
    "###############################################################################################################\n",
    "# Score\n",
    "if phase !=  1:\n",
    "    Human_Behavior_np=np.array(Human_Behavior).reshape(B_N*(baseline_corrected_np.shape[1]), 4)\n",
    "    denoised_im_ins_HB = np.concatenate((denoised, Human_Behavior_np), axis=1)\n",
    "    SCORE = []\n",
    "    for row in denoised_im_ins_HB:\n",
    "        condition1 = (row[-4] == row[-3]) or (row[-4] == row[-2])\n",
    "        condition2 = row[-1] == 1\n",
    "        condition3 = (row[-4] != row[-3]) and (row[-4] != row[-2])\n",
    "        condition4 = row[-1] == 0\n",
    "        if (condition1 and condition2) or (condition3 and condition4):\n",
    "            SCORE.append([1])\n",
    "        else:\n",
    "            SCORE.append([0])\n",
    "    print('score length', len(SCORE))\n",
    "    #score\n",
    "    win_size = 250\n",
    "    S = []\n",
    "    for i in range(0, len(SCORE), win_size):\n",
    "        S_data = SCORE[i:i+win_size]\n",
    "        S.append(S_data)\n",
    "    # print('s lenght', len(S))\n",
    "    # print(S)\n",
    "    S_np = np.array(S)\n",
    "    print('S_np shape', S_np.shape)\n",
    "    result_list = []\n",
    "    # Iterate through the \"images\" (first dimension)\n",
    "    for i in range(S_np.shape[0]):\n",
    "        # Check if all 250 samples are 0\n",
    "        if np.all(S_np[i, :, 0] == 0):\n",
    "            result_list.append(0)\n",
    "        else:\n",
    "            result_list.append(1)\n",
    "    # print(result_list)\n",
    "    mean_value = sum(result_list) / len(result_list)\n",
    "    print(\"Mean of result list:\", mean_value)\n",
    "    percentage_of_ones = mean_value * 100\n",
    "    rounded_percentage_of_ones = round(percentage_of_ones)\n",
    "    n=str(rounded_percentage_of_ones)\n",
    "    print('n', n)\n",
    "    img=Image.new('RGB', (1000,1000), color=(73,109,137))\n",
    "    d=ImageDraw.Draw(img)\n",
    "    font_0=ImageFont.truetype(\"arial.ttf\", 500)\n",
    "    font_1=ImageFont.truetype(\"arial.ttf\", 150)\n",
    "    d.text((150,50), \"Your Score\", font=font_1, fill=(255,255,0))\n",
    "    d.text((250,250), n, font=font_0, fill=(255,255,0))\n",
    "    img_file_name = f\"Score.png\"\n",
    "    img_file_path = os.path.join(full_folder_path_, img_file_name) \n",
    "    print('img_file_path', img_file_path)\n",
    "    img.save(img_file_path, index=False)\n",
    "else:\n",
    "    print(f\"No score in phase_{phase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape (320,)\n",
      "denoised_reshaped.shape (320, 250, 8)\n",
      "mlp_data.shape (320, 2000)\n",
      "(320, 2000) (320,)\n",
      "(320, 2000) (320,)\n",
      "X_resampled_mlp.shape (320, 2000)\n",
      "Validation Accuracy: 0.6551724137931034\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#Input to the classifier\n",
    "label=labels.reshape(int(labels.shape[0]/fs), fs)\n",
    "Y=np.squeeze(label[:,0])\n",
    "print('Y.shape', Y.shape)\n",
    "denoised_reshaped = pp_sig_event_no_event_column.reshape(int(pp_sig_event_no_event_column.shape[0]/250), 250, 8)\n",
    "print('denoised_reshaped.shape',denoised_reshaped.shape)\n",
    "\n",
    "mlp_data=denoised_reshaped.reshape(denoised_reshaped.shape[0], denoised_reshaped.shape[1]*denoised_reshaped.shape[2])\n",
    "print('mlp_data.shape', mlp_data.shape)\n",
    "\n",
    "af_mlp=mlp_data\n",
    "Y_mlp=np.squeeze(label[:,0])\n",
    "print(af_mlp.shape, Y_mlp.shape)\n",
    "af_mlp, Y_mlp= shuffle(af_mlp, Y_mlp)\n",
    "print(af_mlp.shape, Y_mlp.shape)\n",
    "\n",
    "# Balance the dataset\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled_mlp, y_resampled_mlp = oversampler.fit_resample(af_mlp, Y_mlp)\n",
    "X_resampled_mlp= X_resampled_mlp.astype(np.float32)\n",
    "y_resampled_mlp = y_resampled_mlp.astype(np.int32)\n",
    "print('X_resampled_mlp.shape', X_resampled_mlp.shape)\n",
    "#Split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_mlp,y_resampled_mlp, test_size=0.1, random_state=42)\n",
    "#Split to train and validation\n",
    "X_train_mlp, X_validation_mlp, y_train_mlp, y_validation_mlp = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "########################################################################################################################################\n",
    "#  Initialize SGDClassifier\n",
    "svm_model = SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, max_iter=100)\n",
    "\n",
    "# Step 3: Train the model on the combined dataset\n",
    "svm_model.partial_fit(X_train_mlp, y_train_mlp, classes=np.unique(y_train_mlp))\n",
    "\n",
    "# Step 4: Validation (optional)\n",
    "# Evaluate the updated model on a validation set\n",
    "accuracy = svm_model.score(X_validation_mlp, y_validation_mlp)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Wrap your existing SGDClassifier with CalibratedClassifierCV\n",
    "svm_calibrated = CalibratedClassifierCV(svm_model, method='sigmoid', cv='prefit')\n",
    "\n",
    "# Train the calibration model on the validation data\n",
    "svm_calibrated.fit(X_validation_mlp, y_validation_mlp)\n",
    "\n",
    "# Now you can use predict_proba to get probability estimates\n",
    "probabilities = svm_calibrated.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55],\n",
       "       [0.45, 0.55]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
