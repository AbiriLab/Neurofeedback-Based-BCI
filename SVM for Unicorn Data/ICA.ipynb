{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vMaBF4q9pl9k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from numpy import unwrap, diff, abs, angle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import hilbert\n",
        "from sklearn.utils import shuffle\n",
        "import scipy\n",
        "from scipy.signal import butter, filtfilt, hilbert\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.interpolate import CubicSpline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Dense,  BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "import pywt\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preproccesing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "def denoise_data(df, col_names, n_clusters):\n",
        "    df_denoised = df.copy()\n",
        "    for col_name, k in zip(col_names, n_clusters):\n",
        "        df_denoised[col_name] = pd.to_numeric(df_denoised[col_name], errors='coerce') # Convert column to numeric format\n",
        "        X = df_denoised.select_dtypes(include=['float64', 'int64']) # Select only numeric columns\n",
        "        clf = KNeighborsRegressor(n_neighbors=k, weights='uniform') # Fit KNeighborsRegressor\n",
        "        clf.fit(X.index.values[:, np.newaxis], X[col_name])\n",
        "        y_pred = clf.predict(X.index.values[:, np.newaxis]) # Predict values \n",
        "        df_denoised[col_name] = y_pred\n",
        "    return df_denoised\n",
        "\n",
        "def z_score(df, col_names):\n",
        "    df_standard = df.copy()\n",
        "    for col in col_names:\n",
        "        df_standard[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "    return df_standard\n",
        "\n",
        "def custom_detrend(df, col_names):\n",
        "    df_detrended = df.copy()\n",
        "    for col in col_names:\n",
        "        y = df_detrended[col]\n",
        "        x = np.arange(len(y))\n",
        "        p = np.polyfit(x, y, 1)\n",
        "        trend = np.polyval(p, x)\n",
        "        detrended = y - trend\n",
        "        df_detrended[col] = detrended\n",
        "    return df_detrended\n",
        "\n",
        "def preprocess(df, col_names, n_clusters):\n",
        "    df_new = df.copy()\n",
        "    df_new = denoise_data(df, col_names, n_clusters)\n",
        "    # df_new = z_score(df_new, col_names)\n",
        "    # df_new = custom_detrend(df_new, col_names)\n",
        "    return df_new\n",
        "\n",
        "def df_to_raw(df, sfreq=250):\n",
        "    info = mne.create_info(ch_names=list(df.columns), sfreq=sfreq, ch_types=['eeg'] * df.shape[1])\n",
        "    raw = mne.io.RawArray(df.T.values * 1e-6, info)  # Converting values to Volts from microvolts for MNE\n",
        "    return raw\n",
        "\n",
        "def reject_artifacts(df, channel):\n",
        "    threshold_factor = 3\n",
        "    median = df[channel].median()\n",
        "    mad = np.median(np.abs(df[channel] - median))\n",
        "    spikes = np.abs(df[channel] - median) > threshold_factor * mad\n",
        "    x = np.arange(len(df[channel]))\n",
        "    cs = CubicSpline(x[~spikes], df[channel][~spikes]) # Interpolate using Cubic Spline\n",
        "    interpolated_values = cs(x)\n",
        "    interpolated_values[spikes] *= 0.1  # Make interpolated values 0.1 times smaller\n",
        "    # Check each interpolated value's difference from median and compare to the threshold\n",
        "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
        "    interpolated_values[spike_values] *= 0.01 \n",
        "    df[channel] = interpolated_values\n",
        "    return df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "from mne.viz import plot_topomap\n",
        "from scipy.signal import welch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "montage = mne.channels.make_standard_montage('standard_1020')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_name = 'i'\n",
        "selected_columns = ['Fz', 'FC1', 'FC2', 'C3', 'Cz', 'C4', 'CPz', 'Pz']\n",
        "# Define a list of colors, can be extended or modified\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange', 'purple', 'pink', 'brown', 'gray']\n",
        "duration = 40 \n",
        "raw=[]\n",
        "event=[]\n",
        "PP=[]\n",
        "BP=[]\n",
        "cleaned_data_list = []\n",
        "cleaned_smothed_list = []\n",
        "if os.path.exists(folder_name) and os.path.isdir(folder_name):\n",
        "    for file_name in os.listdir(folder_name):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_name, file_name)\n",
        "            s_temp = pd.read_csv(file_path, header=None)\n",
        "            inst = s_temp.iloc[:, 17]\n",
        "            df_temp = s_temp.iloc[:, :8]\n",
        "            # print(df_temp.shape)\n",
        "            # df_temp.plot(figsize=(10, 8))\n",
        "            # plt.show()\n",
        "            raw.append(df_temp)\n",
        "            event.append(inst)\n",
        "            \n",
        "            # 1. Band Pass\n",
        "            raw_bp = np.copy(df_temp)\n",
        "            for column in range(8):\n",
        "                raw_bp[:, column] = butter_bandpass_filter(raw_bp[:, column], lowcut=.4, highcut=40, fs=250) \n",
        "            # plt.plot(raw_bp)\n",
        "            # plt.show()\n",
        "                        \n",
        "            # 2. Create MNE Raw object\n",
        "            info = mne.create_info(ch_names=selected_columns, ch_types=['eeg']*8, sfreq=250)\n",
        "            raw_mne = mne.io.RawArray(raw_bp.T, info)\n",
        "            raw_mne.set_montage(montage)\n",
        "            times = raw_mne.times\n",
        "            \n",
        "            \n",
        "            fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "            for i, channel in enumerate(selected_columns):\n",
        "                axs[i].plot(times, raw_bp[:, i], label=channel, color=colors[i])\n",
        "                axs[i].set_title(channel)\n",
        "                axs[i].legend(loc=\"upper right\")\n",
        "            plt.xlabel('Time (s)')\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'raw_bp for Trial {file_name}', y=1.02)\n",
        "            plt.show()\n",
        "\n",
        "            # 3. Apply ICA\n",
        "            ica = ICA(n_components=8, method='infomax', fit_params=dict(extended=True), random_state=None, max_iter=800)\n",
        "            ica.fit(raw_mne, picks='eeg')\n",
        "           \n",
        "            # After getting ICA sources:\n",
        "            sources = ica.get_sources(raw_mne)\n",
        "            source_data = sources.get_data()\n",
        "            # Define the sampling frequency and parameters for the Welch method\n",
        "            \n",
        "            fs = 250  # Your data's sampling frequency\n",
        "            nperseg = fs  # 1-second window\n",
        "            noverlap = nperseg // 2  # 50% overlap\n",
        "\n",
        "            # Create a figure to encapsulate all plots for this trial/block\n",
        "            n_components = source_data.shape[0]\n",
        "            fig, axes = plt.subplots(n_components, 3, figsize=(8, n_components*1.5))\n",
        "            fig.suptitle(f'Trial_{file_name}', fontsize=10)\n",
        "\n",
        "            for i in range(n_components):\n",
        "                # Topomap\n",
        "                mne.viz.plot_topomap(ica.get_components()[:, i], ica.info, axes=axes[i, 0], cmap='jet', show=False, sphere=0.08)    \n",
        "                # Time Course\n",
        "                axes[i, 1].plot(raw_mne.times, source_data[i, :])\n",
        "                axes[i, 1].set_title(f'Component {i} Time Course')\n",
        "                axes[i, 1].set_xlabel('Time (s)')\n",
        "                axes[i, 1].set_ylabel('Amplitude')\n",
        "                # PSD\n",
        "                frequencies, psd = welch(source_data[i, :], fs=fs, nperseg=nperseg)\n",
        "                mask = (frequencies >= 0) & (frequencies <= 40)\n",
        "                psd_log = 10 * np.log10(psd[mask])\n",
        "                axes[i, 2].plot(frequencies[mask], psd_log)\n",
        "                axes[i, 2].set_title(f'Component {i} PSD')\n",
        "                axes[i, 2].set_xlabel('Frequency (Hz)')\n",
        "                axes[i, 2].set_ylabel('PSD (dB/Hz)')\n",
        "            plt.tight_layout()\n",
        "            plt.subplots_adjust(top=0.95)\n",
        "            plt.show()\n",
        "            \n",
        "            # Prompt user for components to exclude\n",
        "            exclude_components_input = input(\"Enter components to remove as comma-separated values (e.g., 0,2,5). If none, just press Enter: \")\n",
        "\n",
        "            if exclude_components_input.strip():  # Check if the input is not empty\n",
        "                exclude_components = [int(comp.strip()) for comp in exclude_components_input.split(\",\")]\n",
        "                \n",
        "                # Mark components for exclusion\n",
        "                ica.exclude = exclude_components\n",
        "                \n",
        "                # Apply ICA cleaning\n",
        "                raw_mne_clean = raw_mne.copy()\n",
        "                ica.apply(raw_mne_clean)\n",
        "            else:\n",
        "                print(\"No components excluded. Proceeding with the original data.\")\n",
        "                raw_mne_clean = raw_mne.copy()\n",
        "\n",
        "\n",
        "            # Extract the data and times from the cleaned raw object\n",
        "            clean_data = raw_mne_clean.get_data().T\n",
        "            times = raw_mne_clean.times\n",
        "\n",
        "            # Plotting the cleaned data for this trial using matplotlib\n",
        "            fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "            for i, channel in enumerate(selected_columns):\n",
        "                axs[i].plot(times, clean_data[:, i], label=channel, color=colors[i])\n",
        "                axs[i].set_title(channel)\n",
        "                axs[i].legend(loc=\"upper right\")\n",
        "            plt.xlabel('Time (s)')\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'Cleaned EEG Data for Trial {file_name}', y=1.02)\n",
        "            plt.show()\n",
        "            \n",
        "            \n",
        "            # Denoise the cleaned data\n",
        "            clean_data_df = pd.DataFrame(clean_data, columns=selected_columns)\n",
        "            n_clusters = [10]*len(selected_columns)  # adjust the values based on how much smoothing you want\n",
        "            denoised_data_df = denoise_data(clean_data_df, col_names=selected_columns, n_clusters=n_clusters)\n",
        "\n",
        "            fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "            for i, channel in enumerate(selected_columns):\n",
        "                axs[i].plot(times, denoised_data_df[channel], label=channel, color=colors[i])\n",
        "                axs[i].set_title(channel)\n",
        "                axs[i].legend(loc=\"upper right\")\n",
        "            plt.xlabel('Time (s)')\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'Denoised EEG Data for Trial {file_name}', y=1.02)\n",
        "            plt.show()\n",
        "            \n",
        "            \n",
        "            # 3. Smoothing\n",
        "            BP_artifact_RJ_SM=denoised_data_df.copy()\n",
        "            window_size = 10 \n",
        "            for channel in range (8):\n",
        "                BP_artifact_RJ_SM= BP_artifact_RJ_SM.rolling(window=window_size, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
        "            \n",
        "            fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "            for i, channel in enumerate(selected_columns):\n",
        "                axs[i].plot(times, BP_artifact_RJ_SM[channel], label=channel, color=colors[i])\n",
        "                axs[i].set_title(channel)\n",
        "                axs[i].legend(loc=\"upper right\")\n",
        "            plt.xlabel('Time (s)')\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'Smothed EEG Data for Trial {file_name}', y=1.02)\n",
        "            plt.show()\n",
        "\n",
        "            cleaned_data_list.append(denoised_data_df)\n",
        "            cleaned_smothed_list.append(BP_artifact_RJ_SM)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "event (80000, 1) denoised (80000, 8) pp_sig_event (80000, 9) face (40000, 9) scene (40000, 9) labels (80000,)\n"
          ]
        }
      ],
      "source": [
        "fs=250\n",
        "B_N=int(len(cleaned_data_list)) #Number of blocks\n",
        "PP_NP=np.array(cleaned_data_list) #shape: (B_N, 10000, 8=Channel Numbers)\n",
        "event=np.array(event).reshape(B_N*(df_temp.shape[0]), 1) # df_temp.shape[0]=10000\n",
        "denoised=PP_NP.reshape(B_N*(df_temp.shape[0]), 8) # seprate each blocks' signal \n",
        "pp_sig_event=np.concatenate((denoised, event), axis=1) \n",
        "labels=[] \n",
        "face = [] #lable=0\n",
        "scene=[]#lable=1\n",
        "# Aassuming correctness for the human behavior\n",
        "for i in range(len(pp_sig_event)): #len(pp_sig_event) = the whole sample points, (df_temp.shape[0]*B_N)\n",
        "    if 'M' in pp_sig_event[i, 8] or 'F' in pp_sig_event[i, 8]:\n",
        "        face.append(pp_sig_event[i])\n",
        "        labels.append(0)\n",
        "    else:\n",
        "        scene.append(pp_sig_event[i]) \n",
        "        labels.append(1)        \n",
        "face = np.array(face)\n",
        "scene = np.array(scene)\n",
        "labels=np.array(labels) \n",
        "                 \n",
        "print('event', event.shape,  'denoised',  denoised.shape, 'pp_sig_event', pp_sig_event.shape, 'face', face.shape, 'scene', scene.shape, 'labels', labels.shape)  \n",
        "#denoised is all the denoised data with shape: (df_temp.shape[0]*B_N, 8)     \n",
        "# event is all the events with shape: (df_temp.shape[0]*B_N, 1)                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "event (80000, 1) denoised (80000, 8) pp_sig_event (80000, 9) face (40000, 9) scene (40000, 9) labels (80000,)\n"
          ]
        }
      ],
      "source": [
        "# fs=250\n",
        "# B_N=int(len(cleaned_smothed_list)) #Number of blocks\n",
        "# PP_NP=np.array(cleaned_smothed_list) #shape: (B_N, 10000, 8=Channel Numbers)\n",
        "# event=np.array(event).reshape(B_N*(df_temp.shape[0]), 1) # df_temp.shape[0]=10000\n",
        "# denoised=PP_NP.reshape(B_N*(df_temp.shape[0]), 8) # seprate each blocks' signal \n",
        "# pp_sig_event=np.concatenate((denoised, event), axis=1) \n",
        "# labels=[] \n",
        "# face = [] #lable=0\n",
        "# scene=[]#lable=1\n",
        "# # Aassuming correctness for the human behavior\n",
        "# for i in range(len(pp_sig_event)): #len(pp_sig_event) = the whole sample points, (df_temp.shape[0]*B_N)\n",
        "#     if 'M' in pp_sig_event[i, 8] or 'F' in pp_sig_event[i, 8]:\n",
        "#         face.append(pp_sig_event[i])\n",
        "#         labels.append(0)\n",
        "#     else:\n",
        "#         scene.append(pp_sig_event[i]) \n",
        "#         labels.append(1)        \n",
        "# face = np.array(face)\n",
        "# scene = np.array(scene)\n",
        "# labels=np.array(labels) \n",
        "                 \n",
        "# print('event', event.shape,  'denoised',  denoised.shape, 'pp_sig_event', pp_sig_event.shape, 'face', face.shape, 'scene', scene.shape, 'labels', labels.shape)  \n",
        "# #denoised is all the denoised data with shape: (df_temp.shape[0]*B_N, 8)     \n",
        "# # event is all the events with shape: (df_temp.shape[0]*B_N, 1)                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "label=labels.reshape(int(labels.shape[0]/fs), fs)\n",
        "Y=np.squeeze(label[:,0])\n",
        "\n",
        "frequency_bands = {\n",
        "    'delta': (0.5, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 14),\n",
        "    'beta': (14, 30),\n",
        "    'gamma': (30, 40),\n",
        "     }\n",
        "\n",
        "def apply_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered_signal = filtfilt(b, a, signal)\n",
        "    return filtered_signal\n",
        "denoised_reshaped = denoised.reshape(int(denoised.shape[0]/250), 250, 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(320, 2000)\n",
            "(320, 2000) (320,)\n"
          ]
        }
      ],
      "source": [
        "mlp_data=denoised_reshaped.reshape(denoised_reshaped.shape[0], denoised_reshaped.shape[1]*denoised_reshaped.shape[2])\n",
        "print(mlp_data.shape)\n",
        "\n",
        "af_mlp=mlp_data\n",
        "Y_mlp=np.squeeze(label[:,0])\n",
        "af_mlp, Y_mlp= shuffle(af_mlp, Y_mlp)\n",
        "print(af_mlp.shape, Y_mlp.shape)\n",
        "# Balance the dataset\n",
        "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X_resampled_mlp, y_resampled_mlp = oversampler.fit_resample(af_mlp, Y_mlp)\n",
        "X_resampled_mlp= X_resampled_mlp.astype(np.float32)\n",
        "y_resampled_mlp = y_resampled_mlp.astype(np.int32)\n",
        "\n",
        "\n",
        "X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = train_test_split(X_resampled_mlp,y_resampled_mlp, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-14 13:47:13,866] A new study created in memory with name: no-name-063d68bf-c5ef-49a3-bf66-d00746027d08\n",
            "[I 2023-10-14 13:47:14,469] Trial 0 finished with value: 0.46875 and parameters: {'n_layers': 1, 'n_units_layer0': 316, 'activation': 'identity', 'learning_rate_init': 0.011344487532493099, 'max_iter': 630}. Best is trial 0 with value: 0.46875.\n",
            "[I 2023-10-14 13:47:19,283] Trial 1 finished with value: 0.53125 and parameters: {'n_layers': 1, 'n_units_layer0': 372, 'activation': 'tanh', 'learning_rate_init': 0.008200945438368808, 'max_iter': 844}. Best is trial 1 with value: 0.53125.\n",
            "[I 2023-10-14 13:47:19,961] Trial 2 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 192, 'activation': 'relu', 'learning_rate_init': 0.0009031667720960162, 'max_iter': 728}. Best is trial 2 with value: 0.625.\n",
            "[I 2023-10-14 13:47:20,311] Trial 3 finished with value: 0.65625 and parameters: {'n_layers': 1, 'n_units_layer0': 63, 'activation': 'identity', 'learning_rate_init': 0.0018550113445802896, 'max_iter': 399}. Best is trial 3 with value: 0.65625.\n",
            "[I 2023-10-14 13:47:21,232] Trial 4 finished with value: 0.5625 and parameters: {'n_layers': 2, 'n_units_layer0': 230, 'n_units_layer1': 439, 'activation': 'relu', 'learning_rate_init': 0.002073555372446553, 'max_iter': 910}. Best is trial 3 with value: 0.65625.\n",
            "[I 2023-10-14 13:47:27,181] Trial 5 finished with value: 0.5625 and parameters: {'n_layers': 1, 'n_units_layer0': 363, 'activation': 'logistic', 'learning_rate_init': 0.007216842035297356, 'max_iter': 543}. Best is trial 3 with value: 0.65625.\n",
            "[I 2023-10-14 13:47:28,886] Trial 6 finished with value: 0.53125 and parameters: {'n_layers': 3, 'n_units_layer0': 259, 'n_units_layer1': 356, 'n_units_layer2': 279, 'activation': 'tanh', 'learning_rate_init': 0.0850196575941893, 'max_iter': 268}. Best is trial 3 with value: 0.65625.\n",
            "[I 2023-10-14 13:47:31,233] Trial 7 finished with value: 0.6875 and parameters: {'n_layers': 2, 'n_units_layer0': 336, 'n_units_layer1': 318, 'activation': 'identity', 'learning_rate_init': 0.0008796213697312635, 'max_iter': 128}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:31,956] Trial 8 finished with value: 0.5 and parameters: {'n_layers': 3, 'n_units_layer0': 79, 'n_units_layer1': 405, 'n_units_layer2': 337, 'activation': 'tanh', 'learning_rate_init': 0.010952336921264776, 'max_iter': 339}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:34,756] Trial 9 finished with value: 0.6875 and parameters: {'n_layers': 2, 'n_units_layer0': 258, 'n_units_layer1': 77, 'activation': 'tanh', 'learning_rate_init': 0.00012629201546332167, 'max_iter': 668}. Best is trial 7 with value: 0.6875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:47:36,547] Trial 10 finished with value: 0.6875 and parameters: {'n_layers': 2, 'n_units_layer0': 449, 'n_units_layer1': 178, 'activation': 'identity', 'learning_rate_init': 0.0002979885686407392, 'max_iter': 51}. Best is trial 7 with value: 0.6875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (114) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:47:38,221] Trial 11 finished with value: 0.59375 and parameters: {'n_layers': 2, 'n_units_layer0': 163, 'n_units_layer1': 22, 'activation': 'logistic', 'learning_rate_init': 0.00016501424382150574, 'max_iter': 114}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:41,966] Trial 12 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 510, 'n_units_layer1': 212, 'activation': 'identity', 'learning_rate_init': 0.00010028755057105371, 'max_iter': 704}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:43,325] Trial 13 finished with value: 0.59375 and parameters: {'n_layers': 3, 'n_units_layer0': 315, 'n_units_layer1': 66, 'n_units_layer2': 62, 'activation': 'tanh', 'learning_rate_init': 0.00047235846641923476, 'max_iter': 471}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:44,413] Trial 14 finished with value: 0.53125 and parameters: {'n_layers': 2, 'n_units_layer0': 140, 'n_units_layer1': 278, 'activation': 'tanh', 'learning_rate_init': 0.00035470736110903217, 'max_iter': 227}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:48,322] Trial 15 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 420, 'n_units_layer1': 128, 'activation': 'identity', 'learning_rate_init': 0.00012821971804322842, 'max_iter': 597}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:49,961] Trial 16 finished with value: 0.5625 and parameters: {'n_layers': 3, 'n_units_layer0': 309, 'n_units_layer1': 301, 'n_units_layer2': 503, 'activation': 'relu', 'learning_rate_init': 0.0005814166769604771, 'max_iter': 786}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:55,195] Trial 17 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 236, 'n_units_layer1': 486, 'activation': 'logistic', 'learning_rate_init': 0.00022664155763078847, 'max_iter': 993}. Best is trial 7 with value: 0.6875.\n",
            "[I 2023-10-14 13:47:57,399] Trial 18 finished with value: 0.71875 and parameters: {'n_layers': 3, 'n_units_layer0': 367, 'n_units_layer1': 331, 'n_units_layer2': 25, 'activation': 'identity', 'learning_rate_init': 0.0008301719531568065, 'max_iter': 472}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:00,134] Trial 19 finished with value: 0.59375 and parameters: {'n_layers': 3, 'n_units_layer0': 411, 'n_units_layer1': 346, 'n_units_layer2': 77, 'activation': 'identity', 'learning_rate_init': 0.001010844751307135, 'max_iter': 183}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:02,240] Trial 20 finished with value: 0.6875 and parameters: {'n_layers': 3, 'n_units_layer0': 494, 'n_units_layer1': 223, 'n_units_layer2': 173, 'activation': 'identity', 'learning_rate_init': 0.0008648493491712916, 'max_iter': 454}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:04,457] Trial 21 finished with value: 0.59375 and parameters: {'n_layers': 2, 'n_units_layer0': 364, 'n_units_layer1': 335, 'activation': 'identity', 'learning_rate_init': 0.0003004893747981654, 'max_iter': 659}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:06,802] Trial 22 finished with value: 0.5 and parameters: {'n_layers': 2, 'n_units_layer0': 288, 'n_units_layer1': 126, 'activation': 'tanh', 'learning_rate_init': 0.00021278130919831866, 'max_iter': 335}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:08,518] Trial 23 finished with value: 0.6875 and parameters: {'n_layers': 3, 'n_units_layer0': 349, 'n_units_layer1': 410, 'n_units_layer2': 35, 'activation': 'identity', 'learning_rate_init': 0.00053909739152698, 'max_iter': 499}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:15,605] Trial 24 finished with value: 0.6875 and parameters: {'n_layers': 2, 'n_units_layer0': 200, 'n_units_layer1': 255, 'activation': 'identity', 'learning_rate_init': 0.00018335356829230886, 'max_iter': 545}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:18,290] Trial 25 finished with value: 0.5625 and parameters: {'n_layers': 2, 'n_units_layer0': 271, 'n_units_layer1': 308, 'activation': 'tanh', 'learning_rate_init': 0.00010364637412411753, 'max_iter': 391}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:21,497] Trial 26 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 404, 'n_units_layer1': 371, 'activation': 'logistic', 'learning_rate_init': 0.0013265181954772135, 'max_iter': 170}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:23,160] Trial 27 finished with value: 0.625 and parameters: {'n_layers': 3, 'n_units_layer0': 456, 'n_units_layer1': 151, 'n_units_layer2': 178, 'activation': 'relu', 'learning_rate_init': 0.0005471822075238064, 'max_iter': 833}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:23,546] Trial 28 finished with value: 0.53125 and parameters: {'n_layers': 3, 'n_units_layer0': 334, 'n_units_layer1': 256, 'n_units_layer2': 399, 'activation': 'identity', 'learning_rate_init': 0.003627513788577996, 'max_iter': 735}. Best is trial 18 with value: 0.71875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:48:24,844] Trial 29 finished with value: 0.71875 and parameters: {'n_layers': 1, 'n_units_layer0': 299, 'activation': 'identity', 'learning_rate_init': 0.0031506241013169864, 'max_iter': 59}. Best is trial 18 with value: 0.71875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:48:25,022] Trial 30 finished with value: 0.65625 and parameters: {'n_layers': 1, 'n_units_layer0': 24, 'activation': 'identity', 'learning_rate_init': 0.003265445882182091, 'max_iter': 53}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:26,531] Trial 31 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 298, 'activation': 'identity', 'learning_rate_init': 0.0017623592976090117, 'max_iter': 103}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:27,510] Trial 32 finished with value: 0.65625 and parameters: {'n_layers': 1, 'n_units_layer0': 248, 'activation': 'identity', 'learning_rate_init': 0.0008058547787422947, 'max_iter': 613}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:28,898] Trial 33 finished with value: 0.65625 and parameters: {'n_layers': 1, 'n_units_layer0': 331, 'activation': 'identity', 'learning_rate_init': 0.00037940532516217364, 'max_iter': 257}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:31,188] Trial 34 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 211, 'activation': 'tanh', 'learning_rate_init': 0.0012788025329884625, 'max_iter': 152}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:33,998] Trial 35 finished with value: 0.71875 and parameters: {'n_layers': 1, 'n_units_layer0': 384, 'activation': 'identity', 'learning_rate_init': 0.0027213040011160117, 'max_iter': 312}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:35,982] Trial 36 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 388, 'activation': 'identity', 'learning_rate_init': 0.002235938462040185, 'max_iter': 317}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:38,793] Trial 37 finished with value: 0.6875 and parameters: {'n_layers': 1, 'n_units_layer0': 443, 'activation': 'identity', 'learning_rate_init': 0.003613240983512787, 'max_iter': 405}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:39,932] Trial 38 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 380, 'activation': 'relu', 'learning_rate_init': 0.002493901656759736, 'max_iter': 223}. Best is trial 18 with value: 0.71875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (116) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:48:42,786] Trial 39 finished with value: 0.6875 and parameters: {'n_layers': 1, 'n_units_layer0': 335, 'activation': 'identity', 'learning_rate_init': 0.0049964038058749076, 'max_iter': 116}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:45,237] Trial 40 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 477, 'activation': 'identity', 'learning_rate_init': 0.0016687620745542801, 'max_iter': 285}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:46,390] Trial 41 finished with value: 0.53125 and parameters: {'n_layers': 2, 'n_units_layer0': 280, 'n_units_layer1': 311, 'activation': 'tanh', 'learning_rate_init': 0.000737056046497367, 'max_iter': 556}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:48,138] Trial 42 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 357, 'n_units_layer1': 66, 'activation': 'identity', 'learning_rate_init': 0.001200031289518561, 'max_iter': 414}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:55,164] Trial 43 finished with value: 0.5625 and parameters: {'n_layers': 1, 'n_units_layer0': 389, 'activation': 'logistic', 'learning_rate_init': 0.007025974674471409, 'max_iter': 676}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:56,144] Trial 44 finished with value: 0.53125 and parameters: {'n_layers': 2, 'n_units_layer0': 176, 'n_units_layer1': 212, 'activation': 'tanh', 'learning_rate_init': 0.001602405766943718, 'max_iter': 80}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:57,978] Trial 45 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 265, 'n_units_layer1': 459, 'activation': 'identity', 'learning_rate_init': 0.0026016573812995074, 'max_iter': 134}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:48:59,317] Trial 46 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 304, 'activation': 'relu', 'learning_rate_init': 0.00026840737764999566, 'max_iter': 219}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:02,096] Trial 47 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 132, 'n_units_layer1': 368, 'activation': 'identity', 'learning_rate_init': 0.00015075178489716549, 'max_iter': 376}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:03,545] Trial 48 finished with value: 0.59375 and parameters: {'n_layers': 3, 'n_units_layer0': 433, 'n_units_layer1': 409, 'n_units_layer2': 186, 'activation': 'tanh', 'learning_rate_init': 0.000755518237357856, 'max_iter': 572}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:07,351] Trial 49 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 323, 'n_units_layer1': 444, 'activation': 'logistic', 'learning_rate_init': 0.00038437378694453837, 'max_iter': 451}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:08,792] Trial 50 finished with value: 0.6875 and parameters: {'n_layers': 1, 'n_units_layer0': 236, 'activation': 'identity', 'learning_rate_init': 0.0010762841799781642, 'max_iter': 744}. Best is trial 18 with value: 0.71875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:49:10,613] Trial 51 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 397, 'n_units_layer1': 169, 'activation': 'identity', 'learning_rate_init': 0.00018878600965762315, 'max_iter': 59}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:13,861] Trial 52 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 425, 'n_units_layer1': 83, 'activation': 'identity', 'learning_rate_init': 0.0003127788640273749, 'max_iter': 184}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:17,167] Trial 53 finished with value: 0.71875 and parameters: {'n_layers': 2, 'n_units_layer0': 462, 'n_units_layer1': 181, 'activation': 'identity', 'learning_rate_init': 0.00024381217366663618, 'max_iter': 99}. Best is trial 18 with value: 0.71875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:49:19,844] Trial 54 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 461, 'n_units_layer1': 20, 'activation': 'identity', 'learning_rate_init': 0.00013075591096399912, 'max_iter': 87}. Best is trial 18 with value: 0.71875.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (298) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-14 13:49:28,155] Trial 55 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 349, 'n_units_layer1': 232, 'activation': 'identity', 'learning_rate_init': 0.00023041757560610573, 'max_iter': 298}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:29,694] Trial 56 finished with value: 0.59375 and parameters: {'n_layers': 3, 'n_units_layer0': 370, 'n_units_layer1': 187, 'n_units_layer2': 110, 'activation': 'tanh', 'learning_rate_init': 0.00047381031922588685, 'max_iter': 204}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:31,616] Trial 57 finished with value: 0.6875 and parameters: {'n_layers': 2, 'n_units_layer0': 288, 'n_units_layer1': 98, 'activation': 'identity', 'learning_rate_init': 0.0002508534052921557, 'max_iter': 251}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:33,182] Trial 58 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 504, 'n_units_layer1': 274, 'activation': 'relu', 'learning_rate_init': 0.0006187921232242528, 'max_iter': 150}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:35,272] Trial 59 finished with value: 0.71875 and parameters: {'n_layers': 3, 'n_units_layer0': 475, 'n_units_layer1': 389, 'n_units_layer2': 250, 'activation': 'identity', 'learning_rate_init': 0.00042232602875779817, 'max_iter': 482}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:37,382] Trial 60 finished with value: 0.625 and parameters: {'n_layers': 3, 'n_units_layer0': 466, 'n_units_layer1': 390, 'n_units_layer2': 420, 'activation': 'identity', 'learning_rate_init': 0.000983035774575658, 'max_iter': 494}. Best is trial 18 with value: 0.71875.\n",
            "[I 2023-10-14 13:49:40,770] Trial 61 finished with value: 0.75 and parameters: {'n_layers': 3, 'n_units_layer0': 478, 'n_units_layer1': 332, 'n_units_layer2': 264, 'activation': 'identity', 'learning_rate_init': 0.0004262060488321976, 'max_iter': 503}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:49:42,798] Trial 62 finished with value: 0.625 and parameters: {'n_layers': 3, 'n_units_layer0': 493, 'n_units_layer1': 319, 'n_units_layer2': 271, 'activation': 'identity', 'learning_rate_init': 0.00038703332070806184, 'max_iter': 516}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:49:45,135] Trial 63 finished with value: 0.6875 and parameters: {'n_layers': 3, 'n_units_layer0': 481, 'n_units_layer1': 386, 'n_units_layer2': 225, 'activation': 'identity', 'learning_rate_init': 0.0006513554701948046, 'max_iter': 364}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:49:47,357] Trial 64 finished with value: 0.625 and parameters: {'n_layers': 3, 'n_units_layer0': 415, 'n_units_layer1': 292, 'n_units_layer2': 349, 'activation': 'identity', 'learning_rate_init': 0.0004604757941750477, 'max_iter': 452}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:49:49,440] Trial 65 finished with value: 0.6875 and parameters: {'n_layers': 3, 'n_units_layer0': 446, 'n_units_layer1': 341, 'n_units_layer2': 311, 'activation': 'identity', 'learning_rate_init': 0.0008975337923024558, 'max_iter': 580}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:49:52,057] Trial 66 finished with value: 0.65625 and parameters: {'n_layers': 3, 'n_units_layer0': 430, 'n_units_layer1': 353, 'n_units_layer2': 136, 'activation': 'identity', 'learning_rate_init': 0.0003069996241390493, 'max_iter': 434}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:49:55,022] Trial 67 finished with value: 0.65625 and parameters: {'n_layers': 3, 'n_units_layer0': 489, 'n_units_layer1': 429, 'n_units_layer2': 233, 'activation': 'identity', 'learning_rate_init': 0.0006499026015542079, 'max_iter': 526}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:49:57,340] Trial 68 finished with value: 0.65625 and parameters: {'n_layers': 3, 'n_units_layer0': 375, 'n_units_layer1': 320, 'n_units_layer2': 389, 'activation': 'identity', 'learning_rate_init': 0.001296488122343, 'max_iter': 487}. Best is trial 61 with value: 0.75.\n",
            "[I 2023-10-14 13:50:00,143] Trial 69 finished with value: 0.71875 and parameters: {'n_layers': 3, 'n_units_layer0': 472, 'n_units_layer1': 489, 'n_units_layer2': 490, 'activation': 'identity', 'learning_rate_init': 0.00045388547626737575, 'max_iter': 649}. Best is trial 61 with value: 0.75.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials:  70\n",
            "Best trial:\n",
            "  Value:  0.75\n",
            "  Params: \n",
            "    n_layers: 3\n",
            "    n_units_layer0: 478\n",
            "    n_units_layer1: 332\n",
            "    n_units_layer2: 264\n",
            "    activation: identity\n",
            "    learning_rate_init: 0.0004262060488321976\n",
            "    max_iter: 503\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Layers and neurons\n",
        "    n_layers = trial.suggest_int('n_layers', 1,3)\n",
        "    layers = []\n",
        "    for i in range(n_layers):\n",
        "        layers.append(trial.suggest_int(f'n_units_layer{i}', 16,  512))\n",
        "    \n",
        "    # Activation function\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'logistic', 'tanh', 'identity'])\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4,  1e-1, log=True)\n",
        "    \n",
        "    max_iter = trial.suggest_int('max_iter', 50, 1000)\n",
        "\n",
        "    model = MLPClassifier(hidden_layer_sizes=tuple(layers), \n",
        "                          activation=activation, \n",
        "                          learning_rate_init=learning_rate_init,\n",
        "                          max_iter=max_iter ,  # to ensure convergence in most cases\n",
        "                          random_state=42)\n",
        "\n",
        "    model.fit(X_train_mlp, y_train_mlp)\n",
        "\n",
        "    # Evaluate\n",
        "    predictions = model.predict(X_test_mlp)\n",
        "    accuracy = accuracy_score(y_test_mlp, predictions)\n",
        "    return accuracy\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=70)\n",
        "\n",
        "print('Number of finished trials: ', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: ', trial.value)\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.7743\n",
            "Test Accuracy: 0.7500\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Extract the best parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Extract individual parameters\n",
        "n_layers = best_params['n_layers']\n",
        "layers = [best_params[f'n_units_layer{i}'] for i in range(n_layers)]\n",
        "activation = best_params['activation']\n",
        "learning_rate_init = best_params['learning_rate_init']\n",
        "max_iter = best_params['max_iter']\n",
        "\n",
        "# Create the model using the best parameters\n",
        "best_model = MLPClassifier(hidden_layer_sizes=tuple(layers), \n",
        "                           activation=activation, \n",
        "                           learning_rate_init=learning_rate_init,\n",
        "                           max_iter=max_iter ,  # to ensure convergence in most cases\n",
        "                           random_state=42)\n",
        "\n",
        "# Train the model using training data\n",
        "best_model.fit(X_train_mlp, y_train_mlp)\n",
        "\n",
        "# Predict using the test data\n",
        "predictions = best_model.predict(X_test_mlp)\n",
        "\n",
        "\n",
        "# Predict using the training data\n",
        "train_predictions = best_model.predict(X_train_mlp)\n",
        "\n",
        "# Evaluate the model using training data\n",
        "train_accuracy = accuracy_score(y_train_mlp, train_predictions)\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Evaluate the model using test data\n",
        "accuracy = accuracy_score(y_test_mlp, predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 1s 16ms/step - loss: 2.8738 - accuracy: 0.5174 - val_loss: 2.1257 - val_accuracy: 0.5938\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2.7613 - accuracy: 0.5799 - val_loss: 2.5148 - val_accuracy: 0.5625\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2.5808 - accuracy: 0.5417 - val_loss: 3.4738 - val_accuracy: 0.5312\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2.2918 - accuracy: 0.6285 - val_loss: 1.7435 - val_accuracy: 0.5312\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2.2256 - accuracy: 0.5347 - val_loss: 1.5715 - val_accuracy: 0.6250\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2.2763 - accuracy: 0.5903 - val_loss: 1.5595 - val_accuracy: 0.5625\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.9234 - accuracy: 0.6215 - val_loss: 1.1799 - val_accuracy: 0.5938\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.8136 - accuracy: 0.6215 - val_loss: 1.4367 - val_accuracy: 0.5625\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 2.1039 - accuracy: 0.5903 - val_loss: 1.3317 - val_accuracy: 0.5312\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.7640 - accuracy: 0.6285 - val_loss: 1.2073 - val_accuracy: 0.5938\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.8621 - accuracy: 0.6146 - val_loss: 1.3460 - val_accuracy: 0.5625\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.9104 - accuracy: 0.6076 - val_loss: 1.5327 - val_accuracy: 0.6250\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.7324 - accuracy: 0.6354 - val_loss: 1.4952 - val_accuracy: 0.6250\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.5900 - accuracy: 0.6458 - val_loss: 1.3084 - val_accuracy: 0.6250\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4232 - accuracy: 0.6632 - val_loss: 1.3801 - val_accuracy: 0.5312\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.5507 - accuracy: 0.6076 - val_loss: 1.1475 - val_accuracy: 0.5625\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.5042 - accuracy: 0.5938 - val_loss: 0.9358 - val_accuracy: 0.6250\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.3381 - accuracy: 0.6007 - val_loss: 1.0180 - val_accuracy: 0.6250\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.4031 - accuracy: 0.6424 - val_loss: 1.1605 - val_accuracy: 0.5938\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1340 - accuracy: 0.6562 - val_loss: 1.0818 - val_accuracy: 0.5938\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.1700 - accuracy: 0.6528 - val_loss: 1.1964 - val_accuracy: 0.5625\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0448 - accuracy: 0.6667 - val_loss: 1.5501 - val_accuracy: 0.4375\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2316 - accuracy: 0.6458 - val_loss: 1.5036 - val_accuracy: 0.4688\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0049 - accuracy: 0.6632 - val_loss: 1.1314 - val_accuracy: 0.4688\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8022 - accuracy: 0.7222 - val_loss: 0.9740 - val_accuracy: 0.5000\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.2074 - accuracy: 0.6181 - val_loss: 1.1323 - val_accuracy: 0.5000\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0388 - accuracy: 0.6389 - val_loss: 1.1508 - val_accuracy: 0.5625\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.0144 - accuracy: 0.6667 - val_loss: 1.2961 - val_accuracy: 0.5625\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8558 - accuracy: 0.6840 - val_loss: 1.4115 - val_accuracy: 0.4688\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.9354 - accuracy: 0.6840 - val_loss: 1.2083 - val_accuracy: 0.5312\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0550 - accuracy: 0.6944 - val_loss: 1.0192 - val_accuracy: 0.6250\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0167 - accuracy: 0.6181 - val_loss: 1.0621 - val_accuracy: 0.6562\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7770 - accuracy: 0.6771 - val_loss: 0.9994 - val_accuracy: 0.6250\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.0548 - accuracy: 0.6458 - val_loss: 0.8360 - val_accuracy: 0.6250\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.8196 - accuracy: 0.7188 - val_loss: 0.7743 - val_accuracy: 0.5625\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7614 - accuracy: 0.6840 - val_loss: 0.8991 - val_accuracy: 0.5312\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8481 - accuracy: 0.6944 - val_loss: 0.8701 - val_accuracy: 0.5625\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.8359 - accuracy: 0.6389 - val_loss: 0.7944 - val_accuracy: 0.5625\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7128 - accuracy: 0.7188 - val_loss: 0.7856 - val_accuracy: 0.5312\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6595 - accuracy: 0.7083 - val_loss: 0.7639 - val_accuracy: 0.5625\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7177 - accuracy: 0.7222 - val_loss: 0.8130 - val_accuracy: 0.5625\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7414 - accuracy: 0.7188 - val_loss: 0.8232 - val_accuracy: 0.5625\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8019 - accuracy: 0.7014 - val_loss: 0.8858 - val_accuracy: 0.6562\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6863 - accuracy: 0.7153 - val_loss: 0.8753 - val_accuracy: 0.6875\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.8032 - accuracy: 0.7049 - val_loss: 0.8059 - val_accuracy: 0.6562\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6416 - accuracy: 0.7396 - val_loss: 0.8252 - val_accuracy: 0.6250\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.7840 - accuracy: 0.6701 - val_loss: 0.8746 - val_accuracy: 0.5625\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6946 - accuracy: 0.7326 - val_loss: 0.8519 - val_accuracy: 0.5938\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6671 - accuracy: 0.7222 - val_loss: 0.7973 - val_accuracy: 0.6562\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7301 - accuracy: 0.6840 - val_loss: 0.7875 - val_accuracy: 0.6562\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.7898 - accuracy: 0.6736 - val_loss: 0.7992 - val_accuracy: 0.6250\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6123 - accuracy: 0.7465 - val_loss: 0.7749 - val_accuracy: 0.6562\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5922 - accuracy: 0.7604 - val_loss: 0.7922 - val_accuracy: 0.5938\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6417 - accuracy: 0.7188 - val_loss: 0.7915 - val_accuracy: 0.6562\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6607 - accuracy: 0.6701 - val_loss: 0.7137 - val_accuracy: 0.6562\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6105 - accuracy: 0.7292 - val_loss: 0.7106 - val_accuracy: 0.7188\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5252 - accuracy: 0.7188 - val_loss: 0.7058 - val_accuracy: 0.6875\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5677 - accuracy: 0.7361 - val_loss: 0.7471 - val_accuracy: 0.6562\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6208 - accuracy: 0.7222 - val_loss: 0.8208 - val_accuracy: 0.6250\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6781 - accuracy: 0.7257 - val_loss: 0.8112 - val_accuracy: 0.6562\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5635 - accuracy: 0.7292 - val_loss: 0.8205 - val_accuracy: 0.6250\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5876 - accuracy: 0.7535 - val_loss: 0.7905 - val_accuracy: 0.6250\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4727 - accuracy: 0.8056 - val_loss: 0.7696 - val_accuracy: 0.6562\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5138 - accuracy: 0.7708 - val_loss: 0.7721 - val_accuracy: 0.6250\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4938 - accuracy: 0.7708 - val_loss: 0.8138 - val_accuracy: 0.6562\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5396 - accuracy: 0.7812 - val_loss: 0.8726 - val_accuracy: 0.6562\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4841 - accuracy: 0.7812 - val_loss: 0.8911 - val_accuracy: 0.6250\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5236 - accuracy: 0.7882 - val_loss: 0.8546 - val_accuracy: 0.6562\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5778 - accuracy: 0.7569 - val_loss: 0.9006 - val_accuracy: 0.6562\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4579 - accuracy: 0.7986 - val_loss: 0.9543 - val_accuracy: 0.5625\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5000 - accuracy: 0.7743 - val_loss: 0.9657 - val_accuracy: 0.6562\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4762 - accuracy: 0.7465 - val_loss: 1.0395 - val_accuracy: 0.6250\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5789 - accuracy: 0.7396 - val_loss: 1.0099 - val_accuracy: 0.6562\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5443 - accuracy: 0.7639 - val_loss: 0.8731 - val_accuracy: 0.6562\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4037 - accuracy: 0.7917 - val_loss: 0.8699 - val_accuracy: 0.5938\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5541 - accuracy: 0.7465 - val_loss: 0.9399 - val_accuracy: 0.6250\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3858 - accuracy: 0.8056 - val_loss: 0.9861 - val_accuracy: 0.6562\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4365 - accuracy: 0.7951 - val_loss: 1.0186 - val_accuracy: 0.6562\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4572 - accuracy: 0.7882 - val_loss: 1.0073 - val_accuracy: 0.6562\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4026 - accuracy: 0.8507 - val_loss: 0.9029 - val_accuracy: 0.6562\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.4980 - accuracy: 0.7882 - val_loss: 0.7956 - val_accuracy: 0.6562\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3780 - accuracy: 0.8438 - val_loss: 0.8297 - val_accuracy: 0.6562\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4866 - accuracy: 0.8125 - val_loss: 0.9509 - val_accuracy: 0.6562\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4306 - accuracy: 0.8090 - val_loss: 0.9785 - val_accuracy: 0.6250\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4505 - accuracy: 0.8125 - val_loss: 0.8426 - val_accuracy: 0.6250\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4361 - accuracy: 0.8333 - val_loss: 0.8371 - val_accuracy: 0.6562\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3499 - accuracy: 0.8472 - val_loss: 0.9440 - val_accuracy: 0.6250\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3830 - accuracy: 0.8507 - val_loss: 1.1095 - val_accuracy: 0.6250\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3936 - accuracy: 0.7743 - val_loss: 1.0330 - val_accuracy: 0.5625\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4259 - accuracy: 0.8090 - val_loss: 0.9801 - val_accuracy: 0.5625\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3759 - accuracy: 0.8264 - val_loss: 1.0208 - val_accuracy: 0.5625\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3284 - accuracy: 0.8507 - val_loss: 1.0291 - val_accuracy: 0.6250\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3292 - accuracy: 0.8542 - val_loss: 1.0351 - val_accuracy: 0.6250\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3364 - accuracy: 0.8542 - val_loss: 1.1221 - val_accuracy: 0.5625\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3820 - accuracy: 0.8507 - val_loss: 1.0411 - val_accuracy: 0.5625\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3522 - accuracy: 0.8438 - val_loss: 1.0724 - val_accuracy: 0.5938\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3300 - accuracy: 0.8542 - val_loss: 1.1224 - val_accuracy: 0.6250\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3144 - accuracy: 0.8646 - val_loss: 1.1745 - val_accuracy: 0.5938\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3436 - accuracy: 0.8785 - val_loss: 1.1616 - val_accuracy: 0.5938\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3535 - accuracy: 0.8576 - val_loss: 1.1003 - val_accuracy: 0.5938\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x21591177a90>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Given data (You'd already have this loaded)\n",
        "# X_train, y_train, X_test, y_test\n",
        "\n",
        "# Ensure your target (y_train and y_test) is properly shaped.\n",
        "# For binary classification, it should be of shape (n_samples, 1)\n",
        "# For multi-class single-label classification, it should be one-hot encoded.\n",
        "\n",
        "# Define the MLP model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer (with relu activation and input shape matching your feature count)\n",
        "model.add(Dense(256, activation='relu', input_shape=(2000,)))\n",
        "\n",
        "# Hidden layers\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "# For binary classification, use 1 neuron with sigmoid activation\n",
        "# For multi-class classification, use softmax activation and change units to number of classes\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "# For binary classification, use binary_crossentropy\n",
        "# For multi-class classification, use categorical_crossentropy\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=32, validation_data=(X_test_mlp, y_test_mlp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 98.26%\n",
            "Test Accuracy: 59.38%\n"
          ]
        }
      ],
      "source": [
        "# Continue from the previously mentioned code\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_accuracy = model.evaluate(X_train_mlp, y_train_mlp, verbose=0)\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import optuna \n",
        "# # sklearn\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, LeakyReLU\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# def objective(trial):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # Input layer\n",
        "#     n_units_i = trial.suggest_int('n_units_i', 128, 512)\n",
        "\n",
        "#     activation_choice_i = trial.suggest_categorical('activation_i', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "#     if activation_choice_i == 'leaky_relu':\n",
        "#         model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "#         model.add(LeakyReLU())\n",
        "#     else:\n",
        "#         model.add(Dense(n_units_i, activation=activation_choice_i, input_shape=(2000,)))\n",
        "\n",
        "#     # Hidden layer 1\n",
        "#     dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.1, 0.6)\n",
        "#     model.add(Dropout(dropout_rate_1))\n",
        "\n",
        "#     n_units_h1 = trial.suggest_int('n_units_h1', 32, 256)\n",
        "#     activation_choice_h1 = trial.suggest_categorical('activation_h1', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "#     if activation_choice_h1 == 'leaky_relu':\n",
        "#         model.add(Dense(n_units_h1))\n",
        "#         model.add(LeakyReLU())\n",
        "#     else:\n",
        "#         model.add(Dense(n_units_h1, activation=activation_choice_h1))\n",
        "\n",
        "#     # Hidden layer 2\n",
        "#     dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.1, 0.6)\n",
        "#     model.add(Dropout(dropout_rate_2))\n",
        "\n",
        "#     n_units_h2 = trial.suggest_int('n_units_h2', 16, 128)\n",
        "#     activation_choice_h2 = trial.suggest_categorical('activation_h2', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "#     if activation_choice_h2 == 'leaky_relu':\n",
        "#         model.add(Dense(n_units_h2))\n",
        "#         model.add(LeakyReLU())\n",
        "#     else:\n",
        "#         model.add(Dense(n_units_h2, activation=activation_choice_h2))\n",
        "\n",
        "#     # Output layer\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     # Compile the model\n",
        "#     lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
        "#     optimizer = Adam(learning_rate=lr)\n",
        "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # Train the model\n",
        "#     model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     score = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "#     return score[1]  # Return accuracy\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print('Number of finished trials: ', len(study.trials))\n",
        "# print('Best trial:')\n",
        "# trial = study.best_trial\n",
        "# print('  Value: ', trial.value)\n",
        "# print('  Params: ')\n",
        "# for key, value in trial.params.items():\n",
        "#     print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 1s 6ms/step - loss: 1.5569 - accuracy: 0.4896\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.2491 - accuracy: 0.6458\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.0832 - accuracy: 0.6806\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.7630 - accuracy: 0.7604\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.7794 - accuracy: 0.7188\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.8445 - accuracy: 0.7049\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.7021 - accuracy: 0.7708\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.7702 - accuracy: 0.7674\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.7222\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6762 - accuracy: 0.7917\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7435 - accuracy: 0.7431\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6277 - accuracy: 0.7951\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.4728 - accuracy: 0.8299\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5631 - accuracy: 0.8229\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.4255 - accuracy: 0.8368\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5247 - accuracy: 0.8368\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4870 - accuracy: 0.8160\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5443 - accuracy: 0.8056\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4566 - accuracy: 0.8160\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.3950 - accuracy: 0.8438\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3181 - accuracy: 0.8646\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3399 - accuracy: 0.8750\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.3113 - accuracy: 0.8854\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2490 - accuracy: 0.9062\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.3639 - accuracy: 0.8715\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.8750\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8576\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2839 - accuracy: 0.8993\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2445 - accuracy: 0.8854\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2152 - accuracy: 0.9236\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2458 - accuracy: 0.9132\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2472 - accuracy: 0.9167\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2144 - accuracy: 0.9306\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.1668 - accuracy: 0.9375\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.9410\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2331 - accuracy: 0.9271\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1744 - accuracy: 0.9444\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.1363 - accuracy: 0.9444\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2380 - accuracy: 0.9340\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.1845 - accuracy: 0.9375\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2298 - accuracy: 0.8958\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1452 - accuracy: 0.9444\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.1580 - accuracy: 0.9444\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.9444\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2368 - accuracy: 0.9236\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.9375\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2145 - accuracy: 0.9271\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1111 - accuracy: 0.9514\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1540 - accuracy: 0.9375\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1860 - accuracy: 0.9236\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1594 - accuracy: 0.9236\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1047 - accuracy: 0.9549\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9722\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1454 - accuracy: 0.9444\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1571 - accuracy: 0.9583\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9618\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1331 - accuracy: 0.9618\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9410\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0776 - accuracy: 0.9688\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9618\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9618\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1220 - accuracy: 0.9653\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1468 - accuracy: 0.9583\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9688\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1094 - accuracy: 0.9688\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1184 - accuracy: 0.9618\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1632 - accuracy: 0.9479\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1028 - accuracy: 0.9688\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0712 - accuracy: 0.9792\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0851 - accuracy: 0.9722\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1222 - accuracy: 0.9549\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0663 - accuracy: 0.9757\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0849 - accuracy: 0.9826\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0522 - accuracy: 0.9757\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 0.9826\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0848 - accuracy: 0.9653\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0393 - accuracy: 0.9896\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 0.9792\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.9653\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1349 - accuracy: 0.9618\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9722\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0612 - accuracy: 0.9792\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0469 - accuracy: 0.9757\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0434 - accuracy: 0.9826\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0253 - accuracy: 0.9896\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0349 - accuracy: 0.9826\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0502 - accuracy: 0.9722\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0976 - accuracy: 0.9549\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0685 - accuracy: 0.9826\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.0625 - accuracy: 0.9757\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.0714 - accuracy: 0.9826\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0510 - accuracy: 0.9861\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9826\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 0.9965\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9618\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0694 - accuracy: 0.9826\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0826 - accuracy: 0.9688\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 0.9896\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9861\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0310 - accuracy: 0.9792\n",
            "9/9 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "\n",
            "Accuracy on Training Data: 1.0000\n",
            "Test Loss: 2.1780\n",
            "Test Accuracy: 0.6562\n"
          ]
        }
      ],
      "source": [
        "# # 1. Extract the Best Parameters\n",
        "# best_params = study.best_params\n",
        "\n",
        "# # 2. Build the Model using the Best Parameters\n",
        "# model = Sequential()\n",
        "\n",
        "# # Input Layer\n",
        "# n_units_i = best_params['n_units_i']\n",
        "\n",
        "# if best_params['activation_i'] == 'leaky_relu':\n",
        "#     model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "#     model.add(LeakyReLU())\n",
        "# else:\n",
        "#     model.add(Dense(n_units_i, activation=best_params['activation_i'], input_shape=(2000,)))\n",
        "\n",
        "# # Hidden Layer 1\n",
        "# model.add(Dropout(best_params['dropout_rate_1']))\n",
        "# n_units_h1 = best_params['n_units_h1']\n",
        "\n",
        "# if best_params['activation_h1'] == 'leaky_relu':\n",
        "#     model.add(Dense(n_units_h1))\n",
        "#     model.add(LeakyReLU())\n",
        "# else:\n",
        "#     model.add(Dense(n_units_h1, activation=best_params['activation_h1']))\n",
        "\n",
        "# # Hidden Layer 2\n",
        "# model.add(Dropout(best_params['dropout_rate_2']))\n",
        "# n_units_h2 = best_params['n_units_h2']\n",
        "\n",
        "# if best_params['activation_h2'] == 'leaky_relu':\n",
        "#     model.add(Dense(n_units_h2))\n",
        "#     model.add(LeakyReLU())\n",
        "# else:\n",
        "#     model.add(Dense(n_units_h2, activation=best_params['activation_h2']))\n",
        "\n",
        "# # Output Layer\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile the Model\n",
        "# optimizer = Adam(learning_rate=best_params['lr'])\n",
        "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # 3. Train the Model on the Training Data\n",
        "# model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# # Evaluate the model on the training data\n",
        "# train_loss, train_acc = model.evaluate(X_train_mlp, y_train_mlp, verbose=1)\n",
        "# print(\"\\nAccuracy on Training Data: {:.4f}\".format(train_acc))\n",
        "\n",
        "\n",
        "# # 4. Test the Model on the Test Data\n",
        "# loss, accuracy = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "\n",
        "# print(f\"Test Loss: {loss:.4f}\")\n",
        "# print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Get the best hyperparameters from the Optuna study\n",
        "# best_params = study.best_params\n",
        "\n",
        "# # Build the model using the best hyperparameters\n",
        "# model = Sequential()\n",
        "\n",
        "# model.add(Dense(best_params['n_units_i'], activation='relu', input_shape=(2000,)))\n",
        "# model.add(Dropout(best_params['dropout_rate_1']))\n",
        "# model.add(Dense(best_params['n_units_h1'], activation='relu'))\n",
        "# model.add(Dropout(best_params['dropout_rate_2']))\n",
        "# model.add(Dense(best_params['n_units_h2'], activation='relu'))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# optimizer = Adam(learning_rate=best_params['lr'])\n",
        "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model using the training data\n",
        "# model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# # Evaluate the model on the training data\n",
        "# train_loss, train_acc = model.evaluate(X_train_mlp, y_train_mlp, verbose=1)\n",
        "# print(\"\\nAccuracy on Training Data: {:.4f}\".format(train_acc))\n",
        "\n",
        "# # Evaluate the model on the test data\n",
        "# test_loss, test_acc = model.evaluate(X_test_mlp, y_test_mlp, verbose=1)\n",
        "# print(\"\\nAccuracy on Test Data: {:.4f}\".format(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import optuna\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, LeakyReLU\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# def objective(trial):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # Input layer\n",
        "#     n_units_i = trial.suggest_int('n_units_i', 128, 512)\n",
        "#     activation_choice_i = trial.suggest_categorical('activation_i', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "#     if activation_choice_i == 'leaky_relu':\n",
        "#         model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "#         model.add(LeakyReLU())\n",
        "#     else:\n",
        "#         model.add(Dense(n_units_i, activation=activation_choice_i, input_shape=(2000,)))\n",
        "\n",
        "#     # Hidden layers\n",
        "#     n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "    \n",
        "#     for i in range(n_layers):\n",
        "#         dropout_rate = trial.suggest_float(f'dropout_rate_{i}', 0.1, 0.6)\n",
        "#         model.add(Dropout(dropout_rate))\n",
        "\n",
        "#         n_units = trial.suggest_int(f'n_units_h{i}', 16, 512)\n",
        "#         activation_choice = trial.suggest_categorical(f'activation_h{i}', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "#         if activation_choice == 'leaky_relu':\n",
        "#             model.add(Dense(n_units))\n",
        "#             model.add(LeakyReLU())\n",
        "#         else:\n",
        "#             model.add(Dense(n_units, activation=activation_choice))\n",
        "\n",
        "#     # Output layer\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     # Compile the model\n",
        "#     lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
        "#     optimizer = Adam(learning_rate=lr)\n",
        "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     # Train the model\n",
        "#     model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     score = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "#     return score[1]  # Return accuracy\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print('Number of finished trials: ', len(study.trials))\n",
        "# print('Best trial:')\n",
        "# trial = study.best_trial\n",
        "# print('  Value: ', trial.value)\n",
        "# print('  Params: ')\n",
        "# for key, value in trial.params.items():\n",
        "#     print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy with best parameters: 0.65625\n"
          ]
        }
      ],
      "source": [
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, LeakyReLU\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# def create_model(params):\n",
        "#     model = Sequential()\n",
        "\n",
        "#     # Input layer\n",
        "#     n_units_i = params['n_units_i']\n",
        "#     activation_choice_i = params['activation_i']\n",
        "\n",
        "#     if activation_choice_i == 'leaky_relu':\n",
        "#         model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "#         model.add(LeakyReLU())\n",
        "#     else:\n",
        "#         model.add(Dense(n_units_i, activation=activation_choice_i, input_shape=(2000,)))\n",
        "\n",
        "#     # Hidden layers\n",
        "#     n_layers = params['n_layers']\n",
        "    \n",
        "#     for i in range(n_layers):\n",
        "#         dropout_rate = params[f'dropout_rate_{i}']\n",
        "#         model.add(Dropout(dropout_rate))\n",
        "\n",
        "#         n_units = params[f'n_units_h{i}']\n",
        "#         activation_choice = params[f'activation_h{i}']\n",
        "\n",
        "#         if activation_choice == 'leaky_relu':\n",
        "#             model.add(Dense(n_units))\n",
        "#             model.add(LeakyReLU())\n",
        "#         else:\n",
        "#             model.add(Dense(n_units, activation=activation_choice))\n",
        "\n",
        "#     # Output layer\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     # Compile the model\n",
        "#     lr = params['lr']\n",
        "#     optimizer = Adam(learning_rate=lr)\n",
        "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "#     return model\n",
        "\n",
        "# # Get best parameters from Optuna study\n",
        "# best_params = study.best_params\n",
        "\n",
        "# # Create model using best parameters\n",
        "# best_model = create_model(best_params)\n",
        "\n",
        "# # Train the model using the best parameters\n",
        "# best_model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "\n",
        "# # Evaluate the model on test data\n",
        "# score = best_model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "# print(f\"Test accuracy with best parameters: {score[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-13 22:55:08,532] A new study created in memory with name: no-name-038a5f77-c904-4b2e-a714-cb43648500e3\n",
            "[I 2023-10-13 22:55:11,206] Trial 0 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 66, 'activation': 'logistic', 'learning_rate_init': 0.0037306755973456184, 'max_iter': 584}. Best is trial 0 with value: 0.625.\n",
            "[I 2023-10-13 22:55:15,207] Trial 1 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 296, 'n_units_layer1': 321, 'activation': 'tanh', 'learning_rate_init': 0.017100289635414368, 'max_iter': 457}. Best is trial 1 with value: 0.65625.\n",
            "[I 2023-10-13 22:55:22,395] Trial 2 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 481, 'n_units_layer1': 41, 'activation': 'logistic', 'learning_rate_init': 0.0011916172600448655, 'max_iter': 862}. Best is trial 1 with value: 0.65625.\n",
            "[I 2023-10-13 22:55:25,693] Trial 3 finished with value: 0.71875 and parameters: {'n_layers': 2, 'n_units_layer0': 165, 'n_units_layer1': 182, 'activation': 'logistic', 'learning_rate_init': 0.006321512921278149, 'max_iter': 142}. Best is trial 3 with value: 0.71875.\n",
            "[I 2023-10-13 22:55:30,761] Trial 4 finished with value: 0.59375 and parameters: {'n_layers': 1, 'n_units_layer0': 250, 'activation': 'tanh', 'learning_rate_init': 0.010645157594996912, 'max_iter': 811}. Best is trial 3 with value: 0.71875.\n",
            "[I 2023-10-13 22:55:36,379] Trial 5 finished with value: 0.59375 and parameters: {'n_layers': 1, 'n_units_layer0': 421, 'activation': 'identity', 'learning_rate_init': 0.002040777493351007, 'max_iter': 146}. Best is trial 3 with value: 0.71875.\n",
            "[I 2023-10-13 22:55:40,311] Trial 6 finished with value: 0.75 and parameters: {'n_layers': 3, 'n_units_layer0': 51, 'n_units_layer1': 495, 'n_units_layer2': 267, 'activation': 'logistic', 'learning_rate_init': 0.00457127419789652, 'max_iter': 666}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:45,619] Trial 7 finished with value: 0.625 and parameters: {'n_layers': 1, 'n_units_layer0': 429, 'activation': 'identity', 'learning_rate_init': 0.017573126860754122, 'max_iter': 494}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:46,215] Trial 8 finished with value: 0.71875 and parameters: {'n_layers': 2, 'n_units_layer0': 98, 'n_units_layer1': 363, 'activation': 'tanh', 'learning_rate_init': 0.0031837257041199044, 'max_iter': 407}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:47,546] Trial 9 finished with value: 0.625 and parameters: {'n_layers': 3, 'n_units_layer0': 224, 'n_units_layer1': 23, 'n_units_layer2': 342, 'activation': 'tanh', 'learning_rate_init': 0.008433833409291958, 'max_iter': 313}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:48,007] Trial 10 finished with value: 0.5 and parameters: {'n_layers': 3, 'n_units_layer0': 17, 'n_units_layer1': 499, 'n_units_layer2': 75, 'activation': 'relu', 'learning_rate_init': 0.06641127540945203, 'max_iter': 678}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:50,996] Trial 11 finished with value: 0.65625 and parameters: {'n_layers': 3, 'n_units_layer0': 153, 'n_units_layer1': 180, 'n_units_layer2': 209, 'activation': 'logistic', 'learning_rate_init': 0.0055266930664132265, 'max_iter': 1000}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:55,070] Trial 12 finished with value: 0.65625 and parameters: {'n_layers': 3, 'n_units_layer0': 166, 'n_units_layer1': 185, 'n_units_layer2': 493, 'activation': 'logistic', 'learning_rate_init': 0.0010982504258805858, 'max_iter': 119}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:58,232] Trial 13 finished with value: 0.65625 and parameters: {'n_layers': 2, 'n_units_layer0': 138, 'n_units_layer1': 511, 'activation': 'logistic', 'learning_rate_init': 0.004907264744605718, 'max_iter': 676}. Best is trial 6 with value: 0.75.\n",
            "[I 2023-10-13 22:55:58,721] Trial 14 finished with value: 0.8125 and parameters: {'n_layers': 2, 'n_units_layer0': 18, 'n_units_layer1': 212, 'activation': 'relu', 'learning_rate_init': 0.002351019338230882, 'max_iter': 283}. Best is trial 14 with value: 0.8125.\n",
            "[I 2023-10-13 22:55:59,258] Trial 15 finished with value: 0.6875 and parameters: {'n_layers': 3, 'n_units_layer0': 33, 'n_units_layer1': 418, 'n_units_layer2': 280, 'activation': 'relu', 'learning_rate_init': 0.002047353338586364, 'max_iter': 322}. Best is trial 14 with value: 0.8125.\n",
            "[I 2023-10-13 22:56:00,986] Trial 16 finished with value: 0.78125 and parameters: {'n_layers': 2, 'n_units_layer0': 315, 'n_units_layer1': 264, 'activation': 'relu', 'learning_rate_init': 0.0024468877496082544, 'max_iter': 274}. Best is trial 14 with value: 0.8125.\n",
            "[I 2023-10-13 22:56:03,089] Trial 17 finished with value: 0.84375 and parameters: {'n_layers': 2, 'n_units_layer0': 352, 'n_units_layer1': 279, 'activation': 'relu', 'learning_rate_init': 0.002259911046906652, 'max_iter': 255}. Best is trial 17 with value: 0.84375.\n",
            "[I 2023-10-13 22:56:04,794] Trial 18 finished with value: 0.78125 and parameters: {'n_layers': 2, 'n_units_layer0': 351, 'n_units_layer1': 254, 'activation': 'relu', 'learning_rate_init': 0.0015700028667754219, 'max_iter': 249}. Best is trial 17 with value: 0.84375.\n",
            "[I 2023-10-13 22:56:06,631] Trial 19 finished with value: 0.75 and parameters: {'n_layers': 1, 'n_units_layer0': 383, 'activation': 'relu', 'learning_rate_init': 0.0010178191508990065, 'max_iter': 198}. Best is trial 17 with value: 0.84375.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials:  20\n",
            "Best trial:\n",
            "  Value:  0.84375\n",
            "  Params: \n",
            "    n_layers: 2\n",
            "    n_units_layer0: 352\n",
            "    n_units_layer1: 279\n",
            "    activation: relu\n",
            "    learning_rate_init: 0.002259911046906652\n",
            "    max_iter: 255\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Layers and neurons\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "    layers = []\n",
        "    for i in range(n_layers):\n",
        "        layers.append(trial.suggest_int(f'n_units_layer{i}', 16, 512))\n",
        "    \n",
        "    # Activation function\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'logistic', 'tanh', 'identity'])\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-3, 1e-1, log=True)\n",
        "    \n",
        "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
        "\n",
        "    model = MLPClassifier(hidden_layer_sizes=tuple(layers), \n",
        "                          activation=activation, \n",
        "                          learning_rate_init=learning_rate_init,\n",
        "                          max_iter=max_iter ,  # to ensure convergence in most cases\n",
        "                          random_state=42)\n",
        "\n",
        "    model.fit(X_train_mlp, y_train_mlp)\n",
        "\n",
        "    # Evaluate\n",
        "    predictions = model.predict(X_test_mlp)\n",
        "    accuracy = accuracy_score(y_test_mlp, predictions)\n",
        "    return accuracy\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print('Number of finished trials: ', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: ', trial.value)\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7812\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Extract the best parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Extract individual parameters\n",
        "n_layers = best_params['n_layers']\n",
        "layers = [best_params[f'n_units_layer{i}'] for i in range(n_layers)]\n",
        "activation = best_params['activation']\n",
        "learning_rate_init = best_params['learning_rate_init']\n",
        "max_iter = best_params['max_iter']\n",
        "\n",
        "# Create the model using the best parameters\n",
        "best_model = MLPClassifier(hidden_layer_sizes=tuple(layers), \n",
        "                           activation=activation, \n",
        "                           learning_rate_init=learning_rate_init,\n",
        "                           max_iter=max_iter ,  # to ensure convergence in most cases\n",
        "                           random_state=42)\n",
        "\n",
        "# Train the model using training data\n",
        "best_model.fit(X_train_mlp, y_train_mlp)\n",
        "\n",
        "# Predict using the test data\n",
        "predictions = best_model.predict(X_test_mlp)\n",
        "\n",
        "# Evaluate the model using test data\n",
        "accuracy = accuracy_score(y_test_mlp, predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
