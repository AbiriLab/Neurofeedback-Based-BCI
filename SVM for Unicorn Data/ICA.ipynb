{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vMaBF4q9pl9k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from numpy import unwrap, diff, abs, angle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import hilbert\n",
        "from sklearn.utils import shuffle\n",
        "import scipy\n",
        "from scipy.signal import butter, filtfilt, hilbert\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.interpolate import CubicSpline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Dense,  BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "import pywt\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preproccesing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "def denoise_data(df, col_names, n_clusters):\n",
        "    df_denoised = df.copy()\n",
        "    for col_name, k in zip(col_names, n_clusters):\n",
        "        df_denoised[col_name] = pd.to_numeric(df_denoised[col_name], errors='coerce') # Convert column to numeric format\n",
        "        X = df_denoised.select_dtypes(include=['float64', 'int64']) # Select only numeric columns\n",
        "        clf = KNeighborsRegressor(n_neighbors=k, weights='uniform') # Fit KNeighborsRegressor\n",
        "        clf.fit(X.index.values[:, np.newaxis], X[col_name])\n",
        "        y_pred = clf.predict(X.index.values[:, np.newaxis]) # Predict values \n",
        "        df_denoised[col_name] = y_pred\n",
        "    return df_denoised\n",
        "\n",
        "def z_score(df, col_names):\n",
        "    df_standard = df.copy()\n",
        "    for col in col_names:\n",
        "        df_standard[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "    return df_standard\n",
        "\n",
        "def custom_detrend(df, col_names):\n",
        "    df_detrended = df.copy()\n",
        "    for col in col_names:\n",
        "        y = df_detrended[col]\n",
        "        x = np.arange(len(y))\n",
        "        p = np.polyfit(x, y, 1)\n",
        "        trend = np.polyval(p, x)\n",
        "        detrended = y - trend\n",
        "        df_detrended[col] = detrended\n",
        "    return df_detrended\n",
        "\n",
        "def preprocess(df, col_names, n_clusters):\n",
        "    df_new = df.copy()\n",
        "    df_new = denoise_data(df, col_names, n_clusters)\n",
        "    # df_new = z_score(df_new, col_names)\n",
        "    # df_new = custom_detrend(df_new, col_names)\n",
        "    return df_new\n",
        "\n",
        "def df_to_raw(df, sfreq=250):\n",
        "    info = mne.create_info(ch_names=list(df.columns), sfreq=sfreq, ch_types=['eeg'] * df.shape[1])\n",
        "    raw = mne.io.RawArray(df.T.values * 1e-6, info)  # Converting values to Volts from microvolts for MNE\n",
        "    return raw\n",
        "\n",
        "def reject_artifacts(df, channel):\n",
        "    threshold_factor = 3\n",
        "    median = df[channel].median()\n",
        "    mad = np.median(np.abs(df[channel] - median))\n",
        "    spikes = np.abs(df[channel] - median) > threshold_factor * mad\n",
        "    x = np.arange(len(df[channel]))\n",
        "    cs = CubicSpline(x[~spikes], df[channel][~spikes]) # Interpolate using Cubic Spline\n",
        "    interpolated_values = cs(x)\n",
        "    interpolated_values[spikes] *= 0.1  # Make interpolated values 0.1 times smaller\n",
        "    # Check each interpolated value's difference from median and compare to the threshold\n",
        "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
        "    interpolated_values[spike_values] *= 0.01 \n",
        "    df[channel] = interpolated_values\n",
        "    return df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "from mne.viz import plot_topomap\n",
        "from scipy.signal import welch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "montage = mne.channels.make_standard_montage('standard_1020')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# folder_name = 'i'\n",
        "# selected_columns = ['Fz', 'FC1', 'FC2', 'C3', 'Cz', 'C4', 'CPz', 'Pz']\n",
        "# duration = 40 \n",
        "# raw=[]\n",
        "# event=[]\n",
        "# PP=[]\n",
        "# BP=[]\n",
        "# cleaned_data_list = []\n",
        "# if os.path.exists(folder_name) and os.path.isdir(folder_name):\n",
        "#     for file_name in os.listdir(folder_name):\n",
        "#         if file_name.endswith('.csv'):\n",
        "#             file_path = os.path.join(folder_name, file_name)\n",
        "#             s_temp = pd.read_csv(file_path, header=None)\n",
        "#             inst = s_temp.iloc[:, 17]\n",
        "#             df_temp = s_temp.iloc[:, :8]\n",
        "#             # print(df_temp.shape)\n",
        "#             # df_temp.plot(figsize=(10, 8))\n",
        "#             # plt.show()\n",
        "#             raw.append(df_temp)\n",
        "#             event.append(inst)\n",
        "            \n",
        "#             # 1. Band Pass\n",
        "#             raw_bp = np.copy(df_temp)\n",
        "#             for column in range(8):\n",
        "#                 raw_bp[:, column] = butter_bandpass_filter(raw_bp[:, column], lowcut=.4, highcut=40, fs=250) \n",
        "#             plt.plot(raw_bp)\n",
        "#             plt.show()\n",
        "#             # print(raw_bp.shape)\n",
        "#             # raw_bp = mne.filter.filter_data(raw_bp.T, sfreq=250, l_freq=.4, h_freq=40, method='iir').T\n",
        "\n",
        "#             # # 2. Artifact rejection\n",
        "#             # BP_artifact_RJ = np.copy(raw_bp)\n",
        "#             # for channel in range (8):\n",
        "#             #     BP_artifact_RJ= reject_artifacts(pd.DataFrame(BP_artifact_RJ), channel)\n",
        "#             #     print('BP_artifact_RJ' ,type(BP_artifact_RJ))\n",
        "#             # # plt.plot(BP_artifact_RJ)\n",
        "#             # # plt.show()\n",
        "            \n",
        "#             # # 3. Smoothing\n",
        "#             # BP_artifact_RJ_SM=BP_artifact_RJ.copy()\n",
        "#             # window_size = 10 \n",
        "#             # for channel in range (8):\n",
        "#             #     # channel_data = BP_artifact_RJ_SM[channel, :]\n",
        "#             #     BP_artifact_RJ_SM= BP_artifact_RJ_SM.rolling(window=window_size, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
        "#             # # plt.plot(BP_artifact_RJ_SM)\n",
        "#             # # plt.show()\n",
        "#             # BP.append(BP_artifact_RJ_SM)\n",
        "            \n",
        "#             # 4. Denoising and other preprocessing\n",
        "#             raw_bp_df = pd.DataFrame(raw_bp, columns=selected_columns)\n",
        "#             # BP_artifact_RJ_SM.columns = selected_columns\n",
        "#             # eeg_df_denoised = preprocess(pd.DataFrame(BP_artifact_RJ_SM), col_names=selected_columns, n_clusters=[50]*len(selected_columns))\n",
        "#             eeg_df_denoised = preprocess(raw_bp_df, col_names=selected_columns, n_clusters=[10]*len(selected_columns))\n",
        "#             eeg_df_denoised.plot(subplots=True, figsize=(15, 10), title='Denoised EEG Data')\n",
        "#             plt.show()\n",
        "#             PP.append(eeg_df_denoised)\n",
        "\n",
        "#             # 2. Create MNE Raw object\n",
        "#             info = mne.create_info(ch_names=selected_columns, ch_types=['eeg']*8, sfreq=250)\n",
        "#             raw_mne = mne.io.RawArray(raw_bp.T, info)\n",
        "#             raw_mne.set_montage(montage)\n",
        "\n",
        "#             # 3. Apply ICA\n",
        "#             ica = ICA(n_components=6, method='infomax', fit_params=dict(extended=True), random_state=None, max_iter=800)\n",
        "#             ica.fit(raw_mne, picks='eeg')\n",
        "           \n",
        "#             # After getting ICA sources:\n",
        "#             sources = ica.get_sources(raw_mne)\n",
        "#             source_data = sources.get_data()\n",
        "#             # Define the sampling frequency and parameters for the Welch method\n",
        "            \n",
        "#             fs = 250  # Your data's sampling frequency\n",
        "#             nperseg = fs  # 1-second window\n",
        "#             noverlap = nperseg // 2  # 50% overlap\n",
        "\n",
        "#             # Create a figure to encapsulate all plots for this trial/block\n",
        "#             n_components = source_data.shape[0]\n",
        "#             fig, axes = plt.subplots(n_components, 3, figsize=(8, n_components*1.5))\n",
        "#             fig.suptitle(f'Trial_{file_name}', fontsize=10)\n",
        "\n",
        "#             for i in range(n_components):\n",
        "#                 # Topomap\n",
        "#                 mne.viz.plot_topomap(ica.get_components()[:, i], ica.info, axes=axes[i, 0], cmap='jet', show=False, sphere=0.08)    \n",
        "#                 # Time Course\n",
        "#                 axes[i, 1].plot(raw_mne.times, source_data[i, :])\n",
        "#                 axes[i, 1].set_title(f'Component {i} Time Course')\n",
        "#                 axes[i, 1].set_xlabel('Time (s)')\n",
        "#                 axes[i, 1].set_ylabel('Amplitude')\n",
        "#                 # PSD\n",
        "#                 frequencies, psd = welch(source_data[i, :], fs=fs, nperseg=nperseg)\n",
        "#                 mask = (frequencies >= 0) & (frequencies <= 40)\n",
        "#                 psd_log = 10 * np.log10(psd[mask])\n",
        "#                 axes[i, 2].plot(frequencies[mask], psd_log)\n",
        "#                 axes[i, 2].set_title(f'Component {i} PSD')\n",
        "#                 axes[i, 2].set_xlabel('Frequency (Hz)')\n",
        "#                 axes[i, 2].set_ylabel('PSD (dB/Hz)')\n",
        "#             # Adjust layout for better visualization\n",
        "#             plt.tight_layout()\n",
        "#             plt.subplots_adjust(top=0.95)\n",
        "#             plt.show()\n",
        "            \n",
        "#             # Prompt user for components to exclude\n",
        "#             exclude_components_input = input(\"Enter components to remove as comma-separated values (e.g., 0,2,5). If none, just press Enter: \")\n",
        "\n",
        "#             if exclude_components_input.strip():  # Check if the input is not empty\n",
        "#                 exclude_components = [int(comp.strip()) for comp in exclude_components_input.split(\",\")]\n",
        "                \n",
        "#                 # Mark components for exclusion\n",
        "#                 ica.exclude = exclude_components\n",
        "                \n",
        "#                 # Apply ICA cleaning\n",
        "#                 raw_mne_clean = raw_mne.copy()\n",
        "#                 ica.apply(raw_mne_clean)\n",
        "#             else:\n",
        "#                 print(\"No components excluded. Proceeding with the original data.\")\n",
        "#                 raw_mne_clean = raw_mne.copy()\n",
        "\n",
        "\n",
        "#             # Extract the data and times from the cleaned raw object\n",
        "#             clean_data = raw_mne_clean.get_data().T\n",
        "#             times = raw_mne_clean.times\n",
        "\n",
        "#             # Plotting the cleaned data for this trial using matplotlib\n",
        "#             fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "#             for i, channel in enumerate(selected_columns):\n",
        "#                 axs[i].plot(times, clean_data[:, i], label=channel)\n",
        "#                 axs[i].set_title(channel)\n",
        "#                 axs[i].legend(loc=\"upper right\")\n",
        "#             plt.xlabel('Time (s)')\n",
        "#             plt.tight_layout()\n",
        "#             plt.suptitle(f'Cleaned EEG Data for Trial {file_name}', y=1.02)\n",
        "#             plt.show()\n",
        "\n",
        "\n",
        "#             # Append the cleaned data to a list (assuming you have a list to append to)\n",
        "#             cleaned_data_list.append(raw_mne_clean)  # where cleaned_data_list is a list you've initialized before your loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "            # # 4. Denoising and other preprocessing\n",
        "            # raw_bp_df = pd.DataFrame(raw_bp, columns=selected_columns)\n",
        "            # # BP_artifact_RJ_SM.columns = selected_columns\n",
        "            # # eeg_df_denoised = preprocess(pd.DataFrame(BP_artifact_RJ_SM), col_names=selected_columns, n_clusters=[50]*len(selected_columns))\n",
        "            # eeg_df_denoised = preprocess(raw_bp_df, col_names=selected_columns, n_clusters=[10]*len(selected_columns))\n",
        "            # eeg_df_denoised.plot(subplots=True, figsize=(15, 10), title='Denoised EEG Data')\n",
        "            # plt.show()\n",
        "            # PP.append(eeg_df_denoised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_name = 'i'\n",
        "selected_columns = ['Fz', 'FC1', 'FC2', 'C3', 'Cz', 'C4', 'CPz', 'Pz']\n",
        "# Define a list of colors, can be extended or modified\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange', 'purple', 'pink', 'brown', 'gray']\n",
        "duration = 40 \n",
        "raw=[]\n",
        "event=[]\n",
        "PP=[]\n",
        "BP=[]\n",
        "cleaned_data_list = []\n",
        "if os.path.exists(folder_name) and os.path.isdir(folder_name):\n",
        "    for file_name in os.listdir(folder_name):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_name, file_name)\n",
        "            s_temp = pd.read_csv(file_path, header=None)\n",
        "            inst = s_temp.iloc[:, 17]\n",
        "            df_temp = s_temp.iloc[:, :8]\n",
        "            # print(df_temp.shape)\n",
        "            # df_temp.plot(figsize=(10, 8))\n",
        "            # plt.show()\n",
        "            raw.append(df_temp)\n",
        "            event.append(inst)\n",
        "            \n",
        "            # 1. Band Pass\n",
        "            raw_bp = np.copy(df_temp)\n",
        "            for column in range(8):\n",
        "                raw_bp[:, column] = butter_bandpass_filter(raw_bp[:, column], lowcut=.4, highcut=40, fs=250) \n",
        "            # plt.plot(raw_bp)\n",
        "            # plt.show()\n",
        "            \n",
        "                        \n",
        "            fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "            for i, channel in enumerate(selected_columns):\n",
        "                axs[i].plot(times, raw_bp[:, i], label=channel, color=colors[i])\n",
        "                axs[i].set_title(channel)\n",
        "                axs[i].legend(loc=\"upper right\")\n",
        "            plt.xlabel('Time (s)')\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'raw_bp for Trial {file_name}', y=1.02)\n",
        "            plt.show()\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "            # 2. Create MNE Raw object\n",
        "            info = mne.create_info(ch_names=selected_columns, ch_types=['eeg']*8, sfreq=250)\n",
        "            raw_mne = mne.io.RawArray(raw_bp.T, info)\n",
        "            raw_mne.set_montage(montage)\n",
        "\n",
        "            # 3. Apply ICA\n",
        "            ica = ICA(n_components=6, method='infomax', fit_params=dict(extended=True), random_state=None, max_iter=800)\n",
        "            ica.fit(raw_mne, picks='eeg')\n",
        "           \n",
        "            # After getting ICA sources:\n",
        "            sources = ica.get_sources(raw_mne)\n",
        "            source_data = sources.get_data()\n",
        "            # Define the sampling frequency and parameters for the Welch method\n",
        "            \n",
        "            fs = 250  # Your data's sampling frequency\n",
        "            nperseg = fs  # 1-second window\n",
        "            noverlap = nperseg // 2  # 50% overlap\n",
        "\n",
        "            # Create a figure to encapsulate all plots for this trial/block\n",
        "            n_components = source_data.shape[0]\n",
        "            fig, axes = plt.subplots(n_components, 3, figsize=(8, n_components*1.5))\n",
        "            fig.suptitle(f'Trial_{file_name}', fontsize=10)\n",
        "\n",
        "            for i in range(n_components):\n",
        "                # Topomap\n",
        "                mne.viz.plot_topomap(ica.get_components()[:, i], ica.info, axes=axes[i, 0], cmap='jet', show=False, sphere=0.08)    \n",
        "                # Time Course\n",
        "                axes[i, 1].plot(raw_mne.times, source_data[i, :])\n",
        "                axes[i, 1].set_title(f'Component {i} Time Course')\n",
        "                axes[i, 1].set_xlabel('Time (s)')\n",
        "                axes[i, 1].set_ylabel('Amplitude')\n",
        "                # PSD\n",
        "                frequencies, psd = welch(source_data[i, :], fs=fs, nperseg=nperseg)\n",
        "                mask = (frequencies >= 0) & (frequencies <= 40)\n",
        "                psd_log = 10 * np.log10(psd[mask])\n",
        "                axes[i, 2].plot(frequencies[mask], psd_log)\n",
        "                axes[i, 2].set_title(f'Component {i} PSD')\n",
        "                axes[i, 2].set_xlabel('Frequency (Hz)')\n",
        "                axes[i, 2].set_ylabel('PSD (dB/Hz)')\n",
        "            plt.tight_layout()\n",
        "            plt.subplots_adjust(top=0.95)\n",
        "            plt.show()\n",
        "            \n",
        "            # Prompt user for components to exclude\n",
        "            exclude_components_input = input(\"Enter components to remove as comma-separated values (e.g., 0,2,5). If none, just press Enter: \")\n",
        "\n",
        "            if exclude_components_input.strip():  # Check if the input is not empty\n",
        "                exclude_components = [int(comp.strip()) for comp in exclude_components_input.split(\",\")]\n",
        "                \n",
        "                # Mark components for exclusion\n",
        "                ica.exclude = exclude_components\n",
        "                \n",
        "                # Apply ICA cleaning\n",
        "                raw_mne_clean = raw_mne.copy()\n",
        "                ica.apply(raw_mne_clean)\n",
        "            else:\n",
        "                print(\"No components excluded. Proceeding with the original data.\")\n",
        "                raw_mne_clean = raw_mne.copy()\n",
        "\n",
        "\n",
        "            # Extract the data and times from the cleaned raw object\n",
        "            clean_data = raw_mne_clean.get_data().T\n",
        "            times = raw_mne_clean.times\n",
        "\n",
        "            # Plotting the cleaned data for this trial using matplotlib\n",
        "            fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "            for i, channel in enumerate(selected_columns):\n",
        "                axs[i].plot(times, clean_data[:, i], label=channel, color=colors[i])\n",
        "                axs[i].set_title(channel)\n",
        "                axs[i].legend(loc=\"upper right\")\n",
        "            plt.xlabel('Time (s)')\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'Cleaned EEG Data for Trial {file_name}', y=1.02)\n",
        "            plt.show()\n",
        "            \n",
        "            \n",
        "            # Denoise the cleaned data\n",
        "            clean_data_df = pd.DataFrame(clean_data, columns=selected_columns)\n",
        "            n_clusters = [10]*len(selected_columns)  # adjust the values based on how much smoothing you want\n",
        "            denoised_data_df = denoise_data(clean_data_df, col_names=selected_columns, n_clusters=n_clusters)\n",
        "\n",
        "\n",
        "\n",
        "            fig, axs = plt.subplots(len(selected_columns), 1, figsize=(15, 10), sharex=True)\n",
        "            for i, channel in enumerate(selected_columns):\n",
        "                axs[i].plot(times, denoised_data_df[channel], label=channel, color=colors[i])\n",
        "                axs[i].set_title(channel)\n",
        "                axs[i].legend(loc=\"upper right\")\n",
        "            plt.xlabel('Time (s)')\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f'denoised EEG Data for Trial {file_name}', y=1.02)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "            # # Append the cleaned data to a list (assuming you have a list to append to)\n",
        "            # cleaned_data_list.append(raw_mne_clean)  # where cleaned_data_list is a list you've initialized before your loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fs=250\n",
        "B_N=int(len(PP)) #Number of blocks\n",
        "PP_NP=np.array(PP) #shape: (B_N, 10000, 8=Channel Numbers)\n",
        "event=np.array(event).reshape(B_N*(df_temp.shape[0]), 1) # df_temp.shape[0]=10000\n",
        "denoised=PP_NP.reshape(B_N*(df_temp.shape[0]), 8) # seprate each blocks' signal \n",
        "pp_sig_event=np.concatenate((denoised, event), axis=1) \n",
        "labels=[] \n",
        "face = [] #lable=0\n",
        "scene=[]#lable=1\n",
        "# Aassuming correctness for the human behavior\n",
        "for i in range(len(pp_sig_event)): #len(pp_sig_event) = the whole sample points, (df_temp.shape[0]*B_N)\n",
        "    if 'M' in pp_sig_event[i, 8] or 'F' in pp_sig_event[i, 8]:\n",
        "        face.append(pp_sig_event[i])\n",
        "        labels.append(0)\n",
        "    else:\n",
        "        scene.append(pp_sig_event[i]) \n",
        "        labels.append(1)        \n",
        "face = np.array(face)\n",
        "scene = np.array(scene)\n",
        "labels=np.array(labels) \n",
        "                 \n",
        "print('event', event.shape,  'denoised',  denoised.shape, 'pp_sig_event', pp_sig_event.shape, 'face', face.shape, 'scene', scene.shape, 'labels', labels.shape)  \n",
        "#denoised is all the denoised data with shape: (df_temp.shape[0]*B_N, 8)     \n",
        "# event is all the events with shape: (df_temp.shape[0]*B_N, 1)                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label=labels.reshape(int(labels.shape[0]/fs), fs)\n",
        "Y=np.squeeze(label[:,0])\n",
        "\n",
        "frequency_bands = {\n",
        "    'delta': (0.5, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 14),\n",
        "    'beta': (14, 30),\n",
        "    'gamma': (30, 40),\n",
        "     }\n",
        "\n",
        "def apply_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered_signal = filtfilt(b, a, signal)\n",
        "    return filtered_signal\n",
        "denoised_reshaped = denoised.reshape(int(denoised.shape[0]/250), 250, 8)\n",
        "\n",
        "# def extract_statistical_features_from_amplitude(coefficients):\n",
        "#     \"\"\"Extract statistical features from the amplitude of wavelet coefficients.\"\"\"\n",
        "#     amplitude = np.abs(coefficients.flatten())  # Get the amplitude and flatten\n",
        "#     # Extract features from amplitude\n",
        "#     mean_amp = np.mean(amplitude)\n",
        "#     variance_amp = np.var(amplitude)\n",
        "#     skewness_amp = skew(amplitude)\n",
        "#     kurtosis_amp = kurtosis(amplitude)\n",
        "#     return [mean_amp, variance_amp, skewness_amp, kurtosis_amp]\n",
        "\n",
        "# features_combined = []\n",
        "# for segment in denoised_reshaped:\n",
        "#     segment_features = []\n",
        "#     for channel_data in segment.T:  # Going through each channel\n",
        "#         # For each frequency band, apply the bandpass filter and then extract wavelet features\n",
        "#         for band, (low, high) in frequency_bands.items():\n",
        "#             filtered_signal = apply_bandpass_filter(channel_data, low, high, fs)\n",
        "#             coefficients, frequencies = pywt.cwt(filtered_signal, scales=np.arange(1, 50), wavelet='cmor')\n",
        "#             # Extract statistical features from the amplitude\n",
        "#             amp_features = extract_statistical_features_from_amplitude(coefficients)\n",
        "#             segment_features.extend(amp_features)            \n",
        "#     features_combined.append(segment_features)\n",
        "# features_combined_np = np.array(features_combined)\n",
        "# print(features_combined_np.shape)\n",
        "# wavelet_np=features_combined_np.reshape(features_combined_np.shape[0], 8, int(features_combined_np.shape[1]/8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wavelet combined with frequency bands\n",
        "features_combined = []\n",
        "\n",
        "for segment in denoised_reshaped:\n",
        "    segment_features = []\n",
        "    for channel_data in segment.T:  # Going through each channel\n",
        "        \n",
        "        # For each frequency band, apply the bandpass filter and then extract wavelet features\n",
        "        for band, (low, high) in frequency_bands.items():\n",
        "            filtered_signal = apply_bandpass_filter(channel_data, low, high, fs)\n",
        "            coeffs = pywt.wavedec(filtered_signal, 'db4', level=4)\n",
        "            coeff_array = np.concatenate(coeffs)\n",
        "            amplitude = np.abs(coeff_array)\n",
        "            for coeff in coeffs:\n",
        "                segment_features.extend([np.mean(amplitude), np.var(amplitude), skew(amplitude), kurtosis(amplitude)])\n",
        "                \n",
        "    features_combined.append(segment_features)    \n",
        "features_combined_np = np.array(features_combined)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wavelet_np=features_combined_np.reshape(features_combined_np.shape[0], 8, int(features_combined_np.shape[1]/8))\n",
        "\n",
        "# combined_features = np.concatenate([wavelet_np], axis=2)\n",
        "print(features_combined_np.shape)  # Should print (1600, 8, 11)\n",
        "# af=features_combined_np.reshape(int(features_combined_np.shape[0]), int(8*features_combined_np.shape[2]))\n",
        "af=features_combined_np\n",
        "af, Y = shuffle(af, Y)\n",
        "print(af.shape, Y.shape)\n",
        "# Balance the dataset\n",
        "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(af, Y)\n",
        "X_resampled= X_resampled.astype(np.float32)\n",
        "y_resampled = y_resampled.astype(np.int32)\n",
        "\n",
        "# Split X and y into training and testing sets\n",
        "# X_touched, X_untouch, y_touch, y_untouch = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert y_train and y_test to categorical format for Keras\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
        "# y_untouch=tf.keras.utils.to_categorical(y_untouch, num_classes=2)\n",
        "\n",
        "\n",
        "# Convert the data to a numerical type (float)\n",
        "X_train = X_train.astype(np.float64)\n",
        "print(X_train.shape)\n",
        "\n",
        "# Convert one-hot-encoded labels to integer-encoded labels\n",
        "y_train = np.argmax(y_train, axis=-1)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "# y_untouch = np.argmax(y_untouch, axis=-1)\n",
        "print(y_train.shape, y_test.shape)\n",
        "\n",
        "\n",
        "print('X_train:', X_train.shape, 'y_train:', y_train.shape, 'X_test:', X_test.shape, 'y_test:',\n",
        "      y_test.shape, 'X_untouch:', 'y_untouch:' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp_data=denoised_reshaped.reshape(denoised_reshaped.shape[0], denoised_reshaped.shape[1]*denoised_reshaped.shape[2])\n",
        "print(mlp_data.shape)\n",
        "\n",
        "af_mlp=mlp_data\n",
        "Y_mlp=np.squeeze(label[:,0])\n",
        "af_mlp, Y_mlp= shuffle(af_mlp, Y_mlp)\n",
        "print(af_mlp.shape, Y_mlp.shape)\n",
        "# Balance the dataset\n",
        "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X_resampled_mlp, y_resampled_mlp = oversampler.fit_resample(af_mlp, Y_mlp)\n",
        "X_resampled_mlp= X_resampled_mlp.astype(np.float32)\n",
        "y_resampled_mlp = y_resampled_mlp.astype(np.int32)\n",
        "\n",
        "\n",
        "X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = train_test_split(X_resampled_mlp,y_resampled_mlp, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Given data (You'd already have this loaded)\n",
        "# X_train, y_train, X_test, y_test\n",
        "\n",
        "# Ensure your target (y_train and y_test) is properly shaped.\n",
        "# For binary classification, it should be of shape (n_samples, 1)\n",
        "# For multi-class single-label classification, it should be one-hot encoded.\n",
        "\n",
        "# Define the MLP model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer (with relu activation and input shape matching your feature count)\n",
        "model.add(Dense(256, activation='relu', input_shape=(2000,)))\n",
        "\n",
        "# Hidden layers\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "# For binary classification, use 1 neuron with sigmoid activation\n",
        "# For multi-class classification, use softmax activation and change units to number of classes\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "# For binary classification, use binary_crossentropy\n",
        "# For multi-class classification, use categorical_crossentropy\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=32, validation_data=(X_test_mlp, y_test_mlp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue from the previously mentioned code\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_accuracy = model.evaluate(X_train_mlp, y_train_mlp, verbose=0)\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def objective(trial):\n",
        "    model = Sequential()\n",
        "    # Input layer\n",
        "    n_units_i = trial.suggest_int('n_units_i', 128, 512)\n",
        "    model.add(Dense(n_units_i, activation='relu', input_shape=(2000,)))\n",
        "    # Hidden layers\n",
        "    dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_1))\n",
        "    n_units_h1 = trial.suggest_int('n_units_h1', 32, 256)\n",
        "    model.add(Dense(n_units_h1, activation='relu'))\n",
        "    dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_2))\n",
        "    n_units_h2 = trial.suggest_int('n_units_h2', 16, 128)\n",
        "    model.add(Dense(n_units_h2, activation='relu'))\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile the model\n",
        "    lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    # Train the model\n",
        "    model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "    # Evaluate the model\n",
        "    score = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "    return score[1]  # Return accuracy\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "print('Number of finished trials: ', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: ', trial.value)\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna \n",
        "# sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def objective(trial):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    n_units_i = trial.suggest_int('n_units_i', 128, 512)\n",
        "\n",
        "    activation_choice_i = trial.suggest_categorical('activation_i', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "    if activation_choice_i == 'leaky_relu':\n",
        "        model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "        model.add(LeakyReLU())\n",
        "    else:\n",
        "        model.add(Dense(n_units_i, activation=activation_choice_i, input_shape=(2000,)))\n",
        "\n",
        "    # Hidden layer 1\n",
        "    dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_1))\n",
        "\n",
        "    n_units_h1 = trial.suggest_int('n_units_h1', 32, 256)\n",
        "    activation_choice_h1 = trial.suggest_categorical('activation_h1', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "    if activation_choice_h1 == 'leaky_relu':\n",
        "        model.add(Dense(n_units_h1))\n",
        "        model.add(LeakyReLU())\n",
        "    else:\n",
        "        model.add(Dense(n_units_h1, activation=activation_choice_h1))\n",
        "\n",
        "    # Hidden layer 2\n",
        "    dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_2))\n",
        "\n",
        "    n_units_h2 = trial.suggest_int('n_units_h2', 16, 128)\n",
        "    activation_choice_h2 = trial.suggest_categorical('activation_h2', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "    if activation_choice_h2 == 'leaky_relu':\n",
        "        model.add(Dense(n_units_h2))\n",
        "        model.add(LeakyReLU())\n",
        "    else:\n",
        "        model.add(Dense(n_units_h2, activation=activation_choice_h2))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    score = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "    return score[1]  # Return accuracy\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print('Number of finished trials: ', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: ', trial.value)\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Extract the Best Parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# 2. Build the Model using the Best Parameters\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "n_units_i = best_params['n_units_i']\n",
        "\n",
        "if best_params['activation_i'] == 'leaky_relu':\n",
        "    model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "    model.add(LeakyReLU())\n",
        "else:\n",
        "    model.add(Dense(n_units_i, activation=best_params['activation_i'], input_shape=(2000,)))\n",
        "\n",
        "# Hidden Layer 1\n",
        "model.add(Dropout(best_params['dropout_rate_1']))\n",
        "n_units_h1 = best_params['n_units_h1']\n",
        "\n",
        "if best_params['activation_h1'] == 'leaky_relu':\n",
        "    model.add(Dense(n_units_h1))\n",
        "    model.add(LeakyReLU())\n",
        "else:\n",
        "    model.add(Dense(n_units_h1, activation=best_params['activation_h1']))\n",
        "\n",
        "# Hidden Layer 2\n",
        "model.add(Dropout(best_params['dropout_rate_2']))\n",
        "n_units_h2 = best_params['n_units_h2']\n",
        "\n",
        "if best_params['activation_h2'] == 'leaky_relu':\n",
        "    model.add(Dense(n_units_h2))\n",
        "    model.add(LeakyReLU())\n",
        "else:\n",
        "    model.add(Dense(n_units_h2, activation=best_params['activation_h2']))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the Model\n",
        "optimizer = Adam(learning_rate=best_params['lr'])\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the Model on the Training Data\n",
        "model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "train_loss, train_acc = model.evaluate(X_train_mlp, y_train_mlp, verbose=1)\n",
        "print(\"\\nAccuracy on Training Data: {:.4f}\".format(train_acc))\n",
        "\n",
        "\n",
        "# 4. Test the Model on the Test Data\n",
        "loss, accuracy = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the best hyperparameters from the Optuna study\n",
        "best_params = study.best_params\n",
        "\n",
        "# Build the model using the best hyperparameters\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(best_params['n_units_i'], activation='relu', input_shape=(2000,)))\n",
        "model.add(Dropout(best_params['dropout_rate_1']))\n",
        "model.add(Dense(best_params['n_units_h1'], activation='relu'))\n",
        "model.add(Dropout(best_params['dropout_rate_2']))\n",
        "model.add(Dense(best_params['n_units_h2'], activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = Adam(learning_rate=best_params['lr'])\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "train_loss, train_acc = model.evaluate(X_train_mlp, y_train_mlp, verbose=1)\n",
        "print(\"\\nAccuracy on Training Data: {:.4f}\".format(train_acc))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(X_test_mlp, y_test_mlp, verbose=1)\n",
        "print(\"\\nAccuracy on Test Data: {:.4f}\".format(test_acc))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
