{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vMaBF4q9pl9k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from numpy import unwrap, diff, abs, angle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import hilbert\n",
        "from sklearn.utils import shuffle\n",
        "import scipy\n",
        "from scipy.signal import butter, filtfilt, hilbert\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.interpolate import CubicSpline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Dense,  BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "import pywt\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preproccesing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "def denoise_data(df, col_names, n_clusters):\n",
        "    df_denoised = df.copy()\n",
        "    for col_name, k in zip(col_names, n_clusters):\n",
        "        df_denoised[col_name] = pd.to_numeric(df_denoised[col_name], errors='coerce') # Convert column to numeric format\n",
        "        X = df_denoised.select_dtypes(include=['float64', 'int64']) # Select only numeric columns\n",
        "        clf = KNeighborsRegressor(n_neighbors=k, weights='uniform') # Fit KNeighborsRegressor\n",
        "        clf.fit(X.index.values[:, np.newaxis], X[col_name])\n",
        "        y_pred = clf.predict(X.index.values[:, np.newaxis]) # Predict values \n",
        "        df_denoised[col_name] = y_pred\n",
        "    return df_denoised\n",
        "\n",
        "def z_score(df, col_names):\n",
        "    df_standard = df.copy()\n",
        "    for col in col_names:\n",
        "        df_standard[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "    return df_standard\n",
        "\n",
        "def custom_detrend(df, col_names):\n",
        "    df_detrended = df.copy()\n",
        "    for col in col_names:\n",
        "        y = df_detrended[col]\n",
        "        x = np.arange(len(y))\n",
        "        p = np.polyfit(x, y, 1)\n",
        "        trend = np.polyval(p, x)\n",
        "        detrended = y - trend\n",
        "        df_detrended[col] = detrended\n",
        "    return df_detrended\n",
        "\n",
        "def preprocess(df, col_names, n_clusters):\n",
        "    df_new = df.copy()\n",
        "    df_new = denoise_data(df, col_names, n_clusters)\n",
        "    df_new = z_score(df_new, col_names)\n",
        "    df_new = custom_detrend(df_new, col_names)\n",
        "    return df_new\n",
        "\n",
        "def df_to_raw(df, sfreq=250):\n",
        "    info = mne.create_info(ch_names=list(df.columns), sfreq=sfreq, ch_types=['eeg'] * df.shape[1])\n",
        "    raw = mne.io.RawArray(df.T.values * 1e-6, info)  # Converting values to Volts from microvolts for MNE\n",
        "    return raw\n",
        "\n",
        "def reject_artifacts(df, channel):\n",
        "    threshold_factor = 3\n",
        "    median = df[channel].median()\n",
        "    mad = np.median(np.abs(df[channel] - median))\n",
        "    spikes = np.abs(df[channel] - median) > threshold_factor * mad\n",
        "    x = np.arange(len(df[channel]))\n",
        "    cs = CubicSpline(x[~spikes], df[channel][~spikes]) # Interpolate using Cubic Spline\n",
        "    interpolated_values = cs(x)\n",
        "    interpolated_values[spikes] *= 0.1  # Make interpolated values 0.1 times smaller\n",
        "    # Check each interpolated value's difference from median and compare to the threshold\n",
        "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
        "    interpolated_values[spike_values] *= 0.01 \n",
        "    df[channel] = interpolated_values\n",
        "    return df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 8)\n",
            "(10000, 8)\n",
            "(10000, 8)\n",
            "(10000, 8)\n",
            "(10000, 8)\n",
            "(10000, 8)\n",
            "(10000, 8)\n",
            "(10000, 8)\n"
          ]
        }
      ],
      "source": [
        "folder_name = 'f'\n",
        "selected_columns = ['Fz', 'FC1', 'FC2', 'C3', 'Cz', 'C4', 'CPz', 'Pz']\n",
        "duration = 40 \n",
        "raw=[]\n",
        "event=[]\n",
        "BP=[]\n",
        "PP=[]\n",
        "if os.path.exists(folder_name) and os.path.isdir(folder_name):\n",
        "    for file_name in os.listdir(folder_name):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_name, file_name)\n",
        "            s_temp = pd.read_csv(file_path, header=None)\n",
        "            inst = s_temp.iloc[:, 17]\n",
        "            df_temp = s_temp.iloc[:, :8]\n",
        "            # print(df_temp.shape)\n",
        "            # df_temp.plot(figsize=(10, 8))\n",
        "            # plt.show()\n",
        "            raw.append(df_temp)\n",
        "            event.append(inst)\n",
        "            \n",
        "            # 1. Band Pass\n",
        "            raw_bp = np.copy(df_temp)\n",
        "            for column in range(8):\n",
        "                raw_bp[:, column] = butter_bandpass_filter(raw_bp[:, column], lowcut=.4, highcut=40, fs=250) \n",
        "            print(raw_bp.shape)\n",
        "            # plt.plot(raw_bp)\n",
        "            # plt.show()\n",
        "            \n",
        "            # 2. Artifact rejection\n",
        "            BP_artifact_RJ = np.copy(raw_bp)\n",
        "            for channel in range (8):\n",
        "                BP_artifact_RJ= reject_artifacts(pd.DataFrame(BP_artifact_RJ), channel)\n",
        "            # plt.plot(BP_artifact_RJ)\n",
        "            # plt.show()\n",
        "            \n",
        "            # # 3. Smoothing\n",
        "            # BP_artifact_RJ_SM=BP_artifact_RJ.copy()\n",
        "            # window_size = 10 \n",
        "            # for channel in range (8):\n",
        "            #     # channel_data = BP_artifact_RJ_SM[channel, :]\n",
        "            #     BP_artifact_RJ_SM= BP_artifact_RJ_SM.rolling(window=window_size, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
        "            # # plt.plot(BP_artifact_RJ_SM)\n",
        "            # # plt.show()\n",
        "            # BP.append(BP_artifact_RJ_SM)\n",
        "            \n",
        "            # 4. Denoising and other preprocessing\n",
        "            BP_artifact_RJ.columns = selected_columns\n",
        "            eeg_df_denoised = preprocess(pd.DataFrame(BP_artifact_RJ), col_names=selected_columns, n_clusters=[50]*len(selected_columns))\n",
        "            # eeg_df_denoised.plot(subplots=True, figsize=(5, 5), title='Denoised EEG Data')\n",
        "            # plt.show()\n",
        "            PP.append(eeg_df_denoised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "event (80000, 1) denoised (80000, 8) pp_sig_event (80000, 9) face (30000, 9) scene (50000, 9) labels (80000,)\n"
          ]
        }
      ],
      "source": [
        "fs=250\n",
        "B_N=int(len(PP)) #Number of blocks\n",
        "PP_NP=np.array(PP) #shape: (B_N, 10000, 8=Channel Numbers)\n",
        "event=np.array(event).reshape(B_N*(df_temp.shape[0]), 1) # df_temp.shape[0]=10000\n",
        "denoised=PP_NP.reshape(B_N*(df_temp.shape[0]), 8) # seprate each blocks' signal \n",
        "pp_sig_event=np.concatenate((denoised, event), axis=1) \n",
        "labels=[] \n",
        "face = [] #lable=0\n",
        "scene=[]#lable=1\n",
        "# Aassuming correctness for the human behavior\n",
        "for i in range(len(pp_sig_event)): #len(pp_sig_event) = the whole sample points, (df_temp.shape[0]*B_N)\n",
        "    if 'M' in pp_sig_event[i, 8] or 'F' in pp_sig_event[i, 8]:\n",
        "        face.append(pp_sig_event[i])\n",
        "        labels.append(0)\n",
        "    else:\n",
        "        scene.append(pp_sig_event[i]) \n",
        "        labels.append(1)        \n",
        "face = np.array(face)\n",
        "scene = np.array(scene)\n",
        "labels=np.array(labels) \n",
        "                 \n",
        "print('event', event.shape,  'denoised',  denoised.shape, 'pp_sig_event', pp_sig_event.shape, 'face', face.shape, 'scene', scene.shape, 'labels', labels.shape)  \n",
        "#denoised is all the denoised data with shape: (df_temp.shape[0]*B_N, 8)     \n",
        "# event is all the events with shape: (df_temp.shape[0]*B_N, 1)                                                                                                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "label=labels.reshape(int(labels.shape[0]/fs), fs)\n",
        "Y=np.squeeze(label[:,0])\n",
        "\n",
        "frequency_bands = {\n",
        "    'delta': (0.5, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 14),\n",
        "    'beta': (14, 30),\n",
        "    'gamma': (30, 40),\n",
        "     }\n",
        "\n",
        "def apply_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered_signal = filtfilt(b, a, signal)\n",
        "    return filtered_signal\n",
        "denoised_reshaped = denoised.reshape(int(denoised.shape[0]/250), 250, 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(320, 2000)\n",
            "(320, 2000) (320,)\n"
          ]
        }
      ],
      "source": [
        "mlp_data=denoised_reshaped.reshape(denoised_reshaped.shape[0], denoised_reshaped.shape[1]*denoised_reshaped.shape[2])\n",
        "print(mlp_data.shape)\n",
        "\n",
        "af_mlp=mlp_data\n",
        "Y_mlp=np.squeeze(label[:,0])\n",
        "af_mlp, Y_mlp= shuffle(af_mlp, Y_mlp)\n",
        "print(af_mlp.shape, Y_mlp.shape)\n",
        "# Balance the dataset\n",
        "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X_resampled_mlp, y_resampled_mlp = oversampler.fit_resample(af_mlp, Y_mlp)\n",
        "X_resampled_mlp= X_resampled_mlp.astype(np.float32)\n",
        "y_resampled_mlp = y_resampled_mlp.astype(np.int32)\n",
        "\n",
        "\n",
        "X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = train_test_split(X_resampled_mlp,y_resampled_mlp, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# # Given data (You'd already have this loaded)\n",
        "# # X_train, y_train, X_test, y_test\n",
        "\n",
        "# # Ensure your target (y_train and y_test) is properly shaped.\n",
        "# # For binary classification, it should be of shape (n_samples, 1)\n",
        "# # For multi-class single-label classification, it should be one-hot encoded.\n",
        "\n",
        "# # Define the MLP model\n",
        "# model = Sequential()\n",
        "\n",
        "# # Input layer (with relu activation and input shape matching your feature count)\n",
        "# model.add(Dense(256, activation='relu', input_shape=(2000,)))\n",
        "\n",
        "# # Hidden layers\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(250, activation='relu'))\n",
        "# # model.add(Dropout(0.4))\n",
        "# # model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# # Output layer\n",
        "# # For binary classification, use 1 neuron with sigmoid activation\n",
        "# # For multi-class classification, use softmax activation and change units to number of classes\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile the model\n",
        "# # For binary classification, use binary_crossentropy\n",
        "# # For multi-class classification, use categorical_crossentropy\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=32, validation_data=(X_test_mlp, y_test_mlp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 99.65%\n",
            "Test Accuracy: 50.00%\n"
          ]
        }
      ],
      "source": [
        "# # Continue from the previously mentioned code\n",
        "# # Evaluate the model on the training set\n",
        "# train_loss, train_accuracy = model.evaluate(X_train_mlp, y_train_mlp, verbose=0)\n",
        "# print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# # Evaluate the model on the test set\n",
        "# test_loss, test_accuracy = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\11-software\\Python3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[I 2023-10-19 12:49:38,415] A new study created in memory with name: no-name-7ac060a1-3107-4aae-823c-2972e646f5e4\n",
            "[I 2023-10-19 12:49:41,126] Trial 0 finished with value: 0.8 and parameters: {'n_layers': 2, 'n_units_layer0': 416, 'n_units_layer1': 316, 'activation': 'tanh', 'learning_rate_init': 0.0013672497076009126, 'max_iter': 173}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:42,448] Trial 1 finished with value: 0.675 and parameters: {'n_layers': 2, 'n_units_layer0': 159, 'n_units_layer1': 336, 'activation': 'identity', 'learning_rate_init': 0.0011777566142703918, 'max_iter': 769}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:43,975] Trial 2 finished with value: 0.75 and parameters: {'n_layers': 3, 'n_units_layer0': 38, 'n_units_layer1': 89, 'n_units_layer2': 20, 'activation': 'tanh', 'learning_rate_init': 0.0001464214367679674, 'max_iter': 382}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:47,251] Trial 3 finished with value: 0.75 and parameters: {'n_layers': 3, 'n_units_layer0': 226, 'n_units_layer1': 401, 'n_units_layer2': 332, 'activation': 'tanh', 'learning_rate_init': 0.00985293536567593, 'max_iter': 895}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:49,077] Trial 4 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 309, 'n_units_layer1': 276, 'activation': 'tanh', 'learning_rate_init': 0.026151068930504617, 'max_iter': 599}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:50,744] Trial 5 finished with value: 0.75 and parameters: {'n_layers': 1, 'n_units_layer0': 176, 'activation': 'logistic', 'learning_rate_init': 0.09603861915263483, 'max_iter': 655}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:53,682] Trial 6 finished with value: 0.6 and parameters: {'n_layers': 3, 'n_units_layer0': 453, 'n_units_layer1': 228, 'n_units_layer2': 329, 'activation': 'identity', 'learning_rate_init': 0.00020479774131295275, 'max_iter': 271}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:56,824] Trial 7 finished with value: 0.675 and parameters: {'n_layers': 2, 'n_units_layer0': 252, 'n_units_layer1': 208, 'activation': 'tanh', 'learning_rate_init': 0.0002328413934773576, 'max_iter': 301}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:49:59,243] Trial 8 finished with value: 0.75 and parameters: {'n_layers': 3, 'n_units_layer0': 239, 'n_units_layer1': 375, 'n_units_layer2': 470, 'activation': 'tanh', 'learning_rate_init': 0.00016670444660284598, 'max_iter': 965}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:49:59,674] Trial 9 finished with value: 0.675 and parameters: {'n_layers': 1, 'n_units_layer0': 128, 'activation': 'relu', 'learning_rate_init': 0.0018566033942367854, 'max_iter': 62}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:50:01,647] Trial 10 finished with value: 0.675 and parameters: {'n_layers': 1, 'n_units_layer0': 509, 'activation': 'relu', 'learning_rate_init': 0.0008482090089688921, 'max_iter': 55}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (368) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:50:02,740] Trial 11 finished with value: 0.775 and parameters: {'n_layers': 2, 'n_units_layer0': 21, 'n_units_layer1': 67, 'activation': 'tanh', 'learning_rate_init': 0.00010320000122565849, 'max_iter': 368}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:07,630] Trial 12 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 356, 'n_units_layer1': 506, 'activation': 'logistic', 'learning_rate_init': 0.0006602417294346682, 'max_iter': 446}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:10,547] Trial 13 finished with value: 0.775 and parameters: {'n_layers': 2, 'n_units_layer0': 394, 'n_units_layer1': 37, 'activation': 'tanh', 'learning_rate_init': 0.0036842362101717745, 'max_iter': 207}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (170) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:50:11,089] Trial 14 finished with value: 0.775 and parameters: {'n_layers': 2, 'n_units_layer0': 17, 'n_units_layer1': 128, 'activation': 'tanh', 'learning_rate_init': 0.0005419121003373913, 'max_iter': 170}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:20,390] Trial 15 finished with value: 0.725 and parameters: {'n_layers': 1, 'n_units_layer0': 411, 'activation': 'tanh', 'learning_rate_init': 0.0001135796735174672, 'max_iter': 439}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:23,679] Trial 16 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 94, 'n_units_layer1': 152, 'activation': 'logistic', 'learning_rate_init': 0.0003850272224226729, 'max_iter': 572}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:27,008] Trial 17 finished with value: 0.675 and parameters: {'n_layers': 1, 'n_units_layer0': 344, 'activation': 'relu', 'learning_rate_init': 0.002638812344137874, 'max_iter': 336}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:30,629] Trial 18 finished with value: 0.65 and parameters: {'n_layers': 3, 'n_units_layer0': 501, 'n_units_layer1': 20, 'n_units_layer2': 38, 'activation': 'identity', 'learning_rate_init': 0.0003168964989015403, 'max_iter': 181}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:31,319] Trial 19 finished with value: 0.775 and parameters: {'n_layers': 2, 'n_units_layer0': 83, 'n_units_layer1': 299, 'activation': 'tanh', 'learning_rate_init': 0.0004206796084703472, 'max_iter': 459}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:35,946] Trial 20 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 308, 'n_units_layer1': 443, 'activation': 'tanh', 'learning_rate_init': 0.00010191688348825367, 'max_iter': 741}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:39,284] Trial 21 finished with value: 0.65 and parameters: {'n_layers': 2, 'n_units_layer0': 397, 'n_units_layer1': 21, 'activation': 'tanh', 'learning_rate_init': 0.0044916494810348035, 'max_iter': 186}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:42,609] Trial 22 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 428, 'n_units_layer1': 93, 'activation': 'tanh', 'learning_rate_init': 0.00129816971978129, 'max_iter': 274}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:46,136] Trial 23 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 467, 'n_units_layer1': 186, 'activation': 'tanh', 'learning_rate_init': 0.004566971599321136, 'max_iter': 136}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:48,938] Trial 24 finished with value: 0.775 and parameters: {'n_layers': 2, 'n_units_layer0': 374, 'n_units_layer1': 64, 'activation': 'tanh', 'learning_rate_init': 0.002071696164481792, 'max_iter': 221}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:53,769] Trial 25 finished with value: 0.725 and parameters: {'n_layers': 1, 'n_units_layer0': 324, 'activation': 'tanh', 'learning_rate_init': 0.0007372764417119855, 'max_iter': 367}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (109) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:50:57,547] Trial 26 finished with value: 0.725 and parameters: {'n_layers': 3, 'n_units_layer0': 281, 'n_units_layer1': 144, 'n_units_layer2': 178, 'activation': 'logistic', 'learning_rate_init': 0.00023278844589719782, 'max_iter': 109}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:50:59,666] Trial 27 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 438, 'n_units_layer1': 322, 'activation': 'relu', 'learning_rate_init': 0.001055466499105468, 'max_iter': 215}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:01,849] Trial 28 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 210, 'n_units_layer1': 247, 'activation': 'identity', 'learning_rate_init': 0.0004019613613876887, 'max_iter': 511}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:03,731] Trial 29 finished with value: 0.675 and parameters: {'n_layers': 3, 'n_units_layer0': 393, 'n_units_layer1': 57, 'n_units_layer2': 477, 'activation': 'identity', 'learning_rate_init': 0.0012513807908205682, 'max_iter': 255}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:07,880] Trial 30 finished with value: 0.775 and parameters: {'n_layers': 1, 'n_units_layer0': 183, 'activation': 'tanh', 'learning_rate_init': 0.004288552052616078, 'max_iter': 370}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:08,366] Trial 31 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 37, 'n_units_layer1': 134, 'activation': 'tanh', 'learning_rate_init': 0.000584308300301311, 'max_iter': 132}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (158) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:08,842] Trial 32 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 16, 'n_units_layer1': 105, 'activation': 'tanh', 'learning_rate_init': 0.0005531020036509831, 'max_iter': 158}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:10,873] Trial 33 finished with value: 0.625 and parameters: {'n_layers': 2, 'n_units_layer0': 76, 'n_units_layer1': 56, 'activation': 'tanh', 'learning_rate_init': 0.00013480021093005692, 'max_iter': 337}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:11,448] Trial 34 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 127, 'n_units_layer1': 176, 'activation': 'tanh', 'learning_rate_init': 0.0013948597350929722, 'max_iter': 236}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (114) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:11,990] Trial 35 finished with value: 0.8 and parameters: {'n_layers': 2, 'n_units_layer0': 57, 'n_units_layer1': 113, 'activation': 'tanh', 'learning_rate_init': 0.00028707516402031445, 'max_iter': 114}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:12,451] Trial 36 finished with value: 0.775 and parameters: {'n_layers': 2, 'n_units_layer0': 62, 'n_units_layer1': 22, 'activation': 'tanh', 'learning_rate_init': 0.00017336510590553898, 'max_iter': 91}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:15,317] Trial 37 finished with value: 0.775 and parameters: {'n_layers': 3, 'n_units_layer0': 477, 'n_units_layer1': 377, 'n_units_layer2': 177, 'activation': 'tanh', 'learning_rate_init': 0.00027859657603825414, 'max_iter': 295}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (413) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:22,781] Trial 38 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 142, 'n_units_layer1': 69, 'activation': 'logistic', 'learning_rate_init': 0.00014264478315247469, 'max_iter': 413}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:24,474] Trial 39 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 275, 'n_units_layer1': 110, 'activation': 'tanh', 'learning_rate_init': 0.008699518293057904, 'max_iter': 326}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:26,331] Trial 40 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 106, 'n_units_layer1': 291, 'activation': 'identity', 'learning_rate_init': 0.0001997426714113644, 'max_iter': 676}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (174) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:26,875] Trial 41 finished with value: 0.8 and parameters: {'n_layers': 2, 'n_units_layer0': 17, 'n_units_layer1': 121, 'activation': 'tanh', 'learning_rate_init': 0.0003005968669306088, 'max_iter': 174}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:27,264] Trial 42 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 57, 'n_units_layer1': 92, 'activation': 'tanh', 'learning_rate_init': 0.00029902828128910525, 'max_iter': 86}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:27,546] Trial 43 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 42, 'n_units_layer1': 43, 'activation': 'tanh', 'learning_rate_init': 0.00010433985713870821, 'max_iter': 52}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (202) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:28,422] Trial 44 finished with value: 0.675 and parameters: {'n_layers': 2, 'n_units_layer0': 41, 'n_units_layer1': 168, 'activation': 'tanh', 'learning_rate_init': 0.00018647775199381013, 'max_iter': 202}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (120) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:29,848] Trial 45 finished with value: 0.725 and parameters: {'n_layers': 3, 'n_units_layer0': 111, 'n_units_layer1': 251, 'n_units_layer2': 163, 'activation': 'relu', 'learning_rate_init': 0.00016412758023755353, 'max_iter': 120}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:31,470] Trial 46 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 205, 'n_units_layer1': 210, 'activation': 'tanh', 'learning_rate_init': 0.0007897572668530425, 'max_iter': 254}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:33,734] Trial 47 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 166, 'n_units_layer1': 346, 'activation': 'tanh', 'learning_rate_init': 0.00023895433710860773, 'max_iter': 170}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:36,835] Trial 48 finished with value: 0.775 and parameters: {'n_layers': 1, 'n_units_layer0': 66, 'activation': 'logistic', 'learning_rate_init': 0.00043518313969240793, 'max_iter': 913}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:37,227] Trial 49 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 20, 'n_units_layer1': 119, 'activation': 'tanh', 'learning_rate_init': 0.0008809506467423584, 'max_iter': 512}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:41,751] Trial 50 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 352, 'n_units_layer1': 72, 'activation': 'tanh', 'learning_rate_init': 0.000305140322389795, 'max_iter': 292}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (153) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:42,254] Trial 51 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 16, 'n_units_layer1': 127, 'activation': 'tanh', 'learning_rate_init': 0.0005240335790554418, 'max_iter': 153}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:42,978] Trial 52 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 48, 'n_units_layer1': 84, 'activation': 'tanh', 'learning_rate_init': 0.0003410883667737622, 'max_iter': 191}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:43,460] Trial 53 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 86, 'n_units_layer1': 44, 'activation': 'tanh', 'learning_rate_init': 0.0004804735879953638, 'max_iter': 77}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:43,984] Trial 54 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 36, 'n_units_layer1': 153, 'activation': 'relu', 'learning_rate_init': 0.0006324260123481637, 'max_iter': 238}. Best is trial 0 with value: 0.8.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (113) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:51:47,646] Trial 55 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 414, 'n_units_layer1': 191, 'activation': 'tanh', 'learning_rate_init': 0.000125481464066081, 'max_iter': 113}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:52,659] Trial 56 finished with value: 0.775 and parameters: {'n_layers': 2, 'n_units_layer0': 383, 'n_units_layer1': 34, 'activation': 'tanh', 'learning_rate_init': 0.00023287530571125398, 'max_iter': 404}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:53,190] Trial 57 finished with value: 0.675 and parameters: {'n_layers': 2, 'n_units_layer0': 95, 'n_units_layer1': 100, 'activation': 'tanh', 'learning_rate_init': 0.000904129675379424, 'max_iter': 176}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:55,229] Trial 58 finished with value: 0.575 and parameters: {'n_layers': 2, 'n_units_layer0': 311, 'n_units_layer1': 329, 'activation': 'identity', 'learning_rate_init': 0.00161045999043041, 'max_iter': 327}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:51:58,150] Trial 59 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 447, 'n_units_layer1': 434, 'activation': 'tanh', 'learning_rate_init': 0.0010752776777511292, 'max_iter': 275}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:52:04,203] Trial 60 finished with value: 0.75 and parameters: {'n_layers': 1, 'n_units_layer0': 485, 'activation': 'logistic', 'learning_rate_init': 0.002078021824256651, 'max_iter': 227}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:52:04,765] Trial 61 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 67, 'n_units_layer1': 306, 'activation': 'tanh', 'learning_rate_init': 0.0005202569105741671, 'max_iter': 460}. Best is trial 0 with value: 0.8.\n",
            "[I 2023-10-19 12:52:05,360] Trial 62 finished with value: 0.825 and parameters: {'n_layers': 2, 'n_units_layer0': 29, 'n_units_layer1': 281, 'activation': 'tanh', 'learning_rate_init': 0.0003614161343128062, 'max_iter': 492}. Best is trial 62 with value: 0.825.\n",
            "[I 2023-10-19 12:52:05,881] Trial 63 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 30, 'n_units_layer1': 263, 'activation': 'tanh', 'learning_rate_init': 0.0004096362354221918, 'max_iter': 533}. Best is trial 62 with value: 0.825.\n",
            "[I 2023-10-19 12:52:06,550] Trial 64 finished with value: 0.675 and parameters: {'n_layers': 2, 'n_units_layer0': 48, 'n_units_layer1': 228, 'activation': 'tanh', 'learning_rate_init': 0.00034545191253338467, 'max_iter': 611}. Best is trial 62 with value: 0.825.\n",
            "[I 2023-10-19 12:52:07,036] Trial 65 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 76, 'n_units_layer1': 346, 'activation': 'tanh', 'learning_rate_init': 0.0006880242714025035, 'max_iter': 148}. Best is trial 62 with value: 0.825.\n",
            "[I 2023-10-19 12:52:07,982] Trial 66 finished with value: 0.725 and parameters: {'n_layers': 2, 'n_units_layer0': 23, 'n_units_layer1': 284, 'activation': 'relu', 'learning_rate_init': 0.0002515477411326856, 'max_iter': 466}. Best is trial 62 with value: 0.825.\n",
            "[I 2023-10-19 12:52:13,116] Trial 67 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 332, 'n_units_layer1': 81, 'activation': 'tanh', 'learning_rate_init': 0.00016063788546453062, 'max_iter': 808}. Best is trial 62 with value: 0.825.\n",
            "d:\\11-software\\Python3.8\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (111) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "[I 2023-10-19 12:52:16,787] Trial 68 finished with value: 0.7 and parameters: {'n_layers': 2, 'n_units_layer0': 426, 'n_units_layer1': 153, 'activation': 'tanh', 'learning_rate_init': 0.00019614035518367346, 'max_iter': 111}. Best is trial 62 with value: 0.825.\n",
            "[I 2023-10-19 12:52:18,288] Trial 69 finished with value: 0.75 and parameters: {'n_layers': 2, 'n_units_layer0': 55, 'n_units_layer1': 136, 'activation': 'tanh', 'learning_rate_init': 0.00010030231595306219, 'max_iter': 591}. Best is trial 62 with value: 0.825.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials:  70\n",
            "Best trial:\n",
            "  Value:  0.825\n",
            "  Params: \n",
            "    n_layers: 2\n",
            "    n_units_layer0: 29\n",
            "    n_units_layer1: 281\n",
            "    activation: tanh\n",
            "    learning_rate_init: 0.0003614161343128062\n",
            "    max_iter: 492\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Layers and neurons\n",
        "    n_layers = trial.suggest_int('n_layers', 1,3)\n",
        "    layers = []\n",
        "    for i in range(n_layers):\n",
        "        layers.append(trial.suggest_int(f'n_units_layer{i}', 16,  512))\n",
        "    \n",
        "    # Activation function\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'logistic', 'tanh', 'identity'])\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4,  1e-1, log=True)\n",
        "    \n",
        "    max_iter = trial.suggest_int('max_iter', 50, 1000)\n",
        "\n",
        "    model = MLPClassifier(hidden_layer_sizes=tuple(layers), \n",
        "                          activation=activation, \n",
        "                          learning_rate_init=learning_rate_init,\n",
        "                          max_iter=max_iter ,  # to ensure convergence in most cases\n",
        "                          random_state=42)\n",
        "\n",
        "    model.fit(X_train_mlp, y_train_mlp)\n",
        "\n",
        "    # Evaluate\n",
        "    predictions = model.predict(X_test_mlp)\n",
        "    accuracy = accuracy_score(y_test_mlp, predictions)\n",
        "    return accuracy\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=70)\n",
        "\n",
        "print('Number of finished trials: ', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: ', trial.value)\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.0000\n",
            "Test Accuracy: 0.8250\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Extract the best parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Extract individual parameters\n",
        "n_layers = best_params['n_layers']\n",
        "layers = [best_params[f'n_units_layer{i}'] for i in range(n_layers)]\n",
        "activation = best_params['activation']\n",
        "learning_rate_init = best_params['learning_rate_init']\n",
        "max_iter = best_params['max_iter']\n",
        "\n",
        "# Create the model using the best parameters\n",
        "best_model = MLPClassifier(hidden_layer_sizes=tuple(layers), \n",
        "                           activation=activation, \n",
        "                           learning_rate_init=learning_rate_init,\n",
        "                           max_iter=max_iter ,  # to ensure convergence in most cases\n",
        "                           random_state=42)\n",
        "\n",
        "# Train the model using training data\n",
        "best_model.fit(X_train_mlp, y_train_mlp)\n",
        "\n",
        "# Predict using the test data\n",
        "predictions = best_model.predict(X_test_mlp)\n",
        "\n",
        "# Predict using the training data\n",
        "train_predictions = best_model.predict(X_train_mlp)\n",
        "\n",
        "# Evaluate the model using training data\n",
        "train_accuracy = accuracy_score(y_train_mlp, train_predictions)\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the model using test data\n",
        "accuracy = accuracy_score(y_test_mlp, predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-14 14:04:19,432] A new study created in memory with name: no-name-38071058-c379-4c98-bc92-5521cc07b1ec\n",
            "[I 2023-10-14 14:04:24,851] Trial 0 finished with value: 0.59375 and parameters: {'n_units_i': 220, 'dropout_rate_1': 0.4604194999622985, 'n_units_h1': 162, 'dropout_rate_2': 0.15008127922277684, 'n_units_h2': 117, 'lr': 0.001084045276968205}. Best is trial 0 with value: 0.59375.\n",
            "[I 2023-10-14 14:04:31,996] Trial 1 finished with value: 0.59375 and parameters: {'n_units_i': 479, 'dropout_rate_1': 0.15479312816685095, 'n_units_h1': 215, 'dropout_rate_2': 0.29033845702401284, 'n_units_h2': 16, 'lr': 0.0029068951230053893}. Best is trial 0 with value: 0.59375.\n",
            "[I 2023-10-14 14:04:37,351] Trial 2 finished with value: 0.59375 and parameters: {'n_units_i': 279, 'dropout_rate_1': 0.29702015827014416, 'n_units_h1': 106, 'dropout_rate_2': 0.4746165910903912, 'n_units_h2': 45, 'lr': 0.006785089288374672}. Best is trial 0 with value: 0.59375.\n",
            "[I 2023-10-14 14:04:45,296] Trial 3 finished with value: 0.59375 and parameters: {'n_units_i': 362, 'dropout_rate_1': 0.11578647257666733, 'n_units_h1': 154, 'dropout_rate_2': 0.17451921889652242, 'n_units_h2': 85, 'lr': 0.002948335649808873}. Best is trial 0 with value: 0.59375.\n",
            "[I 2023-10-14 14:04:53,220] Trial 4 finished with value: 0.4375 and parameters: {'n_units_i': 399, 'dropout_rate_1': 0.3991399157446506, 'n_units_h1': 174, 'dropout_rate_2': 0.10330520540699276, 'n_units_h2': 31, 'lr': 0.010426726245447302}. Best is trial 0 with value: 0.59375.\n",
            "[I 2023-10-14 14:04:58,989] Trial 5 finished with value: 0.5625 and parameters: {'n_units_i': 212, 'dropout_rate_1': 0.15382035262257057, 'n_units_h1': 107, 'dropout_rate_2': 0.4735750278033716, 'n_units_h2': 122, 'lr': 0.007253974261140095}. Best is trial 0 with value: 0.59375.\n",
            "[I 2023-10-14 14:05:06,775] Trial 6 finished with value: 0.59375 and parameters: {'n_units_i': 402, 'dropout_rate_1': 0.5128350367527792, 'n_units_h1': 235, 'dropout_rate_2': 0.33104634196620175, 'n_units_h2': 71, 'lr': 0.0025444367736360326}. Best is trial 0 with value: 0.59375.\n",
            "[I 2023-10-14 14:05:10,772] Trial 7 finished with value: 0.71875 and parameters: {'n_units_i': 130, 'dropout_rate_1': 0.47512487330136044, 'n_units_h1': 65, 'dropout_rate_2': 0.3212851851262384, 'n_units_h2': 98, 'lr': 0.01569471338210945}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:17,200] Trial 8 finished with value: 0.59375 and parameters: {'n_units_i': 379, 'dropout_rate_1': 0.3067253398104226, 'n_units_h1': 131, 'dropout_rate_2': 0.22863742901807002, 'n_units_h2': 96, 'lr': 0.024947509818896287}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:22,682] Trial 9 finished with value: 0.4375 and parameters: {'n_units_i': 257, 'dropout_rate_1': 0.11921652099857674, 'n_units_h1': 156, 'dropout_rate_2': 0.40486550315150394, 'n_units_h2': 84, 'lr': 0.04789146772912444}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:27,067] Trial 10 finished with value: 0.4375 and parameters: {'n_units_i': 153, 'dropout_rate_1': 0.5832962269627853, 'n_units_h1': 37, 'dropout_rate_2': 0.5560213859120013, 'n_units_h2': 61, 'lr': 0.07908366233080584}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:31,143] Trial 11 finished with value: 0.65625 and parameters: {'n_units_i': 134, 'dropout_rate_1': 0.4643526807689727, 'n_units_h1': 35, 'dropout_rate_2': 0.24100457384398294, 'n_units_h2': 123, 'lr': 0.0011376109969028023}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:34,518] Trial 12 finished with value: 0.4375 and parameters: {'n_units_i': 129, 'dropout_rate_1': 0.4681360232610416, 'n_units_h1': 33, 'dropout_rate_2': 0.26213207258015964, 'n_units_h2': 104, 'lr': 0.017473026375746038}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:39,024] Trial 13 finished with value: 0.6875 and parameters: {'n_units_i': 177, 'dropout_rate_1': 0.5913265078929206, 'n_units_h1': 75, 'dropout_rate_2': 0.3285878892203337, 'n_units_h2': 128, 'lr': 0.021009370734079186}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:43,643] Trial 14 finished with value: 0.4375 and parameters: {'n_units_i': 188, 'dropout_rate_1': 0.5995758461030702, 'n_units_h1': 78, 'dropout_rate_2': 0.3416770950956675, 'n_units_h2': 107, 'lr': 0.03013775104753135}. Best is trial 7 with value: 0.71875.\n",
            "[I 2023-10-14 14:05:48,871] Trial 15 finished with value: 0.75 and parameters: {'n_units_i': 294, 'dropout_rate_1': 0.5350895405431693, 'n_units_h1': 79, 'dropout_rate_2': 0.3873433313045219, 'n_units_h2': 91, 'lr': 0.015609078350217189}. Best is trial 15 with value: 0.75.\n",
            "[I 2023-10-14 14:05:54,236] Trial 16 finished with value: 0.65625 and parameters: {'n_units_i': 324, 'dropout_rate_1': 0.5323139084580927, 'n_units_h1': 66, 'dropout_rate_2': 0.4021345382212438, 'n_units_h2': 87, 'lr': 0.013387908617269332}. Best is trial 15 with value: 0.75.\n",
            "[I 2023-10-14 14:06:00,517] Trial 17 finished with value: 0.5625 and parameters: {'n_units_i': 461, 'dropout_rate_1': 0.38732665705367125, 'n_units_h1': 104, 'dropout_rate_2': 0.3851859294488119, 'n_units_h2': 62, 'lr': 0.03599222461566596}. Best is trial 15 with value: 0.75.\n",
            "[I 2023-10-14 14:06:05,707] Trial 18 finished with value: 0.4375 and parameters: {'n_units_i': 286, 'dropout_rate_1': 0.528646030398361, 'n_units_h1': 61, 'dropout_rate_2': 0.2929773554736239, 'n_units_h2': 101, 'lr': 0.016653263768947796}. Best is trial 15 with value: 0.75.\n",
            "[I 2023-10-14 14:06:10,646] Trial 19 finished with value: 0.4375 and parameters: {'n_units_i': 230, 'dropout_rate_1': 0.42502714995449314, 'n_units_h1': 123, 'dropout_rate_2': 0.4458933516629181, 'n_units_h2': 71, 'lr': 0.05628961025159461}. Best is trial 15 with value: 0.75.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials:  20\n",
            "Best trial:\n",
            "  Value:  0.75\n",
            "  Params: \n",
            "    n_units_i: 294\n",
            "    dropout_rate_1: 0.5350895405431693\n",
            "    n_units_h1: 79\n",
            "    dropout_rate_2: 0.3873433313045219\n",
            "    n_units_h2: 91\n",
            "    lr: 0.015609078350217189\n"
          ]
        }
      ],
      "source": [
        "# import optuna\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# def objective(trial):\n",
        "#     model = Sequential()\n",
        "#     # Input layer\n",
        "#     n_units_i = trial.suggest_int('n_units_i', 128, 512)\n",
        "#     model.add(Dense(n_units_i, activation='relu', input_shape=(2000,)))\n",
        "#     # Hidden layers\n",
        "#     dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.1, 0.6)\n",
        "#     model.add(Dropout(dropout_rate_1))\n",
        "#     n_units_h1 = trial.suggest_int('n_units_h1', 32, 256)\n",
        "#     model.add(Dense(n_units_h1, activation='relu'))\n",
        "#     dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.1, 0.6)\n",
        "#     model.add(Dropout(dropout_rate_2))\n",
        "#     n_units_h2 = trial.suggest_int('n_units_h2', 16, 128)\n",
        "#     model.add(Dense(n_units_h2, activation='relu'))\n",
        "#     # Output layer\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "#     # Compile the model\n",
        "#     lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
        "#     optimizer = Adam(learning_rate=lr)\n",
        "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     # Train the model\n",
        "#     model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "#     # Evaluate the model\n",
        "#     score = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "#     return score[1]  # Return accuracy\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=20)\n",
        "# print('Number of finished trials: ', len(study.trials))\n",
        "# print('Best trial:')\n",
        "# trial = study.best_trial\n",
        "# print('  Value: ', trial.value)\n",
        "# print('  Params: ')\n",
        "# for key, value in trial.params.items():\n",
        "#     print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 1s 33ms/step - loss: 7.4861 - accuracy: 0.5139 - val_loss: 2.9239 - val_accuracy: 0.3438\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 2.2552 - accuracy: 0.5278 - val_loss: 0.5973 - val_accuracy: 0.6250\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 1.1619 - accuracy: 0.5243 - val_loss: 0.7301 - val_accuracy: 0.5938\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 1.0126 - accuracy: 0.5486 - val_loss: 0.7150 - val_accuracy: 0.5625\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7577 - accuracy: 0.5590 - val_loss: 0.7238 - val_accuracy: 0.5625\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.9086 - accuracy: 0.5451 - val_loss: 0.7838 - val_accuracy: 0.5625\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.8416 - accuracy: 0.4965 - val_loss: 0.7104 - val_accuracy: 0.5000\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7643 - accuracy: 0.5208 - val_loss: 0.7222 - val_accuracy: 0.5625\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6730 - accuracy: 0.5208 - val_loss: 0.7394 - val_accuracy: 0.4375\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.7384 - accuracy: 0.5417 - val_loss: 0.7222 - val_accuracy: 0.5625\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7177 - accuracy: 0.5104 - val_loss: 0.7197 - val_accuracy: 0.5312\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6796 - accuracy: 0.5347 - val_loss: 0.7014 - val_accuracy: 0.5625\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6876 - accuracy: 0.5347 - val_loss: 0.6934 - val_accuracy: 0.5625\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6690 - accuracy: 0.5486 - val_loss: 0.6914 - val_accuracy: 0.5625\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7000 - accuracy: 0.5208 - val_loss: 0.6974 - val_accuracy: 0.5312\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6739 - accuracy: 0.5208 - val_loss: 0.6935 - val_accuracy: 0.5312\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7334 - accuracy: 0.5208 - val_loss: 0.6942 - val_accuracy: 0.5312\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6787 - accuracy: 0.5347 - val_loss: 0.6981 - val_accuracy: 0.5312\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6688 - accuracy: 0.5347 - val_loss: 0.6891 - val_accuracy: 0.5938\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.6687 - accuracy: 0.5278 - val_loss: 0.6941 - val_accuracy: 0.5938\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.7066 - accuracy: 0.5590 - val_loss: 0.6701 - val_accuracy: 0.5938\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6611 - accuracy: 0.5382 - val_loss: 0.6579 - val_accuracy: 0.6250\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6686 - accuracy: 0.5451 - val_loss: 0.6701 - val_accuracy: 0.6250\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7082 - accuracy: 0.5174 - val_loss: 0.6715 - val_accuracy: 0.6250\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6672 - accuracy: 0.5486 - val_loss: 0.6628 - val_accuracy: 0.6250\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6631 - accuracy: 0.5417 - val_loss: 0.6481 - val_accuracy: 0.6250\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6546 - accuracy: 0.5625 - val_loss: 0.6466 - val_accuracy: 0.6250\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6622 - accuracy: 0.5486 - val_loss: 0.6431 - val_accuracy: 0.6250\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7068 - accuracy: 0.5278 - val_loss: 0.6290 - val_accuracy: 0.6562\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6537 - accuracy: 0.5694 - val_loss: 0.6240 - val_accuracy: 0.6562\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6506 - accuracy: 0.5556 - val_loss: 0.6225 - val_accuracy: 0.6562\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6479 - accuracy: 0.5660 - val_loss: 0.6140 - val_accuracy: 0.6562\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6964 - accuracy: 0.5382 - val_loss: 0.6339 - val_accuracy: 0.6562\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6624 - accuracy: 0.5729 - val_loss: 0.6449 - val_accuracy: 0.6562\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6810 - accuracy: 0.5694 - val_loss: 0.6452 - val_accuracy: 0.6875\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6689 - accuracy: 0.5590 - val_loss: 0.6534 - val_accuracy: 0.6562\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6545 - accuracy: 0.5625 - val_loss: 0.6867 - val_accuracy: 0.6562\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7043 - accuracy: 0.5694 - val_loss: 0.7192 - val_accuracy: 0.6562\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6890 - accuracy: 0.5694 - val_loss: 0.6710 - val_accuracy: 0.5938\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6523 - accuracy: 0.5590 - val_loss: 0.6389 - val_accuracy: 0.6250\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7006 - accuracy: 0.5486 - val_loss: 0.6444 - val_accuracy: 0.6250\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7007 - accuracy: 0.5694 - val_loss: 0.6686 - val_accuracy: 0.6250\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6572 - accuracy: 0.5729 - val_loss: 0.6500 - val_accuracy: 0.6250\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6382 - accuracy: 0.5521 - val_loss: 0.6621 - val_accuracy: 0.6250\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6777 - accuracy: 0.5590 - val_loss: 0.6828 - val_accuracy: 0.6250\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6587 - accuracy: 0.5521 - val_loss: 0.6819 - val_accuracy: 0.6250\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6525 - accuracy: 0.5590 - val_loss: 0.7079 - val_accuracy: 0.5938\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6784 - accuracy: 0.5451 - val_loss: 0.7299 - val_accuracy: 0.5938\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6488 - accuracy: 0.5625 - val_loss: 0.7303 - val_accuracy: 0.6250\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.6705 - accuracy: 0.5799 - val_loss: 0.7105 - val_accuracy: 0.6250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7105 - accuracy: 0.6250\n",
            "Test Loss: 0.7105\n",
            "Test Accuracy: 62.50%\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# # Function to define and compile the model based on hyperparameters\n",
        "# def create_model(n_units_i, dropout_rate_1, n_units_h1, dropout_rate_2, n_units_h2, lr):\n",
        "#     model = Sequential()\n",
        "    \n",
        "#     # Input layer\n",
        "#     model.add(Dense(n_units_i, activation='relu', input_shape=(2000,)))\n",
        "\n",
        "#     # Hidden layers\n",
        "#     model.add(Dropout(dropout_rate_1))\n",
        "#     model.add(Dense(n_units_h1, activation='relu'))\n",
        "#     model.add(Dropout(dropout_rate_2))\n",
        "#     model.add(Dense(n_units_h2, activation='relu'))\n",
        "\n",
        "#     # Output layer\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     # Compile the model\n",
        "#     optimizer = Adam(learning_rate=lr)\n",
        "#     model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     return model\n",
        "\n",
        "# # Using the best hyperparameters found by Optuna\n",
        "# best_params = study.best_params\n",
        "# best_model = create_model(\n",
        "#     best_params['n_units_i'], \n",
        "#     best_params['dropout_rate_1'], \n",
        "#     best_params['n_units_h1'], \n",
        "#     best_params['dropout_rate_2'], \n",
        "#     best_params['n_units_h2'], \n",
        "#     best_params['lr']\n",
        "# )\n",
        "\n",
        "# # Train the model using the entire training dataset\n",
        "# best_model.fit(X_train_mlp, y_train_mlp, epochs=50, batch_size=64, validation_data=(X_test_mlp, y_test_mlp))\n",
        "\n",
        "# # Evaluate the model using the test dataset\n",
        "# loss, accuracy = best_model.evaluate(X_test_mlp, y_test_mlp)\n",
        "\n",
        "# print(f\"Test Loss: {loss:.4f}\")\n",
        "# print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
