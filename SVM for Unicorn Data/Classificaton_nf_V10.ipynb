{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vMaBF4q9pl9k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from numpy import unwrap, diff, abs, angle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import hilbert\n",
        "from sklearn.utils import shuffle\n",
        "import scipy\n",
        "from scipy.signal import butter, filtfilt, hilbert\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.interpolate import CubicSpline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Dense,  BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "import pywt\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preproccesing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "def denoise_data(df, col_names, n_clusters):\n",
        "    df_denoised = df.copy()\n",
        "    for col_name, k in zip(col_names, n_clusters):\n",
        "        df_denoised[col_name] = pd.to_numeric(df_denoised[col_name], errors='coerce') # Convert column to numeric format\n",
        "        X = df_denoised.select_dtypes(include=['float64', 'int64']) # Select only numeric columns\n",
        "        clf = KNeighborsRegressor(n_neighbors=k, weights='uniform') # Fit KNeighborsRegressor\n",
        "        clf.fit(X.index.values[:, np.newaxis], X[col_name])\n",
        "        y_pred = clf.predict(X.index.values[:, np.newaxis]) # Predict values \n",
        "        df_denoised[col_name] = y_pred\n",
        "    return df_denoised\n",
        "\n",
        "def z_score(df, col_names):\n",
        "    df_standard = df.copy()\n",
        "    for col in col_names:\n",
        "        df_standard[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "    return df_standard\n",
        "\n",
        "def custom_detrend(df, col_names):\n",
        "    df_detrended = df.copy()\n",
        "    for col in col_names:\n",
        "        y = df_detrended[col]\n",
        "        x = np.arange(len(y))\n",
        "        p = np.polyfit(x, y, 1)\n",
        "        trend = np.polyval(p, x)\n",
        "        detrended = y - trend\n",
        "        df_detrended[col] = detrended\n",
        "    return df_detrended\n",
        "\n",
        "def preprocess(df, col_names, n_clusters):\n",
        "    df_new = df.copy()\n",
        "    df_new = denoise_data(df, col_names, n_clusters)\n",
        "    # df_new = z_score(df_new, col_names)\n",
        "    # df_new = custom_detrend(df_new, col_names)\n",
        "    return df_new\n",
        "\n",
        "def df_to_raw(df, sfreq=250):\n",
        "    info = mne.create_info(ch_names=list(df.columns), sfreq=sfreq, ch_types=['eeg'] * df.shape[1])\n",
        "    raw = mne.io.RawArray(df.T.values * 1e-6, info)  # Converting values to Volts from microvolts for MNE\n",
        "    return raw\n",
        "\n",
        "def reject_artifacts(df, channel):\n",
        "    threshold_factor = 3\n",
        "    median = df[channel].median()\n",
        "    mad = np.median(np.abs(df[channel] - median))\n",
        "    spikes = np.abs(df[channel] - median) > threshold_factor * mad\n",
        "    x = np.arange(len(df[channel]))\n",
        "    cs = CubicSpline(x[~spikes], df[channel][~spikes]) # Interpolate using Cubic Spline\n",
        "    interpolated_values = cs(x)\n",
        "    interpolated_values[spikes] *= 0.1  # Make interpolated values 0.1 times smaller\n",
        "    # Check each interpolated value's difference from median and compare to the threshold\n",
        "    spike_values = np.abs(interpolated_values - median) > threshold_factor * mad\n",
        "    interpolated_values[spike_values] *= 0.01 \n",
        "    df[channel] = interpolated_values\n",
        "    return df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n",
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n",
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n",
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n",
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n",
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n",
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n",
            "<class 'numpy.ndarray'> (10000, 8)\n",
            "<class 'pandas.core.frame.DataFrame'> (10000, 8)\n"
          ]
        }
      ],
      "source": [
        "folder_name = 'i'\n",
        "selected_columns = ['Fz', 'FC1', 'FC2', 'C3', 'Cz', 'C4', 'CPz', 'Pz']\n",
        "duration = 40 \n",
        "raw=[]\n",
        "event=[]\n",
        "BP=[]\n",
        "PP=[]\n",
        "if os.path.exists(folder_name) and os.path.isdir(folder_name):\n",
        "    for file_name in os.listdir(folder_name):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_name, file_name)\n",
        "            s_temp = pd.read_csv(file_path, header=None)\n",
        "            inst = s_temp.iloc[:, 17]\n",
        "            df_temp = s_temp.iloc[:, :8]\n",
        "            # print(df_temp.shape)\n",
        "            # df_temp.plot(figsize=(10, 8))\n",
        "            # plt.show()\n",
        "            raw.append(df_temp)\n",
        "            event.append(inst)\n",
        "            \n",
        "            # 1. Band Pass\n",
        "            raw_bp = np.copy(df_temp)\n",
        "            for column in range(8):\n",
        "                raw_bp[:, column] = butter_bandpass_filter(raw_bp[:, column], lowcut=.4, highcut=40, fs=250) \n",
        "            print(type(raw_bp), raw_bp.shape)\n",
        "            # plt.plot(raw_bp)\n",
        "            # plt.show()\n",
        "            \n",
        "            # 2. Artifact rejection\n",
        "            BP_artifact_RJ = np.copy(raw_bp)\n",
        "            for channel in range (8):\n",
        "                BP_artifact_RJ= reject_artifacts(pd.DataFrame(BP_artifact_RJ), channel)\n",
        "            print(type(BP_artifact_RJ), BP_artifact_RJ.shape)\n",
        "            # plt.plot(BP_artifact_RJ)\n",
        "            # plt.show()\n",
        "            \n",
        "            # 3. Smoothing\n",
        "            BP_artifact_RJ_SM=BP_artifact_RJ.copy()\n",
        "            window_size = 10 \n",
        "            for channel in range (8):\n",
        "                # channel_data = BP_artifact_RJ_SM[channel, :]\n",
        "                BP_artifact_RJ_SM= BP_artifact_RJ_SM.rolling(window=window_size, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n",
        "            # plt.plot(BP_artifact_RJ_SM)\n",
        "            # plt.show()\n",
        "            BP.append(BP_artifact_RJ_SM)\n",
        "            \n",
        "            # 4. Denoising and other preprocessing\n",
        "            BP_artifact_RJ_SM.columns = selected_columns\n",
        "            eeg_df_denoised = preprocess(pd.DataFrame(BP_artifact_RJ_SM), col_names=selected_columns, n_clusters=[50]*len(selected_columns))\n",
        "            # eeg_df_denoised.plot(subplots=True, figsize=(15, 10), title='Denoised EEG Data')\n",
        "            # plt.show()\n",
        "            PP.append(eeg_df_denoised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "event (120000, 1) denoised (120000, 8) pp_sig_event (120000, 9) face (50000, 9) scene (70000, 9) labels (120000,)\n"
          ]
        }
      ],
      "source": [
        "fs=250\n",
        "B_N=int(len(PP)) #Number of blocks\n",
        "PP_NP=np.array(PP) #shape: (B_N, 10000, 8=Channel Numbers)\n",
        "event=np.array(event).reshape(B_N*(df_temp.shape[0]), 1) # df_temp.shape[0]=10000\n",
        "denoised=PP_NP.reshape(B_N*(df_temp.shape[0]), 8) # seprate each blocks' signal \n",
        "pp_sig_event=np.concatenate((denoised, event), axis=1) \n",
        "labels=[] \n",
        "face = [] #lable=0\n",
        "scene=[]#lable=1\n",
        "# Aassuming correctness for the human behavior\n",
        "for i in range(len(pp_sig_event)): #len(pp_sig_event) = the whole sample points, (df_temp.shape[0]*B_N)\n",
        "    if 'M' in pp_sig_event[i, 8] or 'F' in pp_sig_event[i, 8]:\n",
        "        face.append(pp_sig_event[i])\n",
        "        labels.append(0)\n",
        "    else:\n",
        "        scene.append(pp_sig_event[i]) \n",
        "        labels.append(1)        \n",
        "face = np.array(face)\n",
        "scene = np.array(scene)\n",
        "labels=np.array(labels) \n",
        "                 \n",
        "print('event', event.shape,  'denoised',  denoised.shape, 'pp_sig_event', pp_sig_event.shape, 'face', face.shape, 'scene', scene.shape, 'labels', labels.shape)  \n",
        "#denoised is all the denoised data with shape: (df_temp.shape[0]*B_N, 8)     \n",
        "# event is all the events with shape: (df_temp.shape[0]*B_N, 1)                                                                                                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "label=labels.reshape(int(labels.shape[0]/fs), fs)\n",
        "Y=np.squeeze(label[:,0])\n",
        "\n",
        "frequency_bands = {\n",
        "    'delta': (0.5, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 14),\n",
        "    'beta': (14, 30),\n",
        "    'gamma': (30, 40),\n",
        "     }\n",
        "\n",
        "def apply_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered_signal = filtfilt(b, a, signal)\n",
        "    return filtered_signal\n",
        "denoised_reshaped = denoised.reshape(int(denoised.shape[0]/250), 250, 8)\n",
        "\n",
        "# def extract_statistical_features_from_amplitude(coefficients):\n",
        "#     \"\"\"Extract statistical features from the amplitude of wavelet coefficients.\"\"\"\n",
        "#     amplitude = np.abs(coefficients.flatten())  # Get the amplitude and flatten\n",
        "#     # Extract features from amplitude\n",
        "#     mean_amp = np.mean(amplitude)\n",
        "#     variance_amp = np.var(amplitude)\n",
        "#     skewness_amp = skew(amplitude)\n",
        "#     kurtosis_amp = kurtosis(amplitude)\n",
        "#     return [mean_amp, variance_amp, skewness_amp, kurtosis_amp]\n",
        "\n",
        "# features_combined = []\n",
        "# for segment in denoised_reshaped:\n",
        "#     segment_features = []\n",
        "#     for channel_data in segment.T:  # Going through each channel\n",
        "#         # For each frequency band, apply the bandpass filter and then extract wavelet features\n",
        "#         for band, (low, high) in frequency_bands.items():\n",
        "#             filtered_signal = apply_bandpass_filter(channel_data, low, high, fs)\n",
        "#             coefficients, frequencies = pywt.cwt(filtered_signal, scales=np.arange(1, 50), wavelet='cmor')\n",
        "#             # Extract statistical features from the amplitude\n",
        "#             amp_features = extract_statistical_features_from_amplitude(coefficients)\n",
        "#             segment_features.extend(amp_features)            \n",
        "#     features_combined.append(segment_features)\n",
        "# features_combined_np = np.array(features_combined)\n",
        "# print(features_combined_np.shape)\n",
        "# wavelet_np=features_combined_np.reshape(features_combined_np.shape[0], 8, int(features_combined_np.shape[1]/8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wavelet combined with frequency bands\n",
        "features_combined = []\n",
        "\n",
        "for segment in denoised_reshaped:\n",
        "    segment_features = []\n",
        "    for channel_data in segment.T:  # Going through each channel\n",
        "        \n",
        "        # For each frequency band, apply the bandpass filter and then extract wavelet features\n",
        "        for band, (low, high) in frequency_bands.items():\n",
        "            filtered_signal = apply_bandpass_filter(channel_data, low, high, fs)\n",
        "            coeffs = pywt.wavedec(filtered_signal, 'db4', level=4)\n",
        "            coeff_array = np.concatenate(coeffs)\n",
        "            amplitude = np.abs(coeff_array)\n",
        "            for coeff in coeffs:\n",
        "                segment_features.extend([np.mean(amplitude), np.var(amplitude), skew(amplitude), kurtosis(amplitude)])\n",
        "                \n",
        "    features_combined.append(segment_features)    \n",
        "features_combined_np = np.array(features_combined)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(480, 800)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_combined_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(480, 800)\n",
            "(480, 800) (480,)\n",
            "(504, 800)\n",
            "(504,) (56,)\n",
            "X_train: (504, 800) y_train: (504,) X_test: (56, 800) y_test: (56,) X_untouch: y_untouch:\n"
          ]
        }
      ],
      "source": [
        "# wavelet_np=features_combined_np.reshape(features_combined_np.shape[0], 8, int(features_combined_np.shape[1]/8))\n",
        "\n",
        "# combined_features = np.concatenate([wavelet_np], axis=2)\n",
        "print(features_combined_np.shape)  # Should print (1600, 8, 11)\n",
        "# af=features_combined_np.reshape(int(features_combined_np.shape[0]), int(8*features_combined_np.shape[2]))\n",
        "af=features_combined_np\n",
        "af, Y = shuffle(af, Y)\n",
        "print(af.shape, Y.shape)\n",
        "# Balance the dataset\n",
        "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(af, Y)\n",
        "X_resampled= X_resampled.astype(np.float32)\n",
        "y_resampled = y_resampled.astype(np.int32)\n",
        "\n",
        "# Split X and y into training and testing sets\n",
        "# X_touched, X_untouch, y_touch, y_untouch = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert y_train and y_test to categorical format for Keras\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
        "# y_untouch=tf.keras.utils.to_categorical(y_untouch, num_classes=2)\n",
        "\n",
        "\n",
        "# Convert the data to a numerical type (float)\n",
        "X_train = X_train.astype(np.float64)\n",
        "print(X_train.shape)\n",
        "\n",
        "# Convert one-hot-encoded labels to integer-encoded labels\n",
        "y_train = np.argmax(y_train, axis=-1)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "# y_untouch = np.argmax(y_untouch, axis=-1)\n",
        "print(y_train.shape, y_test.shape)\n",
        "\n",
        "\n",
        "print('X_train:', X_train.shape, 'y_train:', y_train.shape, 'X_test:', X_test.shape, 'y_test:',\n",
        "      y_test.shape, 'X_untouch:', 'y_untouch:' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(480, 2000)\n",
            "(480, 2000) (480,)\n"
          ]
        }
      ],
      "source": [
        "mlp_data=denoised_reshaped.reshape(denoised_reshaped.shape[0], denoised_reshaped.shape[1]*denoised_reshaped.shape[2])\n",
        "print(mlp_data.shape)\n",
        "\n",
        "af_mlp=mlp_data\n",
        "Y_mlp=np.squeeze(label[:,0])\n",
        "af_mlp, Y_mlp= shuffle(af_mlp, Y_mlp)\n",
        "print(af_mlp.shape, Y_mlp.shape)\n",
        "# Balance the dataset\n",
        "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "X_resampled_mlp, y_resampled_mlp = oversampler.fit_resample(af_mlp, Y_mlp)\n",
        "X_resampled_mlp= X_resampled_mlp.astype(np.float32)\n",
        "y_resampled_mlp = y_resampled_mlp.astype(np.int32)\n",
        "\n",
        "\n",
        "X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = train_test_split(X_resampled_mlp,y_resampled_mlp, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 21ms/step - loss: 1.0425 - accuracy: 0.5099 - val_loss: 0.7461 - val_accuracy: 0.5536\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 1.0569 - accuracy: 0.5536 - val_loss: 0.7303 - val_accuracy: 0.5714\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.9711 - accuracy: 0.5615 - val_loss: 0.7363 - val_accuracy: 0.6071\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.9201 - accuracy: 0.5417 - val_loss: 0.6414 - val_accuracy: 0.7143\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 1.0369 - accuracy: 0.5198 - val_loss: 0.6935 - val_accuracy: 0.5536\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.8816 - accuracy: 0.6131 - val_loss: 0.6427 - val_accuracy: 0.5893\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.8755 - accuracy: 0.5437 - val_loss: 0.7035 - val_accuracy: 0.5000\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.8254 - accuracy: 0.5615 - val_loss: 0.6639 - val_accuracy: 0.6071\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.8463 - accuracy: 0.5437 - val_loss: 0.6426 - val_accuracy: 0.6607\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7838 - accuracy: 0.5992 - val_loss: 0.6811 - val_accuracy: 0.6607\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.8445 - accuracy: 0.5575 - val_loss: 0.6968 - val_accuracy: 0.6786\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7384 - accuracy: 0.5913 - val_loss: 0.6752 - val_accuracy: 0.5893\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7529 - accuracy: 0.5813 - val_loss: 0.6983 - val_accuracy: 0.5357\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7484 - accuracy: 0.5873 - val_loss: 0.6479 - val_accuracy: 0.6964\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.7331 - accuracy: 0.6052 - val_loss: 0.6348 - val_accuracy: 0.7321\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6720 - accuracy: 0.6528 - val_loss: 0.6453 - val_accuracy: 0.6964\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7195 - accuracy: 0.6270 - val_loss: 0.6906 - val_accuracy: 0.6071\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.7616 - accuracy: 0.6052 - val_loss: 0.6657 - val_accuracy: 0.6964\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6726 - accuracy: 0.6548 - val_loss: 0.6270 - val_accuracy: 0.6429\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.6600 - accuracy: 0.6825 - val_loss: 0.6304 - val_accuracy: 0.6607\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6761 - accuracy: 0.6548 - val_loss: 0.6525 - val_accuracy: 0.6071\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6641 - accuracy: 0.6667 - val_loss: 0.6206 - val_accuracy: 0.6250\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6518 - accuracy: 0.6567 - val_loss: 0.6075 - val_accuracy: 0.6250\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6369 - accuracy: 0.6567 - val_loss: 0.6490 - val_accuracy: 0.6071\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6838 - accuracy: 0.6587 - val_loss: 0.6326 - val_accuracy: 0.6071\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6606 - accuracy: 0.6627 - val_loss: 0.6222 - val_accuracy: 0.6607\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5842 - accuracy: 0.7103 - val_loss: 0.5865 - val_accuracy: 0.7321\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6275 - accuracy: 0.6806 - val_loss: 0.5480 - val_accuracy: 0.7500\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5798 - accuracy: 0.7044 - val_loss: 0.5360 - val_accuracy: 0.7857\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.6541 - accuracy: 0.6746 - val_loss: 0.5833 - val_accuracy: 0.7500\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6151 - accuracy: 0.6905 - val_loss: 0.6128 - val_accuracy: 0.7143\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6201 - accuracy: 0.6984 - val_loss: 0.6072 - val_accuracy: 0.6607\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5962 - accuracy: 0.6944 - val_loss: 0.6047 - val_accuracy: 0.6786\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6131 - accuracy: 0.6885 - val_loss: 0.6337 - val_accuracy: 0.6786\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5754 - accuracy: 0.6964 - val_loss: 0.5961 - val_accuracy: 0.6607\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5655 - accuracy: 0.7183 - val_loss: 0.6093 - val_accuracy: 0.6786\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5897 - accuracy: 0.7123 - val_loss: 0.5669 - val_accuracy: 0.6964\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5951 - accuracy: 0.6925 - val_loss: 0.5884 - val_accuracy: 0.6964\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 0.7044 - val_loss: 0.5554 - val_accuracy: 0.6607\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5421 - accuracy: 0.7341 - val_loss: 0.5629 - val_accuracy: 0.6964\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5503 - accuracy: 0.7083 - val_loss: 0.5803 - val_accuracy: 0.6607\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5534 - accuracy: 0.7183 - val_loss: 0.5531 - val_accuracy: 0.6607\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5572 - accuracy: 0.7381 - val_loss: 0.5560 - val_accuracy: 0.6964\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5431 - accuracy: 0.7302 - val_loss: 0.5312 - val_accuracy: 0.6786\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5646 - accuracy: 0.7103 - val_loss: 0.5508 - val_accuracy: 0.7143\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5330 - accuracy: 0.7262 - val_loss: 0.5547 - val_accuracy: 0.7143\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5425 - accuracy: 0.7361 - val_loss: 0.5311 - val_accuracy: 0.7143\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5030 - accuracy: 0.7758 - val_loss: 0.5257 - val_accuracy: 0.7679\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5247 - accuracy: 0.7480 - val_loss: 0.5076 - val_accuracy: 0.7857\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4821 - accuracy: 0.7560 - val_loss: 0.5105 - val_accuracy: 0.7321\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5277 - accuracy: 0.7599 - val_loss: 0.5048 - val_accuracy: 0.7679\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5008 - accuracy: 0.7480 - val_loss: 0.5002 - val_accuracy: 0.8036\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5101 - accuracy: 0.7222 - val_loss: 0.4729 - val_accuracy: 0.8571\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4805 - accuracy: 0.7619 - val_loss: 0.4546 - val_accuracy: 0.8036\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4774 - accuracy: 0.7679 - val_loss: 0.5068 - val_accuracy: 0.7500\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5137 - accuracy: 0.7599 - val_loss: 0.5122 - val_accuracy: 0.7143\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4338 - accuracy: 0.7837 - val_loss: 0.5123 - val_accuracy: 0.7500\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5078 - accuracy: 0.7579 - val_loss: 0.4990 - val_accuracy: 0.8036\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.5041 - accuracy: 0.7837 - val_loss: 0.5021 - val_accuracy: 0.7321\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5172 - accuracy: 0.7778 - val_loss: 0.4957 - val_accuracy: 0.8036\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4364 - accuracy: 0.7917 - val_loss: 0.4916 - val_accuracy: 0.7679\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4829 - accuracy: 0.7698 - val_loss: 0.5027 - val_accuracy: 0.7143\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4526 - accuracy: 0.7698 - val_loss: 0.5185 - val_accuracy: 0.6964\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4383 - accuracy: 0.7698 - val_loss: 0.4985 - val_accuracy: 0.7857\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4565 - accuracy: 0.7837 - val_loss: 0.5118 - val_accuracy: 0.7679\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4255 - accuracy: 0.7976 - val_loss: 0.4886 - val_accuracy: 0.8214\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4651 - accuracy: 0.7937 - val_loss: 0.4609 - val_accuracy: 0.7500\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4736 - accuracy: 0.7659 - val_loss: 0.4916 - val_accuracy: 0.7857\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4826 - accuracy: 0.7579 - val_loss: 0.4777 - val_accuracy: 0.7679\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4582 - accuracy: 0.7619 - val_loss: 0.4840 - val_accuracy: 0.7679\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4476 - accuracy: 0.7817 - val_loss: 0.5009 - val_accuracy: 0.7321\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4447 - accuracy: 0.7937 - val_loss: 0.4755 - val_accuracy: 0.7679\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4456 - accuracy: 0.7917 - val_loss: 0.5289 - val_accuracy: 0.7500\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4217 - accuracy: 0.7956 - val_loss: 0.5062 - val_accuracy: 0.7500\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4332 - accuracy: 0.7996 - val_loss: 0.5052 - val_accuracy: 0.7500\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4252 - accuracy: 0.7937 - val_loss: 0.4946 - val_accuracy: 0.7321\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4275 - accuracy: 0.7917 - val_loss: 0.4978 - val_accuracy: 0.7679\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4353 - accuracy: 0.8016 - val_loss: 0.4743 - val_accuracy: 0.7857\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4291 - accuracy: 0.8095 - val_loss: 0.4949 - val_accuracy: 0.8036\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.4234 - accuracy: 0.7996 - val_loss: 0.4962 - val_accuracy: 0.7500\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4031 - accuracy: 0.8115 - val_loss: 0.5070 - val_accuracy: 0.7500\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4442 - accuracy: 0.7996 - val_loss: 0.4846 - val_accuracy: 0.7500\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4151 - accuracy: 0.8075 - val_loss: 0.4946 - val_accuracy: 0.7679\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4098 - accuracy: 0.8274 - val_loss: 0.5321 - val_accuracy: 0.7500\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3806 - accuracy: 0.8036 - val_loss: 0.5414 - val_accuracy: 0.7143\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4238 - accuracy: 0.8115 - val_loss: 0.5162 - val_accuracy: 0.7679\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4001 - accuracy: 0.8095 - val_loss: 0.5345 - val_accuracy: 0.7500\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4268 - accuracy: 0.8194 - val_loss: 0.4946 - val_accuracy: 0.7679\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3543 - accuracy: 0.8135 - val_loss: 0.5231 - val_accuracy: 0.7679\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4123 - accuracy: 0.8175 - val_loss: 0.5503 - val_accuracy: 0.7143\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3781 - accuracy: 0.8294 - val_loss: 0.5418 - val_accuracy: 0.7500\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.4106 - accuracy: 0.8016 - val_loss: 0.5366 - val_accuracy: 0.7857\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3683 - accuracy: 0.8095 - val_loss: 0.5128 - val_accuracy: 0.7321\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3739 - accuracy: 0.8194 - val_loss: 0.5333 - val_accuracy: 0.7143\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3677 - accuracy: 0.8353 - val_loss: 0.5256 - val_accuracy: 0.7321\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.3959 - accuracy: 0.8095 - val_loss: 0.5067 - val_accuracy: 0.8214\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3819 - accuracy: 0.8135 - val_loss: 0.4988 - val_accuracy: 0.7679\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4017 - accuracy: 0.8016 - val_loss: 0.5070 - val_accuracy: 0.7857\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3911 - accuracy: 0.8115 - val_loss: 0.5202 - val_accuracy: 0.7679\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3616 - accuracy: 0.8313 - val_loss: 0.5028 - val_accuracy: 0.7321\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2821fc839d0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Given data (You'd already have this loaded)\n",
        "# X_train, y_train, X_test, y_test\n",
        "\n",
        "# Ensure your target (y_train and y_test) is properly shaped.\n",
        "# For binary classification, it should be of shape (n_samples, 1)\n",
        "# For multi-class single-label classification, it should be one-hot encoded.\n",
        "\n",
        "# Define the MLP model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer (with relu activation and input shape matching your feature count)\n",
        "model.add(Dense(256, activation='relu', input_shape=(2000,)))\n",
        "\n",
        "# Hidden layers\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "# For binary classification, use 1 neuron with sigmoid activation\n",
        "# For multi-class classification, use softmax activation and change units to number of classes\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "# For binary classification, use binary_crossentropy\n",
        "# For multi-class classification, use categorical_crossentropy\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=32, validation_data=(X_test_mlp, y_test_mlp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 88.29%\n",
            "Test Accuracy: 73.21%\n"
          ]
        }
      ],
      "source": [
        "# Continue from the previously mentioned code\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_accuracy = model.evaluate(X_train_mlp, y_train_mlp, verbose=0)\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-12 21:06:19,268] A new study created in memory with name: no-name-219d32ba-677b-481a-bd7c-81d550756448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-12 21:06:30,077] Trial 0 finished with value: 0.5714285969734192 and parameters: {'n_units_i': 397, 'dropout_rate_1': 0.39034802208860664, 'n_units_h1': 68, 'dropout_rate_2': 0.44598741631781813, 'n_units_h2': 67, 'lr': 0.015887083182572738}. Best is trial 0 with value: 0.5714285969734192.\n",
            "[I 2023-10-12 21:06:40,231] Trial 1 finished with value: 0.6428571343421936 and parameters: {'n_units_i': 430, 'dropout_rate_1': 0.49825724466979926, 'n_units_h1': 36, 'dropout_rate_2': 0.1352095645514846, 'n_units_h2': 60, 'lr': 0.00490564738027178}. Best is trial 1 with value: 0.6428571343421936.\n",
            "[I 2023-10-12 21:06:50,153] Trial 2 finished with value: 0.5892857313156128 and parameters: {'n_units_i': 386, 'dropout_rate_1': 0.22732240623546587, 'n_units_h1': 154, 'dropout_rate_2': 0.26861922306687447, 'n_units_h2': 124, 'lr': 0.025318198393738924}. Best is trial 1 with value: 0.6428571343421936.\n",
            "[I 2023-10-12 21:06:58,054] Trial 3 finished with value: 0.8035714030265808 and parameters: {'n_units_i': 162, 'dropout_rate_1': 0.44391892970277413, 'n_units_h1': 191, 'dropout_rate_2': 0.2109605521318558, 'n_units_h2': 59, 'lr': 0.0036379098377749925}. Best is trial 3 with value: 0.8035714030265808.\n",
            "[I 2023-10-12 21:07:08,243] Trial 4 finished with value: 0.4464285671710968 and parameters: {'n_units_i': 448, 'dropout_rate_1': 0.5773450219807962, 'n_units_h1': 49, 'dropout_rate_2': 0.20295905516642312, 'n_units_h2': 26, 'lr': 0.05823248023404792}. Best is trial 3 with value: 0.8035714030265808.\n",
            "[I 2023-10-12 21:07:15,906] Trial 5 finished with value: 0.8392857313156128 and parameters: {'n_units_i': 169, 'dropout_rate_1': 0.31273258780130064, 'n_units_h1': 256, 'dropout_rate_2': 0.4196179185478752, 'n_units_h2': 97, 'lr': 0.0032147738984559925}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:07:27,362] Trial 6 finished with value: 0.75 and parameters: {'n_units_i': 407, 'dropout_rate_1': 0.3014959119974936, 'n_units_h1': 204, 'dropout_rate_2': 0.2896684679504007, 'n_units_h2': 119, 'lr': 0.0022663439345640187}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:07:32,839] Trial 7 finished with value: 0.8214285969734192 and parameters: {'n_units_i': 130, 'dropout_rate_1': 0.2846100976147542, 'n_units_h1': 74, 'dropout_rate_2': 0.19642116447768757, 'n_units_h2': 92, 'lr': 0.0031574172386059514}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:07:44,789] Trial 8 finished with value: 0.7857142686843872 and parameters: {'n_units_i': 268, 'dropout_rate_1': 0.4010831079911271, 'n_units_h1': 216, 'dropout_rate_2': 0.2033147108383229, 'n_units_h2': 23, 'lr': 0.003195983336636884}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:07:56,114] Trial 9 finished with value: 0.4464285671710968 and parameters: {'n_units_i': 509, 'dropout_rate_1': 0.34827009653853847, 'n_units_h1': 166, 'dropout_rate_2': 0.352169253863588, 'n_units_h2': 45, 'lr': 0.06829384495770167}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:08:07,932] Trial 10 finished with value: 0.8392857313156128 and parameters: {'n_units_i': 236, 'dropout_rate_1': 0.11975489650226853, 'n_units_h1': 245, 'dropout_rate_2': 0.5699770370003938, 'n_units_h2': 92, 'lr': 0.0010076047754935507}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:08:19,320] Trial 11 finished with value: 0.8035714030265808 and parameters: {'n_units_i': 231, 'dropout_rate_1': 0.12451664639387156, 'n_units_h1': 254, 'dropout_rate_2': 0.5855016755796678, 'n_units_h2': 98, 'lr': 0.0010547899991171332}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:08:30,336] Trial 12 finished with value: 0.8392857313156128 and parameters: {'n_units_i': 211, 'dropout_rate_1': 0.1177520189612839, 'n_units_h1': 248, 'dropout_rate_2': 0.558481818477104, 'n_units_h2': 95, 'lr': 0.001085577382849448}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:08:40,865] Trial 13 finished with value: 0.6964285969734192 and parameters: {'n_units_i': 306, 'dropout_rate_1': 0.17532388923716674, 'n_units_h1': 119, 'dropout_rate_2': 0.49107563447453795, 'n_units_h2': 83, 'lr': 0.0075441244974341765}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:09:02,561] Trial 14 finished with value: 0.7857142686843872 and parameters: {'n_units_i': 195, 'dropout_rate_1': 0.2238079640791923, 'n_units_h1': 223, 'dropout_rate_2': 0.43943208836847386, 'n_units_h2': 108, 'lr': 0.001449567570583733}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:09:25,623] Trial 15 finished with value: 0.75 and parameters: {'n_units_i': 269, 'dropout_rate_1': 0.19672234531334068, 'n_units_h1': 122, 'dropout_rate_2': 0.5197079512920748, 'n_units_h2': 79, 'lr': 0.0021427005988121587}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:09:54,728] Trial 16 finished with value: 0.7321428656578064 and parameters: {'n_units_i': 332, 'dropout_rate_1': 0.28784207218580193, 'n_units_h1': 241, 'dropout_rate_2': 0.5968262760566311, 'n_units_h2': 112, 'lr': 0.001650552458643534}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:10:10,960] Trial 17 finished with value: 0.8214285969734192 and parameters: {'n_units_i': 129, 'dropout_rate_1': 0.14967053661458168, 'n_units_h1': 183, 'dropout_rate_2': 0.41700165547585694, 'n_units_h2': 100, 'lr': 0.006558723434992378}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:10:32,172] Trial 18 finished with value: 0.7678571343421936 and parameters: {'n_units_i': 178, 'dropout_rate_1': 0.10641337918424185, 'n_units_h1': 228, 'dropout_rate_2': 0.5116719540000588, 'n_units_h2': 83, 'lr': 0.0010200322929701678}. Best is trial 5 with value: 0.8392857313156128.\n",
            "[I 2023-10-12 21:10:52,983] Trial 19 finished with value: 0.8214285969734192 and parameters: {'n_units_i': 245, 'dropout_rate_1': 0.24065190309729856, 'n_units_h1': 134, 'dropout_rate_2': 0.38651352918239157, 'n_units_h2': 49, 'lr': 0.00212200867217545}. Best is trial 5 with value: 0.8392857313156128.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials:  20\n",
            "Best trial:\n",
            "  Value:  0.8392857313156128\n",
            "  Params: \n",
            "    n_units_i: 169\n",
            "    dropout_rate_1: 0.31273258780130064\n",
            "    n_units_h1: 256\n",
            "    dropout_rate_2: 0.4196179185478752\n",
            "    n_units_h2: 97\n",
            "    lr: 0.0032147738984559925\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def objective(trial):\n",
        "    model = Sequential()\n",
        "    # Input layer\n",
        "    n_units_i = trial.suggest_int('n_units_i', 128, 512)\n",
        "    model.add(Dense(n_units_i, activation='relu', input_shape=(2000,)))\n",
        "    # Hidden layers\n",
        "    dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_1))\n",
        "    n_units_h1 = trial.suggest_int('n_units_h1', 32, 256)\n",
        "    model.add(Dense(n_units_h1, activation='relu'))\n",
        "    dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_2))\n",
        "    n_units_h2 = trial.suggest_int('n_units_h2', 16, 128)\n",
        "    model.add(Dense(n_units_h2, activation='relu'))\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile the model\n",
        "    lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    # Train the model\n",
        "    model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "    # Evaluate the model\n",
        "    score = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "    return score[1]  # Return accuracy\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "print('Number of finished trials: ', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: ', trial.value)\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\11-software\\Python3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[I 2023-10-13 15:13:17,467] A new study created in memory with name: no-name-ae8957c7-b9b8-4375-b807-659a871f34f7\n",
            "[W 2023-10-13 15:13:20,082] Trial 0 failed with parameters: {'n_units_i': 506, 'activation_i': 'leaky_relu', 'dropout_rate_1': 0.5521514650540524, 'n_units_h1': 124, 'activation_h1': 'leaky_relu', 'dropout_rate_2': 0.5014915357712346, 'n_units_h2': 127, 'activation_h2': 'elu', 'lr': 0.001053941675509392} because of the following error: NameError(\"name 'X_train_mlp' is not defined\").\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\11-software\\Python3.8\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\tnlab\\AppData\\Local\\Temp\\ipykernel_25076\\3579489060.py\", line 55, in objective\n",
            "    model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
            "NameError: name 'X_train_mlp' is not defined\n",
            "[W 2023-10-13 15:13:20,097] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_train_mlp' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\tnlab\\OneDrive\\Documents\\GitHub\\AlphaFold\\Neurofeedback-Based-BCI\\SVM for Unicorn Data\\Classificaton_nf_V10.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m score[\u001b[39m1\u001b[39m]  \u001b[39m# Return accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32md:\\11-software\\Python3.8\\lib\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
            "File \u001b[1;32md:\\11-software\\Python3.8\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
            "File \u001b[1;32md:\\11-software\\Python3.8\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32md:\\11-software\\Python3.8\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32md:\\11-software\\Python3.8\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "\u001b[1;32mc:\\Users\\tnlab\\OneDrive\\Documents\\GitHub\\AlphaFold\\Neurofeedback-Based-BCI\\SVM for Unicorn Data\\Classificaton_nf_V10.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train_mlp, y_train_mlp, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, validation_data\u001b[39m=\u001b[39m(X_test_mlp, y_test_mlp), verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tnlab/OneDrive/Documents/GitHub/AlphaFold/Neurofeedback-Based-BCI/SVM%20for%20Unicorn%20Data/Classificaton_nf_V10.ipynb#X20sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test_mlp, y_test_mlp, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train_mlp' is not defined"
          ]
        }
      ],
      "source": [
        "import optuna \n",
        "# sklearn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def objective(trial):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    n_units_i = trial.suggest_int('n_units_i', 128, 512)\n",
        "\n",
        "    activation_choice_i = trial.suggest_categorical('activation_i', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "    if activation_choice_i == 'leaky_relu':\n",
        "        model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "        model.add(LeakyReLU())\n",
        "    else:\n",
        "        model.add(Dense(n_units_i, activation=activation_choice_i, input_shape=(2000,)))\n",
        "\n",
        "    # Hidden layer 1\n",
        "    dropout_rate_1 = trial.suggest_float('dropout_rate_1', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_1))\n",
        "\n",
        "    n_units_h1 = trial.suggest_int('n_units_h1', 32, 256)\n",
        "    activation_choice_h1 = trial.suggest_categorical('activation_h1', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "    if activation_choice_h1 == 'leaky_relu':\n",
        "        model.add(Dense(n_units_h1))\n",
        "        model.add(LeakyReLU())\n",
        "    else:\n",
        "        model.add(Dense(n_units_h1, activation=activation_choice_h1))\n",
        "\n",
        "    # Hidden layer 2\n",
        "    dropout_rate_2 = trial.suggest_float('dropout_rate_2', 0.1, 0.6)\n",
        "    model.add(Dropout(dropout_rate_2))\n",
        "\n",
        "    n_units_h2 = trial.suggest_int('n_units_h2', 16, 128)\n",
        "    activation_choice_h2 = trial.suggest_categorical('activation_h2', ['relu', 'leaky_relu', 'elu', 'swish'])\n",
        "\n",
        "    if activation_choice_h2 == 'leaky_relu':\n",
        "        model.add(Dense(n_units_h2))\n",
        "        model.add(LeakyReLU())\n",
        "    else:\n",
        "        model.add(Dense(n_units_h2, activation=activation_choice_h2))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, validation_data=(X_test_mlp, y_test_mlp), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    score = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "    return score[1]  # Return accuracy\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print('Number of finished trials: ', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "print('  Value: ', trial.value)\n",
        "print('  Params: ')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 7ms/step - loss: 3.7920 - accuracy: 0.5198\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3.4069 - accuracy: 0.5913\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4.1256 - accuracy: 0.5556\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2.8554 - accuracy: 0.5675\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2.0168 - accuracy: 0.5952\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1.6406 - accuracy: 0.6171\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1.1707 - accuracy: 0.6131\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7978 - accuracy: 0.6508\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7571 - accuracy: 0.6667\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6666 - accuracy: 0.6607\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6182 - accuracy: 0.7083\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5322 - accuracy: 0.7401\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5772 - accuracy: 0.7123\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4960 - accuracy: 0.7440\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4885 - accuracy: 0.7679\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4588 - accuracy: 0.7718\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4370 - accuracy: 0.7897\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4249 - accuracy: 0.7976\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4015 - accuracy: 0.8234\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4317 - accuracy: 0.8075\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4231 - accuracy: 0.8075\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4314 - accuracy: 0.8214\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3644 - accuracy: 0.8353\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3674 - accuracy: 0.8373\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3586 - accuracy: 0.8433\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3614 - accuracy: 0.8373\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3073 - accuracy: 0.8631\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3049 - accuracy: 0.8651\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3419 - accuracy: 0.8651\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2920 - accuracy: 0.8611\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3036 - accuracy: 0.8512\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3522 - accuracy: 0.8571\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3094 - accuracy: 0.8512\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2851 - accuracy: 0.8829\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3251 - accuracy: 0.8631\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3423 - accuracy: 0.8730\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2980 - accuracy: 0.8770\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3250 - accuracy: 0.8829\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3062 - accuracy: 0.8750\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3548 - accuracy: 0.8552\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2884 - accuracy: 0.8651\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3801 - accuracy: 0.8452\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3214 - accuracy: 0.8770\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3026 - accuracy: 0.8710\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3335 - accuracy: 0.8690\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2783 - accuracy: 0.8770\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2472 - accuracy: 0.8889\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2261 - accuracy: 0.8968\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2405 - accuracy: 0.8869\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2350 - accuracy: 0.9266\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3101 - accuracy: 0.8750\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2559 - accuracy: 0.8988\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2806 - accuracy: 0.8889\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3213 - accuracy: 0.8750\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2935 - accuracy: 0.8849\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2648 - accuracy: 0.8889\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2869 - accuracy: 0.8869\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2226 - accuracy: 0.9048\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2210 - accuracy: 0.9067\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2348 - accuracy: 0.9048\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2587 - accuracy: 0.9048\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1846 - accuracy: 0.9147\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2856 - accuracy: 0.8968\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2535 - accuracy: 0.8889\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2137 - accuracy: 0.9087\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1973 - accuracy: 0.9246\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2369 - accuracy: 0.9187\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2534 - accuracy: 0.9028\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2226 - accuracy: 0.9107\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2062 - accuracy: 0.9246\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1880 - accuracy: 0.9385\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1810 - accuracy: 0.9266\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1888 - accuracy: 0.9266\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2444 - accuracy: 0.9266\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1922 - accuracy: 0.9286\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2487 - accuracy: 0.9028\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9266\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2065 - accuracy: 0.9206\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2049 - accuracy: 0.9206\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1732 - accuracy: 0.9306\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9464\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1392 - accuracy: 0.9425\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1437 - accuracy: 0.9425\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1531 - accuracy: 0.9405\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1499 - accuracy: 0.9405\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1331 - accuracy: 0.9504\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1699 - accuracy: 0.9325\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1681 - accuracy: 0.9325\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1772 - accuracy: 0.9325\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2137 - accuracy: 0.9087\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2488 - accuracy: 0.9127\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1667 - accuracy: 0.9425\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.1951 - accuracy: 0.9306\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.2273 - accuracy: 0.9187\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2490 - accuracy: 0.9187\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1781 - accuracy: 0.9365\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2014 - accuracy: 0.9226\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2366 - accuracy: 0.9008\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1626 - accuracy: 0.9306\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2256 - accuracy: 0.9107\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9544\n",
            "\n",
            "Accuracy on Training Data: 0.9544\n",
            "Test Loss: 0.7788\n",
            "Test Accuracy: 0.8393\n"
          ]
        }
      ],
      "source": [
        "# 1. Extract the Best Parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# 2. Build the Model using the Best Parameters\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "n_units_i = best_params['n_units_i']\n",
        "\n",
        "if best_params['activation_i'] == 'leaky_relu':\n",
        "    model.add(Dense(n_units_i, input_shape=(2000,)))\n",
        "    model.add(LeakyReLU())\n",
        "else:\n",
        "    model.add(Dense(n_units_i, activation=best_params['activation_i'], input_shape=(2000,)))\n",
        "\n",
        "# Hidden Layer 1\n",
        "model.add(Dropout(best_params['dropout_rate_1']))\n",
        "n_units_h1 = best_params['n_units_h1']\n",
        "\n",
        "if best_params['activation_h1'] == 'leaky_relu':\n",
        "    model.add(Dense(n_units_h1))\n",
        "    model.add(LeakyReLU())\n",
        "else:\n",
        "    model.add(Dense(n_units_h1, activation=best_params['activation_h1']))\n",
        "\n",
        "# Hidden Layer 2\n",
        "model.add(Dropout(best_params['dropout_rate_2']))\n",
        "n_units_h2 = best_params['n_units_h2']\n",
        "\n",
        "if best_params['activation_h2'] == 'leaky_relu':\n",
        "    model.add(Dense(n_units_h2))\n",
        "    model.add(LeakyReLU())\n",
        "else:\n",
        "    model.add(Dense(n_units_h2, activation=best_params['activation_h2']))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the Model\n",
        "optimizer = Adam(learning_rate=best_params['lr'])\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the Model on the Training Data\n",
        "model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "train_loss, train_acc = model.evaluate(X_train_mlp, y_train_mlp, verbose=1)\n",
        "print(\"\\nAccuracy on Training Data: {:.4f}\".format(train_acc))\n",
        "\n",
        "\n",
        "# 4. Test the Model on the Test Data\n",
        "loss, accuracy = model.evaluate(X_test_mlp, y_test_mlp, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 10ms/step - loss: 1.2282 - accuracy: 0.5317\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.1109 - accuracy: 0.5893\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.2039 - accuracy: 0.5575\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.9232 - accuracy: 0.5952\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.9003 - accuracy: 0.5933\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.8236 - accuracy: 0.5913\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.7835 - accuracy: 0.5754\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6992 - accuracy: 0.6508\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6407 - accuracy: 0.6706\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6500 - accuracy: 0.6647\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6206 - accuracy: 0.6964\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5815 - accuracy: 0.6984\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5746 - accuracy: 0.7242\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5751 - accuracy: 0.7183\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5668 - accuracy: 0.7341\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6038 - accuracy: 0.7183\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5110 - accuracy: 0.7520\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5264 - accuracy: 0.7619\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5355 - accuracy: 0.7440\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4816 - accuracy: 0.7560\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4664 - accuracy: 0.7679\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4269 - accuracy: 0.8016\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4898 - accuracy: 0.7857\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4498 - accuracy: 0.7897\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4145 - accuracy: 0.8115\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4400 - accuracy: 0.8036\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4517 - accuracy: 0.7897\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3999 - accuracy: 0.8175\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4386 - accuracy: 0.7917\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3750 - accuracy: 0.8294\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3826 - accuracy: 0.8194\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3854 - accuracy: 0.8214\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3673 - accuracy: 0.8254\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4212 - accuracy: 0.8115\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3446 - accuracy: 0.8472\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4141 - accuracy: 0.8056\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3462 - accuracy: 0.8591\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3698 - accuracy: 0.8234\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3483 - accuracy: 0.8234\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3629 - accuracy: 0.8512\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3475 - accuracy: 0.8413\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3439 - accuracy: 0.8552\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3379 - accuracy: 0.8512\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3360 - accuracy: 0.8512\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3309 - accuracy: 0.8671\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3284 - accuracy: 0.8710\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3370 - accuracy: 0.8472\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3171 - accuracy: 0.8631\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3245 - accuracy: 0.8591\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3547 - accuracy: 0.8512\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3330 - accuracy: 0.8671\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3233 - accuracy: 0.8413\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3127 - accuracy: 0.8730\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2844 - accuracy: 0.8849\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3106 - accuracy: 0.8790\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2981 - accuracy: 0.8710\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2776 - accuracy: 0.8929\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2924 - accuracy: 0.8631\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2904 - accuracy: 0.8750\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2761 - accuracy: 0.8770\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2645 - accuracy: 0.8889\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3436 - accuracy: 0.8552\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2825 - accuracy: 0.8829\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2911 - accuracy: 0.8611\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2260 - accuracy: 0.9107\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2738 - accuracy: 0.8869\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2689 - accuracy: 0.8909\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2411 - accuracy: 0.8849\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2269 - accuracy: 0.8948\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2590 - accuracy: 0.9008\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2939 - accuracy: 0.8849\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2361 - accuracy: 0.9147\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2365 - accuracy: 0.9147\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2544 - accuracy: 0.8968\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2466 - accuracy: 0.8988\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2139 - accuracy: 0.9067\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2500 - accuracy: 0.8829\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2123 - accuracy: 0.9206\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2208 - accuracy: 0.9246\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.2226 - accuracy: 0.9226\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2140 - accuracy: 0.9147\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2280 - accuracy: 0.8889\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2570 - accuracy: 0.9107\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2337 - accuracy: 0.9127\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2301 - accuracy: 0.9107\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2268 - accuracy: 0.9087\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2535 - accuracy: 0.9087\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2309 - accuracy: 0.9028\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.2163 - accuracy: 0.9067\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2260 - accuracy: 0.9226\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2029 - accuracy: 0.9187\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1983 - accuracy: 0.9107\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2028 - accuracy: 0.9286\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2090 - accuracy: 0.9147\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2205 - accuracy: 0.9048\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2258 - accuracy: 0.9008\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2301 - accuracy: 0.9048\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.9246\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2347 - accuracy: 0.9167\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1745 - accuracy: 0.9286\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9643\n",
            "\n",
            "Accuracy on Training Data: 0.9643\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6477 - accuracy: 0.8393\n",
            "\n",
            "Accuracy on Test Data: 0.8393\n"
          ]
        }
      ],
      "source": [
        "# Get the best hyperparameters from the Optuna study\n",
        "best_params = study.best_params\n",
        "\n",
        "# Build the model using the best hyperparameters\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(best_params['n_units_i'], activation='relu', input_shape=(2000,)))\n",
        "model.add(Dropout(best_params['dropout_rate_1']))\n",
        "model.add(Dense(best_params['n_units_h1'], activation='relu'))\n",
        "model.add(Dropout(best_params['dropout_rate_2']))\n",
        "model.add(Dense(best_params['n_units_h2'], activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = Adam(learning_rate=best_params['lr'])\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the training data\n",
        "model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=64, verbose=1)\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "train_loss, train_acc = model.evaluate(X_train_mlp, y_train_mlp, verbose=1)\n",
        "print(\"\\nAccuracy on Training Data: {:.4f}\".format(train_acc))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(X_test_mlp, y_test_mlp, verbose=1)\n",
        "print(\"\\nAccuracy on Test Data: {:.4f}\".format(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 1s 27ms/step - loss: 3.3977 - accuracy: 0.4841 - val_loss: 2.1189 - val_accuracy: 0.4464\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 2.0230 - accuracy: 0.5099 - val_loss: 1.4850 - val_accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1.3757 - accuracy: 0.5417 - val_loss: 1.4836 - val_accuracy: 0.5179\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.9627 - accuracy: 0.5317 - val_loss: 0.6867 - val_accuracy: 0.5357\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.7598 - accuracy: 0.5357 - val_loss: 0.6881 - val_accuracy: 0.5000\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6915 - accuracy: 0.5754 - val_loss: 0.7572 - val_accuracy: 0.5893\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.7104 - accuracy: 0.5853 - val_loss: 0.6468 - val_accuracy: 0.6964\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6510 - accuracy: 0.6151 - val_loss: 0.6478 - val_accuracy: 0.6250\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6419 - accuracy: 0.6290 - val_loss: 0.7148 - val_accuracy: 0.5893\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6701 - accuracy: 0.6369 - val_loss: 0.6808 - val_accuracy: 0.5893\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6365 - accuracy: 0.6488 - val_loss: 0.7103 - val_accuracy: 0.5893\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.6039 - accuracy: 0.6647 - val_loss: 0.6383 - val_accuracy: 0.6071\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6001 - accuracy: 0.6508 - val_loss: 0.6018 - val_accuracy: 0.6250\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5818 - accuracy: 0.6984 - val_loss: 0.7271 - val_accuracy: 0.5357\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.5847 - accuracy: 0.6647 - val_loss: 0.6717 - val_accuracy: 0.6429\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5652 - accuracy: 0.6786 - val_loss: 0.7430 - val_accuracy: 0.5893\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5217 - accuracy: 0.7202 - val_loss: 0.6628 - val_accuracy: 0.6071\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5288 - accuracy: 0.7183 - val_loss: 0.6631 - val_accuracy: 0.6429\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5087 - accuracy: 0.7302 - val_loss: 0.6378 - val_accuracy: 0.6429\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4691 - accuracy: 0.7520 - val_loss: 0.6521 - val_accuracy: 0.6429\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4788 - accuracy: 0.7500 - val_loss: 0.6400 - val_accuracy: 0.6786\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4707 - accuracy: 0.7500 - val_loss: 0.7669 - val_accuracy: 0.6429\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4698 - accuracy: 0.7639 - val_loss: 0.7128 - val_accuracy: 0.6250\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4596 - accuracy: 0.7798 - val_loss: 0.6183 - val_accuracy: 0.6607\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4420 - accuracy: 0.7659 - val_loss: 0.7155 - val_accuracy: 0.6786\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4544 - accuracy: 0.7659 - val_loss: 0.7816 - val_accuracy: 0.6786\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.4330 - accuracy: 0.7778 - val_loss: 0.6064 - val_accuracy: 0.6429\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4669 - accuracy: 0.7500 - val_loss: 0.7428 - val_accuracy: 0.6250\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4418 - accuracy: 0.7778 - val_loss: 0.8311 - val_accuracy: 0.6607\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4029 - accuracy: 0.7996 - val_loss: 0.8458 - val_accuracy: 0.6429\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3906 - accuracy: 0.8016 - val_loss: 1.0623 - val_accuracy: 0.6607\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3879 - accuracy: 0.8075 - val_loss: 1.0854 - val_accuracy: 0.6607\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3726 - accuracy: 0.8075 - val_loss: 1.0338 - val_accuracy: 0.6786\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3625 - accuracy: 0.8234 - val_loss: 1.1759 - val_accuracy: 0.6250\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4068 - accuracy: 0.8095 - val_loss: 1.0446 - val_accuracy: 0.5893\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4605 - accuracy: 0.7917 - val_loss: 0.9855 - val_accuracy: 0.6786\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.3939 - accuracy: 0.8036 - val_loss: 1.0276 - val_accuracy: 0.6786\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3892 - accuracy: 0.8095 - val_loss: 0.7063 - val_accuracy: 0.6429\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3557 - accuracy: 0.8135 - val_loss: 0.6876 - val_accuracy: 0.6786\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3607 - accuracy: 0.8155 - val_loss: 0.8249 - val_accuracy: 0.6607\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3823 - accuracy: 0.8135 - val_loss: 1.0376 - val_accuracy: 0.6964\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4278 - accuracy: 0.8115 - val_loss: 0.9713 - val_accuracy: 0.6964\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4792 - accuracy: 0.7857 - val_loss: 1.0206 - val_accuracy: 0.6250\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4498 - accuracy: 0.7758 - val_loss: 0.8768 - val_accuracy: 0.6429\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.4028 - accuracy: 0.7976 - val_loss: 0.7926 - val_accuracy: 0.6964\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.4297 - accuracy: 0.7996 - val_loss: 0.7068 - val_accuracy: 0.6786\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3662 - accuracy: 0.8175 - val_loss: 0.7652 - val_accuracy: 0.6964\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3421 - accuracy: 0.8234 - val_loss: 0.7064 - val_accuracy: 0.6964\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.3248 - accuracy: 0.8294 - val_loss: 0.9566 - val_accuracy: 0.6786\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3699 - accuracy: 0.8175 - val_loss: 1.0738 - val_accuracy: 0.6429\n",
            "2/2 [==============================] - 0s 3ms/step - loss: 1.0738 - accuracy: 0.6429\n",
            "Test Loss: 1.0738\n",
            "Test Accuracy: 64.29%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to define and compile the model based on hyperparameters\n",
        "def create_model(n_units_i, dropout_rate_1, n_units_h1, dropout_rate_2, n_units_h2, lr):\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Input layer\n",
        "    model.add(Dense(n_units_i, activation='relu', input_shape=(2000,)))\n",
        "\n",
        "    # Hidden layers\n",
        "    model.add(Dropout(dropout_rate_1))\n",
        "    model.add(Dense(n_units_h1, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate_2))\n",
        "    model.add(Dense(n_units_h2, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Using the best hyperparameters found by Optuna\n",
        "best_params = study.best_params\n",
        "best_model = create_model(\n",
        "    best_params['n_units_i'], \n",
        "    best_params['dropout_rate_1'], \n",
        "    best_params['n_units_h1'], \n",
        "    best_params['dropout_rate_2'], \n",
        "    best_params['n_units_h2'], \n",
        "    best_params['lr']\n",
        ")\n",
        "\n",
        "# Train the model using the entire training dataset\n",
        "best_model.fit(X_train_mlp, y_train_mlp, epochs=50, batch_size=64, validation_data=(X_test_mlp, y_test_mlp))\n",
        "\n",
        "# Evaluate the model using the test dataset\n",
        "loss, accuracy = best_model.evaluate(X_test_mlp, y_test_mlp)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.74      0.60        23\n",
            "           1       0.73      0.48      0.58        33\n",
            "\n",
            "    accuracy                           0.59        56\n",
            "   macro avg       0.61      0.61      0.59        56\n",
            "weighted avg       0.63      0.59      0.59        56\n",
            "\n",
            "Accuracy: 0.5892857142857143\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Initialize and train a Random Forest classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict class labels for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Generate and print classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "\n",
        "# Print accuracy score\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-09 19:19:29,750] A new study created in memory with name: no-name-3beb3da4-ae0f-4191-a27b-e0a0b293ea23\n",
            "[I 2023-10-09 19:19:30,921] Trial 0 finished with value: 0.5993267326732674 and parameters: {'n_estimators': 127, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 0 with value: 0.5993267326732674.\n",
            "[I 2023-10-09 19:19:31,377] Trial 1 finished with value: 0.5952079207920793 and parameters: {'n_estimators': 110, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 0 with value: 0.5993267326732674.\n",
            "[I 2023-10-09 19:19:31,880] Trial 2 finished with value: 0.5952871287128714 and parameters: {'n_estimators': 142, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 0 with value: 0.5993267326732674.\n",
            "[I 2023-10-09 19:19:32,394] Trial 3 finished with value: 0.5912871287128714 and parameters: {'n_estimators': 132, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 0.5993267326732674.\n",
            "[I 2023-10-09 19:19:33,114] Trial 4 finished with value: 0.5913069306930694 and parameters: {'n_estimators': 134, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 'auto', 'criterion': 'gini', 'class_weight': 'balanced'}. Best is trial 0 with value: 0.5993267326732674.\n",
            "[I 2023-10-09 19:19:33,847] Trial 5 finished with value: 0.5933069306930694 and parameters: {'n_estimators': 135, 'min_samples_split': 10, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 0.5993267326732674.\n",
            "[I 2023-10-09 19:19:34,875] Trial 6 finished with value: 0.5914257425742575 and parameters: {'n_estimators': 125, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 0.5993267326732674.\n",
            "[I 2023-10-09 19:19:36,093] Trial 7 finished with value: 0.6092277227722773 and parameters: {'n_estimators': 135, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 7 with value: 0.6092277227722773.\n",
            "[I 2023-10-09 19:19:37,236] Trial 8 finished with value: 0.5914257425742575 and parameters: {'n_estimators': 131, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 7 with value: 0.6092277227722773.\n",
            "[I 2023-10-09 19:19:38,080] Trial 9 finished with value: 0.5972475247524753 and parameters: {'n_estimators': 133, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 7 with value: 0.6092277227722773.\n",
            "[I 2023-10-09 19:19:39,143] Trial 10 finished with value: 0.6051089108910892 and parameters: {'n_estimators': 149, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 7 with value: 0.6092277227722773.\n",
            "[I 2023-10-09 19:19:40,205] Trial 11 finished with value: 0.6110693069306932 and parameters: {'n_estimators': 150, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 11 with value: 0.6110693069306932.\n",
            "[I 2023-10-09 19:19:41,140] Trial 12 finished with value: 0.5854059405940595 and parameters: {'n_estimators': 148, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 11 with value: 0.6110693069306932.\n",
            "[I 2023-10-09 19:19:42,014] Trial 13 finished with value: 0.5793663366336634 and parameters: {'n_estimators': 117, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 11 with value: 0.6110693069306932.\n",
            "[I 2023-10-09 19:19:42,735] Trial 14 finished with value: 0.5952475247524753 and parameters: {'n_estimators': 100, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 11 with value: 0.6110693069306932.\n",
            "[I 2023-10-09 19:19:43,582] Trial 15 finished with value: 0.6012871287128714 and parameters: {'n_estimators': 143, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 11 with value: 0.6110693069306932.\n",
            "[I 2023-10-09 19:19:44,878] Trial 16 finished with value: 0.6092277227722773 and parameters: {'n_estimators': 141, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 11 with value: 0.6110693069306932.\n",
            "[I 2023-10-09 19:19:45,780] Trial 17 finished with value: 0.5833267326732674 and parameters: {'n_estimators': 120, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 11 with value: 0.6110693069306932.\n",
            "[I 2023-10-09 19:19:46,403] Trial 18 finished with value: 0.6131287128712872 and parameters: {'n_estimators': 139, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 18 with value: 0.6131287128712872.\n",
            "[I 2023-10-09 19:19:46,908] Trial 19 finished with value: 0.5993465346534654 and parameters: {'n_estimators': 145, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 18 with value: 0.6131287128712872.\n",
            "[I 2023-10-09 19:19:47,405] Trial 20 finished with value: 0.6092277227722773 and parameters: {'n_estimators': 139, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 18 with value: 0.6131287128712872.\n",
            "[I 2023-10-09 19:19:48,030] Trial 21 finished with value: 0.6132079207920793 and parameters: {'n_estimators': 138, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 21 with value: 0.6132079207920793.\n",
            "[I 2023-10-09 19:19:48,706] Trial 22 finished with value: 0.6231683168316833 and parameters: {'n_estimators': 150, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:49,329] Trial 23 finished with value: 0.6091881188118813 and parameters: {'n_estimators': 138, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:49,985] Trial 24 finished with value: 0.6112277227722773 and parameters: {'n_estimators': 146, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:50,516] Trial 25 finished with value: 0.5972475247524753 and parameters: {'n_estimators': 129, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:51,030] Trial 26 finished with value: 0.6090693069306932 and parameters: {'n_estimators': 121, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:51,653] Trial 27 finished with value: 0.6132079207920793 and parameters: {'n_estimators': 139, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:52,292] Trial 28 finished with value: 0.6033267326732674 and parameters: {'n_estimators': 145, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:53,420] Trial 29 finished with value: 0.5993267326732674 and parameters: {'n_estimators': 124, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:53,980] Trial 30 finished with value: 0.5834257425742575 and parameters: {'n_estimators': 128, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:54,601] Trial 31 finished with value: 0.6112079207920793 and parameters: {'n_estimators': 140, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:55,204] Trial 32 finished with value: 0.6171683168316833 and parameters: {'n_estimators': 137, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 22 with value: 0.6231683168316833.\n",
            "[I 2023-10-09 19:19:55,831] Trial 33 finished with value: 0.625029702970297 and parameters: {'n_estimators': 137, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 33 with value: 0.625029702970297.\n",
            "[I 2023-10-09 19:19:56,422] Trial 34 finished with value: 0.5952673267326734 and parameters: {'n_estimators': 136, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 33 with value: 0.625029702970297.\n",
            "[I 2023-10-09 19:19:57,077] Trial 35 finished with value: 0.6111485148514852 and parameters: {'n_estimators': 144, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 33 with value: 0.625029702970297.\n",
            "[I 2023-10-09 19:19:57,577] Trial 36 finished with value: 0.5853267326732674 and parameters: {'n_estimators': 131, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 33 with value: 0.625029702970297.\n",
            "[I 2023-10-09 19:19:58,550] Trial 37 finished with value: 0.5874455445544555 and parameters: {'n_estimators': 114, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 33 with value: 0.625029702970297.\n",
            "[I 2023-10-09 19:19:59,209] Trial 38 finished with value: 0.6270099009900991 and parameters: {'n_estimators': 147, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:19:59,867] Trial 39 finished with value: 0.6270099009900991 and parameters: {'n_estimators': 147, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:00,523] Trial 40 finished with value: 0.6111683168316833 and parameters: {'n_estimators': 147, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:01,204] Trial 41 finished with value: 0.6229900990099011 and parameters: {'n_estimators': 150, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:01,902] Trial 42 finished with value: 0.6229900990099011 and parameters: {'n_estimators': 150, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:02,542] Trial 43 finished with value: 0.6210495049504952 and parameters: {'n_estimators': 142, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:03,931] Trial 44 finished with value: 0.6012673267326734 and parameters: {'n_estimators': 148, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:04,617] Trial 45 finished with value: 0.6229900990099011 and parameters: {'n_estimators': 150, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:05,275] Trial 46 finished with value: 0.6091683168316833 and parameters: {'n_estimators': 147, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:05,946] Trial 47 finished with value: 0.6230891089108911 and parameters: {'n_estimators': 143, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:07,310] Trial 48 finished with value: 0.6032673267326734 and parameters: {'n_estimators': 143, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 38 with value: 0.6270099009900991.\n",
            "[I 2023-10-09 19:20:07,951] Trial 49 finished with value: 0.627049504950495 and parameters: {'n_estimators': 134, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 49 with value: 0.627049504950495.\n",
            "[I 2023-10-09 19:20:08,589] Trial 50 finished with value: 0.6092475247524753 and parameters: {'n_estimators': 134, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 49 with value: 0.627049504950495.\n",
            "[I 2023-10-09 19:20:09,264] Trial 51 finished with value: 0.6151089108910891 and parameters: {'n_estimators': 142, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 49 with value: 0.627049504950495.\n",
            "[I 2023-10-09 19:20:09,935] Trial 52 finished with value: 0.6092079207920793 and parameters: {'n_estimators': 146, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 49 with value: 0.627049504950495.\n",
            "[I 2023-10-09 19:20:10,434] Trial 53 finished with value: 0.6171287128712871 and parameters: {'n_estimators': 103, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 49 with value: 0.627049504950495.\n",
            "[I 2023-10-09 19:20:11,089] Trial 54 finished with value: 0.6191287128712871 and parameters: {'n_estimators': 141, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 49 with value: 0.627049504950495.\n",
            "[I 2023-10-09 19:20:11,760] Trial 55 finished with value: 0.631029702970297 and parameters: {'n_estimators': 144, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 55 with value: 0.631029702970297.\n",
            "[I 2023-10-09 19:20:12,448] Trial 56 finished with value: 0.6191089108910892 and parameters: {'n_estimators': 145, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 55 with value: 0.631029702970297.\n",
            "[I 2023-10-09 19:20:13,122] Trial 57 finished with value: 0.6251287128712872 and parameters: {'n_estimators': 148, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced_subsample'}. Best is trial 55 with value: 0.631029702970297.\n",
            "[I 2023-10-09 19:20:13,592] Trial 58 finished with value: 0.5913069306930694 and parameters: {'n_estimators': 148, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 'log2', 'criterion': 'gini', 'class_weight': 'balanced_subsample'}. Best is trial 55 with value: 0.631029702970297.\n",
            "[I 2023-10-09 19:20:14,217] Trial 59 finished with value: 0.6429900990099011 and parameters: {'n_estimators': 133, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:15,296] Trial 60 finished with value: 0.5972079207920793 and parameters: {'n_estimators': 125, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:15,871] Trial 61 finished with value: 0.6389900990099011 and parameters: {'n_estimators': 132, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:16,465] Trial 62 finished with value: 0.6389900990099011 and parameters: {'n_estimators': 132, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:17,075] Trial 63 finished with value: 0.6389900990099011 and parameters: {'n_estimators': 132, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:17,668] Trial 64 finished with value: 0.6389900990099011 and parameters: {'n_estimators': 132, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:18,135] Trial 65 finished with value: 0.5674455445544555 and parameters: {'n_estimators': 132, 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:19,057] Trial 66 finished with value: 0.6072475247524753 and parameters: {'n_estimators': 129, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:19,654] Trial 67 finished with value: 0.5853663366336634 and parameters: {'n_estimators': 134, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:20,231] Trial 68 finished with value: 0.6091485148514852 and parameters: {'n_estimators': 127, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:20,823] Trial 69 finished with value: 0.6211287128712872 and parameters: {'n_estimators': 131, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:21,358] Trial 70 finished with value: 0.642950495049505 and parameters: {'n_estimators': 122, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:21,912] Trial 71 finished with value: 0.642950495049505 and parameters: {'n_estimators': 122, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:22,443] Trial 72 finished with value: 0.5873069306930694 and parameters: {'n_estimators': 122, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 59 with value: 0.6429900990099011.\n",
            "[I 2023-10-09 19:20:22,977] Trial 73 finished with value: 0.6469108910891089 and parameters: {'n_estimators': 118, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 73 with value: 0.6469108910891089.\n",
            "[I 2023-10-09 19:20:23,526] Trial 74 finished with value: 0.6170891089108911 and parameters: {'n_estimators': 118, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 73 with value: 0.6469108910891089.\n",
            "[I 2023-10-09 19:20:24,023] Trial 75 finished with value: 0.5892673267326733 and parameters: {'n_estimators': 114, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 73 with value: 0.6469108910891089.\n",
            "[I 2023-10-09 19:20:24,837] Trial 76 finished with value: 0.6091485148514852 and parameters: {'n_estimators': 127, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced'}. Best is trial 73 with value: 0.6469108910891089.\n",
            "[I 2023-10-09 19:20:25,382] Trial 77 finished with value: 0.6131089108910891 and parameters: {'n_estimators': 124, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 73 with value: 0.6469108910891089.\n",
            "[I 2023-10-09 19:20:25,901] Trial 78 finished with value: 0.6489108910891088 and parameters: {'n_estimators': 116, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:26,417] Trial 79 finished with value: 0.6170693069306932 and parameters: {'n_estimators': 115, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:26,932] Trial 80 finished with value: 0.6488910891089109 and parameters: {'n_estimators': 111, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:27,445] Trial 81 finished with value: 0.638970297029703 and parameters: {'n_estimators': 110, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:27,987] Trial 82 finished with value: 0.6449504950495049 and parameters: {'n_estimators': 117, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:28,534] Trial 83 finished with value: 0.5833465346534654 and parameters: {'n_estimators': 119, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:29,063] Trial 84 finished with value: 0.6489108910891088 and parameters: {'n_estimators': 116, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:30,067] Trial 85 finished with value: 0.5972079207920792 and parameters: {'n_estimators': 110, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:30,582] Trial 86 finished with value: 0.6250693069306932 and parameters: {'n_estimators': 112, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:31,132] Trial 87 finished with value: 0.6191089108910891 and parameters: {'n_estimators': 116, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:31,694] Trial 88 finished with value: 0.619108910891089 and parameters: {'n_estimators': 122, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:32,319] Trial 89 finished with value: 0.5953465346534654 and parameters: {'n_estimators': 107, 'min_samples_split': 8, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:32,758] Trial 90 finished with value: 0.5932475247524753 and parameters: {'n_estimators': 118, 'min_samples_split': 7, 'min_samples_leaf': 9, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 78 with value: 0.6489108910891088.\n",
            "[I 2023-10-09 19:20:33,305] Trial 91 finished with value: 0.648930693069307 and parameters: {'n_estimators': 120, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:33,819] Trial 92 finished with value: 0.6469108910891089 and parameters: {'n_estimators': 112, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:34,332] Trial 93 finished with value: 0.6190495049504949 and parameters: {'n_estimators': 112, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:34,878] Trial 94 finished with value: 0.6151089108910891 and parameters: {'n_estimators': 120, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:35,393] Trial 95 finished with value: 0.5833267326732674 and parameters: {'n_estimators': 116, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:35,908] Trial 96 finished with value: 0.6210891089108912 and parameters: {'n_estimators': 112, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:36,469] Trial 97 finished with value: 0.6250099009900989 and parameters: {'n_estimators': 121, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:37,045] Trial 98 finished with value: 0.625069306930693 and parameters: {'n_estimators': 123, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n",
            "[I 2023-10-09 19:20:37,998] Trial 99 finished with value: 0.5973267326732674 and parameters: {'n_estimators': 117, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 'auto', 'criterion': 'entropy', 'class_weight': 'balanced'}. Best is trial 91 with value: 0.648930693069307.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_estimators': 120, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy', 'class_weight': 'balanced'}\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    # Define search space\n",
        "    n_estimators = trial.suggest_int('n_estimators', 100, 150)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
        "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "    class_weight = trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample'])\n",
        "\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators, \n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        random_state=42,\n",
        "        criterion=criterion,\n",
        "        class_weight= class_weight\n",
        "    )\n",
        "\n",
        "    # Use StratifiedKFold cross-validation\n",
        "    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    return cross_val_score(clf, X_train, y_train, cv=stratified_kfold, n_jobs=-1).mean()\n",
        "\n",
        "# Create a study object and specify the direction is maximize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(study.best_trial.params)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.78      0.61        23\n",
            "           1       0.75      0.45      0.57        33\n",
            "\n",
            "    accuracy                           0.59        56\n",
            "   macro avg       0.62      0.62      0.59        56\n",
            "weighted avg       0.65      0.59      0.58        56\n",
            "\n",
            "Accuracy: 0.5892857142857143\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Splitting the data\n",
        "# X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "#     X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use best hyperparameters from the Optuna optimization (replace these with actual best params)\n",
        "best_params = study.best_trial.params\n",
        "\n",
        "clf_best = RandomForestClassifier(**best_params, random_state=42)\n",
        "clf_best.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = clf_best.predict(X_test)\n",
        "\n",
        "# Generate and print classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "\n",
        "# Print accuracy score\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy:  0.5357142857142857\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.57      0.50        23\n",
            "           1       0.63      0.52      0.57        33\n",
            "\n",
            "    accuracy                           0.54        56\n",
            "   macro avg       0.54      0.54      0.53        56\n",
            "weighted avg       0.56      0.54      0.54        56\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from joblib import dump\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "# Create a linear SVM classifier\n",
        "clf = svm.SVC(kernel='linear')\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "# Save the model to disk\n",
        "filename = 'C:/Users/tnlab/OneDrive/Documents/GitHub/Neurofeedback-Based-BCI/SVM for Unicorn Data/my_svm_model.joblib'\n",
        "dump(clf, filename)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print('Model accuracy: ', accuracy_score(y_test, y_pred))\n",
        "report_svm_matrix = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report_svm_matrix)\n",
        "report_svm = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "report_df_svm = pd.DataFrame(report_svm).transpose()\n",
        "report_df_svm.to_excel(f\"svm_classification_report_{folder_name}.xlsx\", index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-09 18:02:31,953] A new study created in memory with name: no-name-883a66fd-3ec7-44b3-b337-2e21922ac9f2\n",
            "C:\\Users\\tnlab\\AppData\\Local\\Temp\\ipykernel_16848\\3781216325.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 1e-10, 1e10)\n",
            "[I 2023-10-09 18:02:32,028] Trial 0 finished with value: 0.6566336633663367 and parameters: {'C': 37736667.2223723, 'kernel': 'rbf', 'coef0': 0.8298523640508921, 'gamma': 'scale'}. Best is trial 0 with value: 0.6566336633663367.\n",
            "C:\\Users\\tnlab\\AppData\\Local\\Temp\\ipykernel_16848\\3781216325.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  C = trial.suggest_loguniform('C', 1e-10, 1e10)\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    # Define search space\n",
        "    C = trial.suggest_loguniform('C', 1e-10, 1e10)\n",
        "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid'])\n",
        "    if kernel == 'poly':\n",
        "        degree = trial.suggest_int('degree', 1, 5)\n",
        "    else:\n",
        "        degree = 1  # default value, won't be used\n",
        "    coef0 = trial.suggest_float('coef0', -1.0, 1.0)\n",
        "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
        "\n",
        "    # Create SVM classifier\n",
        "    clf = SVC(C=C, kernel=kernel, degree=degree, coef0=coef0, gamma=gamma, random_state=42)\n",
        "\n",
        "    # Use StratifiedKFold cross-validation\n",
        "    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    return cross_val_score(clf, X_train, y_train, cv=stratified_kfold, n_jobs=-1).mean()\n",
        "\n",
        "# Create a study object and specify the direction is maximize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(study.best_trial.params)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
